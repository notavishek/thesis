{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Dataset: 6 sources, 75864 total samples\n",
      "============================================================\n",
      "Total samples: 75864\n",
      "\n",
      "Split counts:\n",
      "split\n",
      "train    45518\n",
      "val      15173\n",
      "test     15173\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Per split language distribution:\n",
      "split  language\n",
      "test   english      6158\n",
      "       bangla       5679\n",
      "       banglish     3336\n",
      "train  english     18475\n",
      "       bangla      17036\n",
      "       banglish    10007\n",
      "val    english      6159\n",
      "       bangla       5678\n",
      "       banglish     3336\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "is_hate by split:\n",
      "split  is_hate\n",
      "test   0          10139\n",
      "       1           5034\n",
      "train  0          30418\n",
      "       1          15100\n",
      "val    0          10141\n",
      "       1           5032\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "hate_type distribution (valid labels):\n",
      "72354/75864 samples have hate_type labels (95.4%)\n",
      "hate_type\n",
      "0    51271\n",
      "1     1382\n",
      "2     1409\n",
      "3      873\n",
      "4    14357\n",
      "5     3062\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "target_group distribution (valid labels):\n",
      "58669/75864 samples have target_group labels (77.3%)\n",
      "target_group\n",
      "0    42826\n",
      "1    12523\n",
      "2     2183\n",
      "3     1137\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "severity distribution:\n",
      "severity\n",
      "0    58265\n",
      "1    12015\n",
      "2     1933\n",
      "3     3651\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "source_dataset distribution:\n",
      "source_dataset\n",
      "toxic_comments_labeled    49998\n",
      "olid                      13240\n",
      "bengali_hate_v2            5698\n",
      "bengali_hate_v1            3418\n",
      "blp25_subtask_1b           2512\n",
      "ethos                       998\n",
      "Name: count, dtype: int64\n",
      "\n",
      "============================================================\n",
      "‚úÖ Using ENHANCED dataset with auto-labeled toxic_comments\n",
      "   - 95% hate_type coverage\n",
      "   - 77% target_group coverage\n",
      "   - Best for final model training!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 1. EDA: Sanity check splits and class balance\n",
    "# =====================================================\n",
    "# CHOOSE YOUR DATASET:\n",
    "# - FILTERED: High quality labels, smaller size (25K), 86% hate_type coverage\n",
    "# - ENHANCED: Auto-labeled, larger size (75K), 95% hate_type coverage ‚≠ê RECOMMENDED\n",
    "# =====================================================\n",
    "import pandas as pd\n",
    "\n",
    "# OPTION 1: Filtered dataset (no toxic_comments) - Fast training, high quality\n",
    "# df = pd.read_csv('dataset/UNIFIED_ALL_SPLIT_FILTERED.csv')\n",
    "\n",
    "# OPTION 2: Enhanced dataset (with auto-labeled toxic_comments) - Best performance ‚≠ê\n",
    "df = pd.read_csv('dataset/UNIFIED_ALL_SPLIT_ENHANCED.csv')\n",
    "\n",
    "print(f'üìä Dataset: {df[\"source_dataset\"].nunique()} sources, {len(df)} total samples')\n",
    "print('='*60)\n",
    "print(f'Total samples: {len(df)}\\n')\n",
    "\n",
    "print('Split counts:')\n",
    "print(df['split'].value_counts(), '\\n')\n",
    "\n",
    "print('Per split language distribution:')\n",
    "print(df.groupby('split')['language'].value_counts(), '\\n')\n",
    "\n",
    "print('is_hate by split:')\n",
    "print(df.groupby('split')['is_hate'].value_counts(), '\\n')\n",
    "\n",
    "print('hate_type distribution (valid labels):')\n",
    "ht_valid = df[df['hate_type'] != -1]\n",
    "print(f'{len(ht_valid)}/{len(df)} samples have hate_type labels ({len(ht_valid)/len(df)*100:.1f}%)')\n",
    "print(ht_valid['hate_type'].value_counts().sort_index(), '\\n')\n",
    "\n",
    "print('target_group distribution (valid labels):')\n",
    "tg_valid = df[df['target_group'] != -1]\n",
    "print(f'{len(tg_valid)}/{len(df)} samples have target_group labels ({len(tg_valid)/len(df)*100:.1f}%)')\n",
    "print(tg_valid['target_group'].value_counts().sort_index(), '\\n')\n",
    "\n",
    "print('severity distribution:')\n",
    "print(df['severity'].value_counts().sort_index(), '\\n')\n",
    "\n",
    "print('source_dataset distribution:')\n",
    "print(df['source_dataset'].value_counts())\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "if 'toxic_comments_labeled' in df['source_dataset'].values:\n",
    "    print('‚úÖ Using ENHANCED dataset with auto-labeled toxic_comments')\n",
    "    print('   - 95% hate_type coverage')\n",
    "    print('   - 77% target_group coverage')\n",
    "    print('   - Best for final model training!')\n",
    "else:\n",
    "    print('‚úÖ Using FILTERED dataset (toxic_comments excluded)')\n",
    "    print('   - 86% hate_type coverage')\n",
    "    print('   - High label quality')\n",
    "    print('   - Fast training!')\n",
    "print('='*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# COLAB SETUP: Mount Google Drive and set paths\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# =====================================================\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Run this cell FIRST on Colab!\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# =====================================================\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[32m      7\u001b[39m drive.mount(\u001b[33m'\u001b[39m\u001b[33m/content/drive\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Create checkpoint directory on Google Drive\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "# COLAB SETUP: Mount Google Drive and set paths\n",
    "# =====================================================\n",
    "# Run this cell FIRST on Colab!\n",
    "# =====================================================\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create checkpoint directory on Google Drive\n",
    "import os\n",
    "COLAB_CHECKPOINT_DIR = '/content/drive/MyDrive/thesis_training/checkpoints_v2/'\n",
    "os.makedirs(COLAB_CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# Set this to resume from a previous epoch (or None to start fresh)\n",
    "resume_checkpoint = None  # e.g., '/content/drive/MyDrive/thesis_training/checkpoints_v2/xlmr_v2_epoch2.pt'\n",
    "\n",
    "print(f'‚úÖ Google Drive mounted!')\n",
    "print(f'üìÅ Checkpoints will be saved to: {COLAB_CHECKPOINT_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample keys: dict_keys(['input_ids', 'attention_mask', 'hate_type', 'target_group', 'severity', 'hate_type_mask', 'target_group_mask', 'severity_mask'])\n",
      "input_ids shape: torch.Size([160])\n",
      "hate_type: 4 mask: True\n"
     ]
    }
   ],
   "source": [
    "# 2. HateDataset: PyTorch Dataset with tokenization and masking for incomplete labels\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import XLMRobertaTokenizer\n",
    "\n",
    "class HateDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=160):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        text = str(row['text'])\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Extract labels (use -1 for missing)\n",
    "        hate_type = int(row['hate_type'])\n",
    "        target_group = int(row['target_group'])\n",
    "        severity = int(row['severity'])\n",
    "        \n",
    "        # Create masks: True if label is valid (not -1)\n",
    "        hate_type_mask = hate_type != -1\n",
    "        target_group_mask = target_group != -1\n",
    "        severity_mask = severity != -1\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'hate_type': torch.tensor(max(0, hate_type), dtype=torch.long),\n",
    "            'target_group': torch.tensor(max(0, target_group), dtype=torch.long),\n",
    "            'severity': torch.tensor(max(0, severity), dtype=torch.long),\n",
    "            'hate_type_mask': torch.tensor(hate_type_mask, dtype=torch.bool),\n",
    "            'target_group_mask': torch.tensor(target_group_mask, dtype=torch.bool),\n",
    "            'severity_mask': torch.tensor(severity_mask, dtype=torch.bool),\n",
    "        }\n",
    "\n",
    "# Test the dataset\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-large')\n",
    "test_df = df.head(5)\n",
    "test_dataset = HateDataset(test_df, tokenizer)\n",
    "sample = test_dataset[0]\n",
    "print('Sample keys:', sample.keys())\n",
    "print('input_ids shape:', sample['input_ids'].shape)\n",
    "print('hate_type:', sample['hate_type'].item(), 'mask:', sample['hate_type_mask'].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Model loaded. Backbone hidden size: 1024\n",
      "MultiTaskXLMRRoberta(\n",
      "  (backbone): XLMRobertaModel(\n",
      "    (embeddings): XLMRobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 1024)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): XLMRobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-23): 24 x XLMRobertaLayer(\n",
      "          (attention): XLMRobertaAttention(\n",
      "            (self): XLMRobertaSdpaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): XLMRobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): XLMRobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): XLMRobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): XLMRobertaPooler(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (hate_type_head): Linear(in_features=1024, out_features=6, bias=True)\n",
      "  (target_group_head): Linear(in_features=1024, out_features=4, bias=True)\n",
      "  (severity_head): Linear(in_features=1024, out_features=4, bias=True)\n",
      ")\n",
      "Model loaded. Backbone hidden size: 1024\n",
      "MultiTaskXLMRRoberta(\n",
      "  (backbone): XLMRobertaModel(\n",
      "    (embeddings): XLMRobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 1024)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): XLMRobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-23): 24 x XLMRobertaLayer(\n",
      "          (attention): XLMRobertaAttention(\n",
      "            (self): XLMRobertaSdpaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): XLMRobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): XLMRobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): XLMRobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): XLMRobertaPooler(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (hate_type_head): Linear(in_features=1024, out_features=6, bias=True)\n",
      "  (target_group_head): Linear(in_features=1024, out_features=4, bias=True)\n",
      "  (severity_head): Linear(in_features=1024, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 3. MultiTaskXLMRRoberta: Model with shared backbone and task-specific heads\n",
    "import torch.nn as nn\n",
    "from transformers import XLMRobertaModel\n",
    "\n",
    "class MultiTaskXLMRRoberta(nn.Module):\n",
    "    def __init__(self, model_name='xlm-roberta-large', dropout=0.2,\n",
    "                 n_hate_type=6, n_target_group=4, n_severity=4):\n",
    "        super().__init__()\n",
    "        self.backbone = XLMRobertaModel.from_pretrained(model_name)\n",
    "        hidden_size = self.backbone.config.hidden_size  # 1024 for large\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Task-specific classification heads\n",
    "        self.hate_type_head = nn.Linear(hidden_size, n_hate_type)\n",
    "        self.target_group_head = nn.Linear(hidden_size, n_target_group)\n",
    "        self.severity_head = nn.Linear(hidden_size, n_severity)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Use CLS token representation\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        cls_output = self.dropout(cls_output)\n",
    "        \n",
    "        hate_type_logits = self.hate_type_head(cls_output)\n",
    "        target_group_logits = self.target_group_head(cls_output)\n",
    "        severity_logits = self.severity_head(cls_output)\n",
    "        \n",
    "        return hate_type_logits, target_group_logits, severity_logits\n",
    "\n",
    "# Instantiate and check model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "model = MultiTaskXLMRRoberta().to(device)\n",
    "print(f'Model loaded. Backbone hidden size: {model.backbone.config.hidden_size}')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ multitask_loss function defined with CLASS WEIGHTS support.\n"
     ]
    }
   ],
   "source": [
    "# 4. multitask_loss: Masked cross-entropy loss with CLASS WEIGHTS for imbalanced data\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_class_weights(df, column, n_classes, smoothing=0.1):\n",
    "    \"\"\"Compute inverse frequency class weights with smoothing.\"\"\"\n",
    "    valid = df[df[column] != -1][column]\n",
    "    counts = valid.value_counts().reindex(range(n_classes), fill_value=1).values\n",
    "    weights = 1.0 / (counts + smoothing * len(valid))\n",
    "    weights = weights / weights.sum() * n_classes  # Normalize to sum to n_classes\n",
    "    return torch.tensor(weights, dtype=torch.float32)\n",
    "\n",
    "def multitask_loss(hate_type_logits, target_group_logits, severity_logits,\n",
    "                   targets, masks, task_weights=(1.0, 1.0, 1.0),\n",
    "                   ht_class_weights=None, tg_class_weights=None, sv_class_weights=None):\n",
    "    \"\"\"\n",
    "    Compute masked cross-entropy loss with optional class weights.\n",
    "    Class weights help the model pay more attention to minority classes.\n",
    "    \"\"\"\n",
    "    total_loss = 0.0\n",
    "    n_tasks = 0\n",
    "    \n",
    "    # Hate type loss (only where mask is True)\n",
    "    ht_mask = masks['hate_type'].bool()\n",
    "    if ht_mask.any():\n",
    "        loss_ht = F.cross_entropy(\n",
    "            hate_type_logits[ht_mask], \n",
    "            targets['hate_type'][ht_mask],\n",
    "            weight=ht_class_weights\n",
    "        )\n",
    "        total_loss += task_weights[0] * loss_ht\n",
    "        n_tasks += 1\n",
    "    \n",
    "    # Target group loss\n",
    "    tg_mask = masks['target_group'].bool()\n",
    "    if tg_mask.any():\n",
    "        loss_tg = F.cross_entropy(\n",
    "            target_group_logits[tg_mask], \n",
    "            targets['target_group'][tg_mask],\n",
    "            weight=tg_class_weights\n",
    "        )\n",
    "        total_loss += task_weights[1] * loss_tg\n",
    "        n_tasks += 1\n",
    "    \n",
    "    # Severity loss\n",
    "    sv_mask = masks['severity'].bool()\n",
    "    if sv_mask.any():\n",
    "        loss_sv = F.cross_entropy(\n",
    "            severity_logits[sv_mask], \n",
    "            targets['severity'][sv_mask],\n",
    "            weight=sv_class_weights\n",
    "        )\n",
    "        total_loss += task_weights[2] * loss_sv\n",
    "        n_tasks += 1\n",
    "    \n",
    "    return total_loss / max(1, n_tasks)\n",
    "\n",
    "print('‚úÖ multitask_loss function defined with CLASS WEIGHTS support.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit shapes:\n",
      "  hate_type: torch.Size([2, 6])\n",
      "  target_group: torch.Size([2, 4])\n",
      "  severity: torch.Size([2, 4])\n",
      "Batch loss: 1.3877\n"
     ]
    }
   ],
   "source": [
    "# 5. Mini-batch validation: Test forward pass and loss computation\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "batch = next(iter(test_loader))\n",
    "\n",
    "def move_batch_to_device(batch):\n",
    "    return {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "batch = move_batch_to_device(batch)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    ht_logits, tg_logits, sv_logits = model(batch['input_ids'], batch['attention_mask'])\n",
    "\n",
    "print('Logit shapes:')\n",
    "print(f'  hate_type: {ht_logits.shape}')\n",
    "print(f'  target_group: {tg_logits.shape}')\n",
    "print(f'  severity: {sv_logits.shape}')\n",
    "\n",
    "targets = {k: batch[k] for k in ['hate_type', 'target_group', 'severity']}\n",
    "masks = {k: batch[f'{k}_mask'] for k in targets.keys()}\n",
    "loss = multitask_loss(ht_logits, tg_logits, sv_logits, targets, masks)\n",
    "print(f'Batch loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using local checkpoint dir: checkpoints/\n",
      "Train: 45518, Val: 15173, Test: 15173\n",
      "\n",
      "üìä Class Weights (higher = more focus on that class):\n",
      "  hate_type:    ['0.20', '1.35', '1.34', '1.44', '0.54', '1.14']\n",
      "  target_group: ['0.24', '0.64', '1.45', '1.67']\n",
      "  severity:     ['0.23', '0.78', '1.62', '1.37']\n",
      "\n",
      "Data loaders created. Batches - Train: 2845, Val: 949, Test: 949\n"
     ]
    }
   ],
   "source": [
    "# 6. Full data loaders setup + COMPUTE CLASS WEIGHTS\n",
    "import os\n",
    "SEED = 1337\n",
    "MAX_LENGTH = 160\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Use Colab checkpoint dir if available, otherwise local\n",
    "try:\n",
    "    CHECKPOINT_DIR = COLAB_CHECKPOINT_DIR\n",
    "    print(f'Using Colab checkpoint dir: {CHECKPOINT_DIR}')\n",
    "except NameError:\n",
    "    CHECKPOINT_DIR = 'checkpoints/'\n",
    "    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "    print(f'Using local checkpoint dir: {CHECKPOINT_DIR}')\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "train_df = df[df['split'] == 'train'].reset_index(drop=True)\n",
    "val_df = df[df['split'] == 'val'].reset_index(drop=True)\n",
    "test_df = df[df['split'] == 'test'].reset_index(drop=True)\n",
    "print(f'Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}')\n",
    "\n",
    "# Compute class weights from training data (helps with imbalanced classes!)\n",
    "ht_weights = compute_class_weights(train_df, 'hate_type', 6).to(device)\n",
    "tg_weights = compute_class_weights(train_df, 'target_group', 4).to(device)\n",
    "sv_weights = compute_class_weights(train_df, 'severity', 4).to(device)\n",
    "\n",
    "print(f'\\nüìä Class Weights (higher = more focus on that class):')\n",
    "print(f'  hate_type:    {[f\"{w:.2f}\" for w in ht_weights.tolist()]}')\n",
    "print(f'  target_group: {[f\"{w:.2f}\" for w in tg_weights.tolist()]}')\n",
    "print(f'  severity:     {[f\"{w:.2f}\" for w in sv_weights.tolist()]}')\n",
    "\n",
    "train_dataset = HateDataset(train_df, tokenizer, max_length=MAX_LENGTH)\n",
    "val_dataset = HateDataset(val_df, tokenizer, max_length=MAX_LENGTH)\n",
    "test_dataset = HateDataset(test_df, tokenizer, max_length=MAX_LENGTH)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "print(f'\\nData loaders created. Batches - Train: {len(train_loader)}, Val: {len(val_loader)}, Test: {len(test_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ evaluate() function defined with class weights support.\n"
     ]
    }
   ],
   "source": [
    "# 7. evaluate() helper: Compute loss and F1 metrics (with class weights)\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "def evaluate(model, data_loader, task_weights=(1.0, 1.0, 1.0),\n",
    "             ht_class_weights=None, tg_class_weights=None, sv_class_weights=None,\n",
    "             verbose=False):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    n_batches = 0\n",
    "    all_preds = {'hate_type': [], 'target_group': [], 'severity': []}\n",
    "    all_labels = {'hate_type': [], 'target_group': [], 'severity': []}\n",
    "    all_masks = {'hate_type': [], 'target_group': [], 'severity': []}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            batch = move_batch_to_device(batch)\n",
    "            ht_logits, tg_logits, sv_logits = model(batch['input_ids'], batch['attention_mask'])\n",
    "            targets = {k: batch[k] for k in ['hate_type', 'target_group', 'severity']}\n",
    "            masks = {k: batch[f'{k}_mask'] for k in targets.keys()}\n",
    "            loss = multitask_loss(ht_logits, tg_logits, sv_logits, targets, masks, task_weights,\n",
    "                                  ht_class_weights, tg_class_weights, sv_class_weights)\n",
    "            total_loss += loss.item()\n",
    "            n_batches += 1\n",
    "            all_preds['hate_type'].extend(ht_logits.argmax(dim=1).cpu().numpy())\n",
    "            all_preds['target_group'].extend(tg_logits.argmax(dim=1).cpu().numpy())\n",
    "            all_preds['severity'].extend(sv_logits.argmax(dim=1).cpu().numpy())\n",
    "            for task in ['hate_type', 'target_group', 'severity']:\n",
    "                all_labels[task].extend(targets[task].cpu().numpy())\n",
    "                all_masks[task].extend(masks[task].cpu().numpy())\n",
    "    \n",
    "    metrics = {'loss': total_loss / max(1, n_batches)}\n",
    "    for task in ['hate_type', 'target_group', 'severity']:\n",
    "        mask = np.array(all_masks[task]).astype(bool)\n",
    "        if mask.sum() > 0:\n",
    "            preds = np.array(all_preds[task])[mask]\n",
    "            labels = np.array(all_labels[task])[mask]\n",
    "            metrics[f'{task}_macro_f1'] = f1_score(labels, preds, average='macro', zero_division=0)\n",
    "            metrics[f'{task}_micro_f1'] = f1_score(labels, preds, average='micro', zero_division=0)\n",
    "            \n",
    "            # Per-class F1 for detailed analysis\n",
    "            if verbose:\n",
    "                print(f'\\n{task.upper()} Classification Report:')\n",
    "                print(classification_report(labels, preds, zero_division=0))\n",
    "        else:\n",
    "            metrics[f'{task}_macro_f1'] = None\n",
    "            metrics[f'{task}_micro_f1'] = None\n",
    "    return metrics\n",
    "\n",
    "print('‚úÖ evaluate() function defined with class weights support.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ train_model() defined with CLASS WEIGHTS + macro F1 early stopping.\n"
     ]
    }
   ],
   "source": [
    "# 8. train_model() function: With SPACE-SAVING checkpoint strategy\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "training_config = {\n",
    "    'epochs': 5,           # Increased from 3\n",
    "    'learning_rate': 1e-5, # Lower LR for stability (was 2e-5)\n",
    "    'weight_decay': 1e-2,\n",
    "    'warmup_ratio': 0.1,\n",
    "    'grad_clip': 1.0,\n",
    "    'patience': 3,         # More patience\n",
    "    'dropout': 0.3,        # Slightly higher dropout\n",
    "    'task_weights': (1.0, 1.0, 1.0),\n",
    "    'use_class_weights': True  # NEW: Enable class weights\n",
    "}\n",
    "\n",
    "def train_model(train_loader, val_loader, config=None, run_name='xlmr_run', use_wandb=False, resume_from=None,\n",
    "                ht_class_weights=None, tg_class_weights=None, sv_class_weights=None):\n",
    "    \"\"\"\n",
    "    Train the multi-task model with class weights for imbalanced data.\n",
    "    SPACE-SAVING: Auto-deletes old epoch checkpoints to save disk space.\n",
    "    \"\"\"\n",
    "    if config is None: config = training_config\n",
    "    if use_wandb:\n",
    "        import wandb\n",
    "        wandb.init(project='multilingual-hate-detection', name=run_name, config=config, resume='allow')\n",
    "    \n",
    "    model = MultiTaskXLMRRoberta(dropout=config['dropout']).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
    "    total_steps = len(train_loader) * config['epochs']\n",
    "    warmup_steps = int(total_steps * config['warmup_ratio'])\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_macro_f1 = 0.0  # Also track best macro F1\n",
    "    patience_counter = 0\n",
    "    start_epoch = 1\n",
    "    history = []\n",
    "    \n",
    "    best_ckpt_path = os.path.join(CHECKPOINT_DIR, f'{run_name}_best.pt')\n",
    "    \n",
    "    # Resume from checkpoint if provided\n",
    "    if resume_from and os.path.exists(resume_from):\n",
    "        print(f'Resuming from checkpoint: {resume_from}')\n",
    "        checkpoint = torch.load(resume_from, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n",
    "        best_macro_f1 = checkpoint.get('best_macro_f1', 0.0)\n",
    "        patience_counter = checkpoint.get('patience_counter', 0)\n",
    "        history = checkpoint.get('history', [])\n",
    "        print(f'Resumed from epoch {checkpoint[\"epoch\"]}. Starting epoch {start_epoch}.')\n",
    "    \n",
    "    for epoch in range(start_epoch, config['epochs'] + 1):\n",
    "        model.train()\n",
    "        start = time.time()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch}/{config[\"epochs\"]}', leave=True)\n",
    "        for batch_idx, batch in enumerate(pbar):\n",
    "            batch = move_batch_to_device(batch)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(batch['input_ids'], batch['attention_mask'])\n",
    "            targets = {k: batch[k] for k in ['hate_type', 'target_group', 'severity']}\n",
    "            masks = {k: batch[f'{k}_mask'] for k in targets.keys()}\n",
    "            \n",
    "            # Use class weights if provided\n",
    "            loss = multitask_loss(*logits, targets, masks, config['task_weights'],\n",
    "                                  ht_class_weights, tg_class_weights, sv_class_weights)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config['grad_clip'])\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            avg_loss = running_loss / (batch_idx + 1)\n",
    "            pbar.set_postfix({'loss': f'{avg_loss:.4f}'})\n",
    "        \n",
    "        train_loss = running_loss / max(1, len(train_loader))\n",
    "        \n",
    "        print(f'Evaluating on validation set...')\n",
    "        val_metrics = evaluate(model, val_loader, config['task_weights'],\n",
    "                               ht_class_weights, tg_class_weights, sv_class_weights)\n",
    "        val_loss = val_metrics['loss']\n",
    "        \n",
    "        # Compute average macro F1 across tasks\n",
    "        macro_f1s = [val_metrics.get(f'{t}_macro_f1', 0) or 0 for t in ['hate_type', 'target_group', 'severity']]\n",
    "        avg_macro_f1 = sum(macro_f1s) / len(macro_f1s)\n",
    "        \n",
    "        epoch_time = time.time() - start\n",
    "        log_payload = {'epoch': epoch, 'train_loss': train_loss, 'val_loss': val_loss, \n",
    "                       'avg_macro_f1': avg_macro_f1, 'epoch_time': epoch_time, **val_metrics}\n",
    "        history.append(log_payload)\n",
    "        if use_wandb: wandb.log(log_payload)\n",
    "        \n",
    "        print(f'Epoch {epoch}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}, avg_macro_f1={avg_macro_f1:.4f}, time={epoch_time:.1f}s')\n",
    "        print(f'  hate_type_macro_f1={val_metrics.get(\"hate_type_macro_f1\", 0):.4f}, target_group_macro_f1={val_metrics.get(\"target_group_macro_f1\", 0):.4f}, severity_macro_f1={val_metrics.get(\"severity_macro_f1\", 0):.4f}')\n",
    "        \n",
    "        # ‚ö° SPACE-SAVING: Save epoch checkpoint (for resume)\n",
    "        epoch_ckpt_path = os.path.join(CHECKPOINT_DIR, f'{run_name}_epoch{epoch}.pt')\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'best_macro_f1': best_macro_f1,\n",
    "            'patience_counter': patience_counter,\n",
    "            'history': history,\n",
    "            'config': config\n",
    "        }, epoch_ckpt_path)\n",
    "        print(f'  üíæ Epoch checkpoint saved to {epoch_ckpt_path}')\n",
    "        \n",
    "        # Save best model based on MACRO F1 (better for imbalanced data)\n",
    "        if avg_macro_f1 > best_macro_f1:\n",
    "            best_macro_f1 = avg_macro_f1\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), best_ckpt_path)\n",
    "            print(f'  ‚úì New best checkpoint saved! (avg_macro_f1={avg_macro_f1:.4f})')\n",
    "            \n",
    "            # ‚ö° DELETE OLD EPOCH CHECKPOINT after saving best (saves 7GB per epoch!)\n",
    "            if epoch > 1:\n",
    "                old_epoch_ckpt = os.path.join(CHECKPOINT_DIR, f'{run_name}_epoch{epoch-1}.pt')\n",
    "                if os.path.exists(old_epoch_ckpt):\n",
    "                    os.remove(old_epoch_ckpt)\n",
    "                    print(f'  üóëÔ∏è Deleted old checkpoint: {old_epoch_ckpt}')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f'  No improvement. Patience: {patience_counter}/{config[\"patience\"]}')\n",
    "            if patience_counter >= config['patience']:\n",
    "                print('Early stopping triggered.')\n",
    "                break\n",
    "    \n",
    "    # ‚ö° FINAL CLEANUP: Delete last epoch checkpoint, keep only best\n",
    "    final_epoch_ckpt = os.path.join(CHECKPOINT_DIR, f'{run_name}_epoch{epoch}.pt')\n",
    "    if os.path.exists(final_epoch_ckpt):\n",
    "        os.remove(final_epoch_ckpt)\n",
    "        print(f'üóëÔ∏è Training complete. Deleted final epoch checkpoint. Only keeping: {best_ckpt_path}')\n",
    "    \n",
    "    if use_wandb: wandb.finish()\n",
    "    return best_ckpt_path, history\n",
    "\n",
    "print('‚úÖ train_model() defined with SPACE-SAVING checkpoint strategy!')\n",
    "print('üíæ Saves: Best model (~2.5GB) + Latest epoch for resume (~7GB)')\n",
    "print('üóëÔ∏è Auto-deletes old epoch checkpoints after each epoch')\n",
    "print('üìä Total space needed: ~10GB max (vs 35GB for 5 epochs)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smoke test sizes: Train=512, Val=256, Test=256\n"
     ]
    }
   ],
   "source": [
    "# 9. Smoke test loaders\n",
    "SMOKE_TRAIN_SIZE = 512\n",
    "SMOKE_VAL_SIZE = 256\n",
    "SMOKE_TEST_SIZE = 256\n",
    "\n",
    "smoke_train_df = train_df.sample(n=SMOKE_TRAIN_SIZE, random_state=SEED)\n",
    "smoke_val_df = val_df.sample(n=SMOKE_VAL_SIZE, random_state=SEED)\n",
    "smoke_test_df = test_df.sample(n=SMOKE_TEST_SIZE, random_state=SEED)\n",
    "\n",
    "smoke_train_loader = DataLoader(HateDataset(smoke_train_df, tokenizer, max_length=MAX_LENGTH), batch_size=8, shuffle=True)\n",
    "smoke_val_loader = DataLoader(HateDataset(smoke_val_df, tokenizer, max_length=MAX_LENGTH), batch_size=8, shuffle=False)\n",
    "smoke_test_loader = DataLoader(HateDataset(smoke_test_df, tokenizer, max_length=MAX_LENGTH), batch_size=8, shuffle=False)\n",
    "print(f'Smoke test sizes: Train={len(smoke_train_df)}, Val={len(smoke_val_df)}, Test={len(smoke_test_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [10:57<00:00, 10.27s/it, loss=1.0116]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on validation set...\n",
      "Epoch 1: train_loss=1.0116, val_loss=0.7571, avg_macro_f1=0.1877, time=718.4s\n",
      "  hate_type_macro_f1=0.1365, target_group_macro_f1=0.2086, severity_macro_f1=0.2181\n",
      "Epoch 1: train_loss=1.0116, val_loss=0.7571, avg_macro_f1=0.1877, time=718.4s\n",
      "  hate_type_macro_f1=0.1365, target_group_macro_f1=0.2086, severity_macro_f1=0.2181\n",
      "  üíæ Epoch checkpoint saved to checkpoints/xlmr_smoke_epoch1.pt\n",
      "  üíæ Epoch checkpoint saved to checkpoints/xlmr_smoke_epoch1.pt\n",
      "  ‚úì New best checkpoint saved! (avg_macro_f1=0.1877)\n",
      "Training history: [{'epoch': 1, 'train_loss': 1.0116422502323985, 'val_loss': 0.7571360222063959, 'avg_macro_f1': 0.18771649739118393, 'epoch_time': 718.3748588562012, 'loss': 0.7571360222063959, 'hate_type_macro_f1': 0.1365079365079365, 'hate_type_micro_f1': 0.6935483870967742, 'target_group_macro_f1': 0.20857988165680474, 'target_group_micro_f1': 0.7157360406091371, 'severity_macro_f1': 0.21806167400881057, 'severity_micro_f1': 0.7734375}]\n",
      "  ‚úì New best checkpoint saved! (avg_macro_f1=0.1877)\n",
      "Training history: [{'epoch': 1, 'train_loss': 1.0116422502323985, 'val_loss': 0.7571360222063959, 'avg_macro_f1': 0.18771649739118393, 'epoch_time': 718.3748588562012, 'loss': 0.7571360222063959, 'hate_type_macro_f1': 0.1365079365079365, 'hate_type_micro_f1': 0.6935483870967742, 'target_group_macro_f1': 0.20857988165680474, 'target_group_micro_f1': 0.7157360406091371, 'severity_macro_f1': 0.21806167400881057, 'severity_micro_f1': 0.7734375}]\n"
     ]
    }
   ],
   "source": [
    "# 10. Smoke test training\n",
    "quick_config = training_config.copy()\n",
    "quick_config.update({'epochs': 1, 'patience': 1})\n",
    "\n",
    "best_checkpoint, history = train_model(smoke_train_loader, smoke_val_loader, config=quick_config, run_name='xlmr_smoke', use_wandb=False)\n",
    "print('Training history:', history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'loss': 0.7571360222063959, 'hate_type_macro_f1': 0.1365079365079365, 'hate_type_micro_f1': 0.6935483870967742, 'target_group_macro_f1': 0.20857988165680474, 'target_group_micro_f1': 0.7157360406091371, 'severity_macro_f1': 0.21806167400881057, 'severity_micro_f1': 0.7734375}\n",
      "Test metrics: {'loss': 0.7242736108601093, 'hate_type_macro_f1': 0.13762927605409706, 'hate_type_micro_f1': 0.7032520325203252, 'target_group_macro_f1': 0.21137026239067055, 'target_group_micro_f1': 0.7323232323232324, 'severity_macro_f1': 0.21991247264770242, 'severity_micro_f1': 0.78515625}\n"
     ]
    }
   ],
   "source": [
    "# 11. Evaluate smoke checkpoint\n",
    "best_model = MultiTaskXLMRRoberta().to(device)\n",
    "best_model.load_state_dict(torch.load(best_checkpoint, map_location=device))\n",
    "val_results = evaluate(best_model, smoke_val_loader)\n",
    "test_results = evaluate(best_model, smoke_test_loader)\n",
    "print('Validation metrics:', val_results)\n",
    "print('Test metrics:', test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\Admin\\_netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\Admin\\_netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mc221025\u001b[0m (\u001b[33mc221025-international-islamic-university-chittagong\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mc221025\u001b[0m (\u001b[33mc221025-international-islamic-university-chittagong\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B login successful!\n"
     ]
    }
   ],
   "source": [
    "# 12. W&B Login\n",
    "try:\n",
    "    import wandb\n",
    "except ModuleNotFoundError:\n",
    "    import sys, subprocess\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'wandb'])\n",
    "    import wandb\n",
    "wandb.login(key='61dd3d59137a5043373cd8ecc8f74c4d1c620ea6')\n",
    "print('W&B login successful!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting full training on 45518 samples...\n",
      "Device: cpu\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\thesis\\wandb\\run-20251204_123742-jcfzeaq4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/c221025-international-islamic-university-chittagong/multilingual-hate-detection/runs/jcfzeaq4' target=\"_blank\">xlmr_full_large</a></strong> to <a href='https://wandb.ai/c221025-international-islamic-university-chittagong/multilingual-hate-detection' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/c221025-international-islamic-university-chittagong/multilingual-hate-detection' target=\"_blank\">https://wandb.ai/c221025-international-islamic-university-chittagong/multilingual-hate-detection</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/c221025-international-islamic-university-chittagong/multilingual-hate-detection/runs/jcfzeaq4' target=\"_blank\">https://wandb.ai/c221025-international-islamic-university-chittagong/multilingual-hate-detection/runs/jcfzeaq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2845/2845 [14:00:57<00:00, 17.74s/it, loss=0.6686]  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on validation set...\n",
      "Epoch 1: train_loss=0.6686, val_loss=0.4283, time=53392.9s\n",
      "  hate_type_macro_f1=0.6151, target_group_macro_f1=0.5770, severity_macro_f1=0.6358\n",
      "Epoch 1: train_loss=0.6686, val_loss=0.4283, time=53392.9s\n",
      "  hate_type_macro_f1=0.6151, target_group_macro_f1=0.5770, severity_macro_f1=0.6358\n",
      "  ‚úì New best checkpoint saved to checkpoints/xlmr_full_large_best.pt\n",
      "  ‚úì New best checkpoint saved to checkpoints/xlmr_full_large_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:   1%|          | 35/2845 [11:10<14:56:47, 19.15s/it, loss=0.3921]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mStarting full training on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m samples...\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mDevice: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m best_checkpoint_full, history_full = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfull_training_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mxlmr_full_large\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_wandb\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     11\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mFull training history:\u001b[39m\u001b[33m'\u001b[39m, history_full)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 43\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(train_loader, val_loader, config, run_name, use_wandb)\u001b[39m\n\u001b[32m     41\u001b[39m masks = {k: batch[\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_mask\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m targets.keys()}\n\u001b[32m     42\u001b[39m loss = multitask_loss(*logits, targets, masks, config[\u001b[33m'\u001b[39m\u001b[33mtask_weights\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), config[\u001b[33m'\u001b[39m\u001b[33mgrad_clip\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     45\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\thesis\\venv\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\thesis\\venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\thesis\\venv\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 13. Full training WITH CLASS WEIGHTS (Enhanced Dataset)\n",
    "# =====================================================\n",
    "# Using ENHANCED dataset with auto-labeled toxic_comments\n",
    "# Expected: 85% test F1 (hate_type), 74% (target_group), 95% (severity)\n",
    "# =====================================================\n",
    "\n",
    "full_training_config = {\n",
    "    'epochs': 5,\n",
    "    'learning_rate': 1e-5,      # Lower LR for stability with auto-labeled data\n",
    "    'weight_decay': 1e-2,\n",
    "    'warmup_ratio': 0.1,\n",
    "    'grad_clip': 1.0,\n",
    "    'patience': 3,              # Early stopping patience\n",
    "    'dropout': 0.3,\n",
    "    'task_weights': (1.0, 1.0, 1.0),\n",
    "    'use_class_weights': True   # CRITICAL for handling class imbalance!\n",
    "}\n",
    "\n",
    "print(f'üöÄ Starting ENHANCED training on {len(train_dataset)} samples...')\n",
    "print(f'Device: {device}')\n",
    "print(f'üìÅ Saving checkpoints to: {CHECKPOINT_DIR}')\n",
    "print(f'\\nüìä Using class weights to handle imbalance!')\n",
    "print(f'  hate_type weights:    {[f\"{w:.2f}\" for w in ht_weights.tolist()]}')\n",
    "print(f'  target_group weights: {[f\"{w:.2f}\" for w in tg_weights.tolist()]}')\n",
    "print(f'  severity weights:     {[f\"{w:.2f}\" for w in sv_weights.tolist()]}')\n",
    "\n",
    "best_checkpoint_full, history_full = train_model(\n",
    "    train_loader, val_loader, \n",
    "    config=full_training_config,\n",
    "    run_name='xlmr_enhanced',      # ‚Üê Changed name to indicate enhanced dataset\n",
    "    use_wandb=True,                # Set to False if no W&B\n",
    "    ht_class_weights=ht_weights,\n",
    "    tg_class_weights=tg_weights,\n",
    "    sv_class_weights=sv_weights\n",
    ")\n",
    "\n",
    "print('\\n‚úÖ Training complete!')\n",
    "print(f'üìÅ Best checkpoint saved to: {best_checkpoint_full}')\n",
    "print('History:', history_full)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint: checkpoints/xlmr_colab_best.pt\n",
      "‚úÖ Model loaded successfully!\n",
      "\n",
      "=== Validation Set Results ===\n",
      "‚úÖ Model loaded successfully!\n",
      "\n",
      "=== Validation Set Results ===\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m‚úÖ Model loaded successfully!\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== Validation Set Results ===\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m val_results_full = \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_model_full\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m val_results_full.items():\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(model, data_loader, task_weights)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[32m     15\u001b[39m     batch = move_batch_to_device(batch)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     ht_logits, tg_logits, sv_logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minput_ids\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mattention_mask\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m     targets = {k: batch[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33mhate_type\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtarget_group\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mseverity\u001b[39m\u001b[33m'\u001b[39m]}\n\u001b[32m     18\u001b[39m     masks = {k: batch[\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_mask\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m targets.keys()}\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\thesis\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\thesis\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mMultiTaskXLMRRoberta.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask):\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m     \u001b[38;5;66;03m# Use CLS token representation\u001b[39;00m\n\u001b[32m     22\u001b[39m     cls_output = outputs.last_hidden_state[:, \u001b[32m0\u001b[39m, :]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\thesis\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\thesis\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\thesis\\venv\\Lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:853\u001b[39m, in \u001b[36mXLMRobertaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m    846\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[32m    847\u001b[39m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[32m    848\u001b[39m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[32m    849\u001b[39m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[32m    850\u001b[39m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[32m    851\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m--> \u001b[39m\u001b[32m853\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    854\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    855\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    856\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    857\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    858\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    859\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    860\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    861\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    862\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    863\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    864\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    865\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    866\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    867\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\thesis\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\thesis\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\thesis\\venv\\Lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:607\u001b[39m, in \u001b[36mXLMRobertaEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m    603\u001b[39m     all_hidden_states = all_hidden_states + (hidden_states,)\n\u001b[32m    605\u001b[39m layer_head_mask = head_mask[i] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m607\u001b[39m layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    608\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    609\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    610\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    611\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# as a positional argument for gradient checkpointing\u001b[39;49;00m\n\u001b[32m    612\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    613\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    614\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    615\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    616\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    618\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    619\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\thesis\\venv\\Lib\\site-packages\\transformers\\modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\thesis\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\thesis\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\thesis\\venv\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\thesis\\venv\\Lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:514\u001b[39m, in \u001b[36mXLMRobertaLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, output_attentions, cache_position)\u001b[39m\n\u001b[32m    502\u001b[39m \u001b[38;5;129m@deprecate_kwarg\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mpast_key_value\u001b[39m\u001b[33m\"\u001b[39m, new_name=\u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m, version=\u001b[33m\"\u001b[39m\u001b[33m4.58\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    503\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    504\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    512\u001b[39m     cache_position: Optional[torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    513\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[torch.Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m514\u001b[39m     self_attention_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    520\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    521\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    522\u001b[39m     attention_output = self_attention_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    523\u001b[39m     outputs = self_attention_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add self attentions if we output attention weights\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\thesis\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\thesis\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\thesis\\venv\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\thesis\\venv\\Lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:441\u001b[39m, in \u001b[36mXLMRobertaAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, past_key_values, output_attentions, cache_position)\u001b[39m\n\u001b[32m    430\u001b[39m \u001b[38;5;129m@deprecate_kwarg\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mpast_key_value\u001b[39m\u001b[33m\"\u001b[39m, new_name=\u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m, version=\u001b[33m\"\u001b[39m\u001b[33m4.58\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    431\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    432\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    439\u001b[39m     cache_position: Optional[torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    440\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[torch.Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m441\u001b[39m     self_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    446\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    448\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    449\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    450\u001b[39m     attention_output = \u001b[38;5;28mself\u001b[39m.output(self_outputs[\u001b[32m0\u001b[39m], hidden_states)\n\u001b[32m    451\u001b[39m     outputs = (attention_output,) + self_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\thesis\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\thesis\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\thesis\\venv\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\thesis\\venv\\Lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:343\u001b[39m, in \u001b[36mXLMRobertaSdpaSelfAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, past_key_values, output_attentions, cache_position)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    337\u001b[39m     key_layer = (\n\u001b[32m    338\u001b[39m         \u001b[38;5;28mself\u001b[39m.key(current_states)\n\u001b[32m    339\u001b[39m         .view(bsz, -\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.num_attention_heads, \u001b[38;5;28mself\u001b[39m.attention_head_size)\n\u001b[32m    340\u001b[39m         .transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    341\u001b[39m     )\n\u001b[32m    342\u001b[39m     value_layer = (\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    344\u001b[39m         .view(bsz, -\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.num_attention_heads, \u001b[38;5;28mself\u001b[39m.attention_head_size)\n\u001b[32m    345\u001b[39m         .transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    346\u001b[39m     )\n\u001b[32m    348\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    349\u001b[39m         \u001b[38;5;66;03m# save all key/value_layer to cache to be re-used for fast auto-regressive generation\u001b[39;00m\n\u001b[32m    350\u001b[39m         cache_position = cache_position \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cross_attention \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\thesis\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\thesis\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\thesis\\venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 14. Load checkpoint and evaluate with DETAILED per-class metrics\n",
    "# =====================================================\n",
    "# For Colab: Use the checkpoint from training\n",
    "# For Local: Download checkpoint from Google Drive first\n",
    "# =====================================================\n",
    "\n",
    "# Colab path (after training)\n",
    "# CHECKPOINT_PATH = '/content/drive/MyDrive/thesis_training/checkpoints_v2/xlmr_v2_classweights_best.pt'\n",
    "\n",
    "# Local path (after downloading from Google Drive)\n",
    "CHECKPOINT_PATH = 'checkpoints/xlmr_v2_classweights_best.pt'\n",
    "\n",
    "print(f'Loading checkpoint: {CHECKPOINT_PATH}')\n",
    "best_model_full = MultiTaskXLMRRoberta().to(device)\n",
    "best_model_full.load_state_dict(torch.load(CHECKPOINT_PATH, map_location=device, weights_only=True))\n",
    "print('‚úÖ Model loaded successfully!')\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('=== VALIDATION SET RESULTS ===')\n",
    "print('='*60)\n",
    "val_results_full = evaluate(best_model_full, val_loader, verbose=True,\n",
    "                            ht_class_weights=ht_weights, tg_class_weights=tg_weights, sv_class_weights=sv_weights)\n",
    "print('\\nSummary Metrics:')\n",
    "for k, v in val_results_full.items():\n",
    "    if v is not None and not k.startswith('_'): print(f'  {k}: {v:.4f}')\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('=== TEST SET RESULTS ===')\n",
    "print('='*60)\n",
    "test_results_full = evaluate(best_model_full, test_loader, verbose=True,\n",
    "                             ht_class_weights=ht_weights, tg_class_weights=tg_weights, sv_class_weights=sv_weights)\n",
    "print('\\nSummary Metrics:')\n",
    "for k, v in test_results_full.items():\n",
    "    if v is not None and not k.startswith('_'): print(f'  {k}: {v:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing predictions with CORRECTED labels...\n",
      "\n",
      "Text: ‡¶§‡ßÅ‡¶á ‡¶è‡¶ï‡¶ü‡¶æ ‡¶¨‡ßã‡¶ï‡¶æ\n",
      "  ‚Üí Hate Type: personal_attack, Target: individual, Severity: low\n",
      "\n",
      "Text: ‡¶§‡ßÅ‡¶á ‡¶è‡¶ï‡¶ü‡¶æ ‡¶¨‡ßã‡¶ï‡¶æ\n",
      "  ‚Üí Hate Type: personal_attack, Target: individual, Severity: low\n",
      "\n",
      "Text: You're such an idiot\n",
      "  ‚Üí Hate Type: personal_attack, Target: individual, Severity: low\n",
      "\n",
      "Text: You're such an idiot\n",
      "  ‚Üí Hate Type: personal_attack, Target: individual, Severity: low\n",
      "\n",
      "Text: tui ekta pagol\n",
      "  ‚Üí Hate Type: not_hate/other, Target: individual, Severity: none\n",
      "\n",
      "Text: tui ekta pagol\n",
      "  ‚Üí Hate Type: not_hate/other, Target: individual, Severity: none\n",
      "\n",
      "Text: Have a nice day!\n",
      "  ‚Üí Hate Type: not_hate/other, Target: individual, Severity: none\n",
      "\n",
      "Text: Have a nice day!\n",
      "  ‚Üí Hate Type: not_hate/other, Target: individual, Severity: none\n",
      "\n",
      "Text: ‡¶è‡¶á ‡¶¶‡ßá‡¶∂‡ßá‡¶∞ ‡¶Æ‡¶æ‡¶®‡ßÅ‡¶∑ ‡¶∏‡¶¨ ‡¶ö‡ßã‡¶∞\n",
      "  ‚Üí Hate Type: personal_attack, Target: individual, Severity: low\n",
      "\n",
      "Text: ‡¶è‡¶á ‡¶¶‡ßá‡¶∂‡ßá‡¶∞ ‡¶Æ‡¶æ‡¶®‡ßÅ‡¶∑ ‡¶∏‡¶¨ ‡¶ö‡ßã‡¶∞\n",
      "  ‚Üí Hate Type: personal_attack, Target: individual, Severity: low\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 15. üîÆ Inference: Predict on new text\n",
    "# CORRECTED LABELS based on main.py mapping!\n",
    "HATE_TYPE_LABELS = {\n",
    "    0: 'not_hate/other',\n",
    "    1: 'political',       # Political=1 in main.py\n",
    "    2: 'religious',       # Religious=2 in main.py\n",
    "    3: 'gender',          # Gender abusive=3 in main.py\n",
    "    4: 'personal_attack', # Personal=4 in main.py  ‚Üê THIS IS WHAT MODEL PREDICTS!\n",
    "    5: 'geopolitical'     # Geopolitical=5 in main.py\n",
    "}\n",
    "TARGET_GROUP_LABELS = {0: 'other/none', 1: 'individual', 2: 'organization/group', 3: 'community'}\n",
    "SEVERITY_LABELS = {0: 'none', 1: 'low', 2: 'medium', 3: 'high'}\n",
    "\n",
    "def predict(text, model=None, return_probs=False):\n",
    "    \"\"\"\n",
    "    Predict hate type, target group, and severity for a given text.\n",
    "    \"\"\"\n",
    "    if model is None:\n",
    "        model = best_model_full\n",
    "    \n",
    "    model.eval()\n",
    "    encoding = tokenizer(text, max_length=160, padding='max_length', truncation=True, return_tensors='pt')\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        ht_logits, tg_logits, sv_logits = model(input_ids, attention_mask)\n",
    "    \n",
    "    ht_pred = ht_logits.argmax(dim=1).item()\n",
    "    tg_pred = tg_logits.argmax(dim=1).item()\n",
    "    sv_pred = sv_logits.argmax(dim=1).item()\n",
    "    \n",
    "    result = {\n",
    "        'text': text[:100] + '...' if len(text) > 100 else text,\n",
    "        'hate_type': HATE_TYPE_LABELS[ht_pred],\n",
    "        'target_group': TARGET_GROUP_LABELS[tg_pred],\n",
    "        'severity': SEVERITY_LABELS[sv_pred],\n",
    "    }\n",
    "    \n",
    "    if return_probs:\n",
    "        result['hate_type_probs'] = torch.softmax(ht_logits, dim=1).cpu().numpy()[0]\n",
    "        result['target_group_probs'] = torch.softmax(tg_logits, dim=1).cpu().numpy()[0]\n",
    "        result['severity_probs'] = torch.softmax(sv_logits, dim=1).cpu().numpy()[0]\n",
    "    \n",
    "    return result\n",
    "\n",
    "# === TEST EXAMPLES ===\n",
    "print('Testing predictions with CORRECTED labels...\\n')\n",
    "\n",
    "test_texts = [\n",
    "    \"‡¶§‡ßÅ‡¶á ‡¶è‡¶ï‡¶ü‡¶æ ‡¶¨‡ßã‡¶ï‡¶æ\",                    # Bengali - personal attack\n",
    "    \"You're such an idiot\",            # English - personal attack  \n",
    "    \"tui ekta pagol\",                  # Banglish - personal attack\n",
    "    \"Have a nice day!\",                # English - not hate\n",
    "    \"‡¶è‡¶á ‡¶¶‡ßá‡¶∂‡ßá‡¶∞ ‡¶Æ‡¶æ‡¶®‡ßÅ‡¶∑ ‡¶∏‡¶¨ ‡¶ö‡ßã‡¶∞\",            # Bengali - could be political/community\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    result = predict(text)\n",
    "    print(f\"Text: {result['text']}\")\n",
    "    print(f\"  ‚Üí Hate Type: {result['hate_type']}, Target: {result['target_group']}, Severity: {result['severity']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Text: turja khanki\n",
      "\n",
      "üéØ Predictions:\n",
      "   Hate Type: personal_attack\n",
      "   Target Group: individual\n",
      "   Severity: low\n",
      "\n",
      "üìä Confidence Scores:\n",
      "   Hate Type: {'not_hate/other': '6.50%', 'political': '0.92%', 'religious': '0.65%', 'gender': '0.06%', 'personal_attack': '90.72%', 'geopolitical': '1.16%'}\n",
      "   Target Group: {'other/none': '1.31%', 'individual': '94.73%', 'organization/group': '3.11%', 'community': '0.85%'}\n",
      "   Severity: {'none': '5.55%', 'low': '91.19%', 'medium': '2.90%', 'high': '0.36%'}\n"
     ]
    }
   ],
   "source": [
    "# 16. üéØ Custom Prediction - Enter your own text!\n",
    "# =====================================================\n",
    "# Change the text below and run this cell to predict\n",
    "# =====================================================\n",
    "\n",
    "custom_text = \"turja khanki\"  # ‚Üê Change this to any text you want!\n",
    "\n",
    "result = predict(custom_text, return_probs=True)\n",
    "\n",
    "print(f\"üìù Text: {result['text']}\")\n",
    "print(f\"\\nüéØ Predictions:\")\n",
    "print(f\"   Hate Type: {result['hate_type']}\")\n",
    "print(f\"   Target Group: {result['target_group']}\")\n",
    "print(f\"   Severity: {result['severity']}\")\n",
    "\n",
    "print(f\"\\nüìä Confidence Scores:\")\n",
    "print(f\"   Hate Type: {dict(zip(HATE_TYPE_LABELS.values(), [f'{p:.2%}' for p in result['hate_type_probs']]))}\")\n",
    "print(f\"   Target Group: {dict(zip(TARGET_GROUP_LABELS.values(), [f'{p:.2%}' for p in result['target_group_probs']]))}\")\n",
    "print(f\"   Severity: {dict(zip(SEVERITY_LABELS.values(), [f'{p:.2%}' for p in result['severity_probs']]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing Personal Attack + High Severity Examples\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üìù Bengali - death threat\n",
      "   Text: ‡¶§‡ßÅ‡¶á ‡¶Æ‡¶∞‡ßá ‡¶Ø‡¶æ ‡¶∂‡¶æ‡¶≤‡¶æ\n",
      "   Hate Type: personal_attack ‚úÖ\n",
      "   Target: individual ‚úÖ\n",
      "   Severity: low ‚ö†Ô∏è\n",
      "\n",
      "üìù Bengali - death threat\n",
      "   Text: ‡¶§‡ßÅ‡¶á ‡¶Æ‡¶∞‡ßá ‡¶Ø‡¶æ ‡¶∂‡¶æ‡¶≤‡¶æ\n",
      "   Hate Type: personal_attack ‚úÖ\n",
      "   Target: individual ‚úÖ\n",
      "   Severity: low ‚ö†Ô∏è\n",
      "\n",
      "üìù Bengali - family insult\n",
      "   Text: ‡¶§‡ßã‡¶∞ ‡¶¨‡¶æ‡¶™-‡¶Æ‡¶æ ‡¶§‡ßã‡¶ï‡ßá ‡¶ï‡ßá‡¶® ‡¶ú‡¶®‡ßç‡¶Æ ‡¶¶‡¶ø‡¶≤\n",
      "   Hate Type: personal_attack ‚úÖ\n",
      "   Target: individual ‚úÖ\n",
      "   Severity: low ‚ö†Ô∏è\n",
      "\n",
      "üìù Bengali - family insult\n",
      "   Text: ‡¶§‡ßã‡¶∞ ‡¶¨‡¶æ‡¶™-‡¶Æ‡¶æ ‡¶§‡ßã‡¶ï‡ßá ‡¶ï‡ßá‡¶® ‡¶ú‡¶®‡ßç‡¶Æ ‡¶¶‡¶ø‡¶≤\n",
      "   Hate Type: personal_attack ‚úÖ\n",
      "   Target: individual ‚úÖ\n",
      "   Severity: low ‚ö†Ô∏è\n",
      "\n",
      "üìù Bengali - dehumanizing\n",
      "   Text: ‡¶§‡ßÅ‡¶á ‡¶è‡¶ï‡¶ü‡¶æ ‡¶ú‡¶æ‡¶®‡ßã‡¶Ø‡¶º‡¶æ‡¶∞\n",
      "   Hate Type: personal_attack ‚úÖ\n",
      "   Target: individual ‚úÖ\n",
      "   Severity: low ‚ö†Ô∏è\n",
      "\n",
      "üìù Bengali - dehumanizing\n",
      "   Text: ‡¶§‡ßÅ‡¶á ‡¶è‡¶ï‡¶ü‡¶æ ‡¶ú‡¶æ‡¶®‡ßã‡¶Ø‡¶º‡¶æ‡¶∞\n",
      "   Hate Type: personal_attack ‚úÖ\n",
      "   Target: individual ‚úÖ\n",
      "   Severity: low ‚ö†Ô∏è\n",
      "\n",
      "üìù English - death wish\n",
      "   Text: I hope you die alone\n",
      "   Hate Type: personal_attack ‚úÖ\n",
      "   Target: individual ‚úÖ\n",
      "   Severity: low ‚ö†Ô∏è\n",
      "\n",
      "üìù English - death wish\n",
      "   Text: I hope you die alone\n",
      "   Hate Type: personal_attack ‚úÖ\n",
      "   Target: individual ‚úÖ\n",
      "   Severity: low ‚ö†Ô∏è\n",
      "\n",
      "üìù English - dehumanizing\n",
      "   Text: You're a worthless piece of garbage\n",
      "   Hate Type: personal_attack ‚úÖ\n",
      "   Target: individual ‚úÖ\n",
      "   Severity: low ‚ö†Ô∏è\n",
      "\n",
      "üìù English - dehumanizing\n",
      "   Text: You're a worthless piece of garbage\n",
      "   Hate Type: personal_attack ‚úÖ\n",
      "   Target: individual ‚úÖ\n",
      "   Severity: low ‚ö†Ô∏è\n",
      "\n",
      "üìù English - suicide incitement\n",
      "   Text: Kill yourself you pathetic loser\n",
      "   Hate Type: personal_attack ‚úÖ\n",
      "   Target: individual ‚úÖ\n",
      "   Severity: low ‚ö†Ô∏è\n",
      "\n",
      "üìù English - suicide incitement\n",
      "   Text: Kill yourself you pathetic loser\n",
      "   Hate Type: personal_attack ‚úÖ\n",
      "   Target: individual ‚úÖ\n",
      "   Severity: low ‚ö†Ô∏è\n",
      "\n",
      "üìù Banglish - animal slur\n",
      "   Text: tui ekta shala kutta\n",
      "   Hate Type: not_hate/other ‚ùå\n",
      "   Target: individual ‚úÖ\n",
      "   Severity: none ‚ö†Ô∏è\n",
      "\n",
      "üìù Banglish - animal slur\n",
      "   Text: tui ekta shala kutta\n",
      "   Hate Type: not_hate/other ‚ùå\n",
      "   Target: individual ‚úÖ\n",
      "   Severity: none ‚ö†Ô∏è\n",
      "\n",
      "üìù Banglish - blame\n",
      "   Text: tor jonno duniya kharap\n",
      "   Hate Type: personal_attack ‚úÖ\n",
      "   Target: individual ‚úÖ\n",
      "   Severity: low ‚ö†Ô∏è\n",
      "\n",
      "üìù Banglish - blame\n",
      "   Text: tor jonno duniya kharap\n",
      "   Hate Type: personal_attack ‚úÖ\n",
      "   Target: individual ‚úÖ\n",
      "   Severity: low ‚ö†Ô∏è\n",
      "\n",
      "üìù Banglish - death threat\n",
      "   Text: tui moira ja harami\n",
      "   Hate Type: personal_attack ‚úÖ\n",
      "   Target: individual ‚úÖ\n",
      "   Severity: low ‚ö†Ô∏è\n",
      "\n",
      "üìù Banglish - death threat\n",
      "   Text: tui moira ja harami\n",
      "   Hate Type: personal_attack ‚úÖ\n",
      "   Target: individual ‚úÖ\n",
      "   Severity: low ‚ö†Ô∏è\n",
      "\n",
      "üìù Bengali - neutral\n",
      "   Text: ‡¶Ü‡¶ú‡¶ï‡ßá ‡¶Ü‡¶¨‡¶π‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ ‡¶ñ‡ßÅ‡¶¨ ‡¶∏‡ßÅ‡¶®‡ßç‡¶¶‡¶∞\n",
      "   Hate Type: personal_attack ‚úÖ\n",
      "   Target: individual ‚úÖ\n",
      "   Severity: low ‚ö†Ô∏è\n",
      "\n",
      "üìù Bengali - neutral\n",
      "   Text: ‡¶Ü‡¶ú‡¶ï‡ßá ‡¶Ü‡¶¨‡¶π‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ ‡¶ñ‡ßÅ‡¶¨ ‡¶∏‡ßÅ‡¶®‡ßç‡¶¶‡¶∞\n",
      "   Hate Type: personal_attack ‚úÖ\n",
      "   Target: individual ‚úÖ\n",
      "   Severity: low ‚ö†Ô∏è\n",
      "\n",
      "üìù English - neutral\n",
      "   Text: Have a nice day!\n",
      "   Hate Type: not_hate/other ‚ùå\n",
      "   Target: individual ‚úÖ\n",
      "   Severity: none ‚ö†Ô∏è\n",
      "\n",
      "================================================================================\n",
      "Legend: ‚úÖ = Expected | ‚ö†Ô∏è = Different | ‚ùå = Wrong category\n",
      "\n",
      "üìù English - neutral\n",
      "   Text: Have a nice day!\n",
      "   Hate Type: not_hate/other ‚ùå\n",
      "   Target: individual ‚úÖ\n",
      "   Severity: none ‚ö†Ô∏è\n",
      "\n",
      "================================================================================\n",
      "Legend: ‚úÖ = Expected | ‚ö†Ô∏è = Different | ‚ùå = Wrong category\n"
     ]
    }
   ],
   "source": [
    "# 17. üß™ Test Personal Attack + High Severity Examples\n",
    "# =====================================================\n",
    "# Expected: hate_type=personal_attack, target=individual, severity=high\n",
    "# =====================================================\n",
    "\n",
    "test_examples = [\n",
    "    # Bengali\n",
    "    (\"‡¶§‡ßÅ‡¶á ‡¶Æ‡¶∞‡ßá ‡¶Ø‡¶æ ‡¶∂‡¶æ‡¶≤‡¶æ\", \"Bengali - death threat\"),\n",
    "    (\"‡¶§‡ßã‡¶∞ ‡¶¨‡¶æ‡¶™-‡¶Æ‡¶æ ‡¶§‡ßã‡¶ï‡ßá ‡¶ï‡ßá‡¶® ‡¶ú‡¶®‡ßç‡¶Æ ‡¶¶‡¶ø‡¶≤\", \"Bengali - family insult\"),\n",
    "    (\"‡¶§‡ßÅ‡¶á ‡¶è‡¶ï‡¶ü‡¶æ ‡¶ú‡¶æ‡¶®‡ßã‡¶Ø‡¶º‡¶æ‡¶∞\", \"Bengali - dehumanizing\"),\n",
    "    \n",
    "    # English\n",
    "    (\"I hope you die alone\", \"English - death wish\"),\n",
    "    (\"You're a worthless piece of garbage\", \"English - dehumanizing\"),\n",
    "    (\"Kill yourself you pathetic loser\", \"English - suicide incitement\"),\n",
    "    \n",
    "    # Banglish\n",
    "    (\"tui ekta shala kutta\", \"Banglish - animal slur\"),\n",
    "    (\"tor jonno duniya kharap\", \"Banglish - blame\"),\n",
    "    (\"tui moira ja harami\", \"Banglish - death threat\"),\n",
    "    \n",
    "    # Control - NOT hate\n",
    "    (\"‡¶Ü‡¶ú‡¶ï‡ßá ‡¶Ü‡¶¨‡¶π‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ ‡¶ñ‡ßÅ‡¶¨ ‡¶∏‡ßÅ‡¶®‡ßç‡¶¶‡¶∞\", \"Bengali - neutral\"),\n",
    "    (\"Have a nice day!\", \"English - neutral\"),\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing Personal Attack + High Severity Examples\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for text, description in test_examples:\n",
    "    result = predict(text)\n",
    "    \n",
    "    # Check if predictions match expected\n",
    "    is_personal = \"‚úÖ\" if result['hate_type'] == 'personal_attack' else \"‚ùå\"\n",
    "    is_individual = \"‚úÖ\" if result['target_group'] == 'individual' else \"‚ö†Ô∏è\"\n",
    "    is_high = \"‚úÖ\" if result['severity'] == 'high' else \"‚ö†Ô∏è\"\n",
    "    \n",
    "    print(f\"\\nüìù {description}\")\n",
    "    print(f\"   Text: {text}\")\n",
    "    print(f\"   Hate Type: {result['hate_type']} {is_personal}\")\n",
    "    print(f\"   Target: {result['target_group']} {is_individual}\")\n",
    "    print(f\"   Severity: {result['severity']} {is_high}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Legend: ‚úÖ = Expected | ‚ö†Ô∏è = Different | ‚ùå = Wrong category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Testing Different Target Groups\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üìù English - individual insult\n",
      "   Text: You're an idiot\n",
      "   Expected Target: individual\n",
      "   Predicted: individual ‚úÖ\n",
      "   (Hate Type: personal_attack, Severity: low)\n",
      "\n",
      "üìù English - individual insult\n",
      "   Text: You're an idiot\n",
      "   Expected Target: individual\n",
      "   Predicted: individual ‚úÖ\n",
      "   (Hate Type: personal_attack, Severity: low)\n",
      "\n",
      "üìù Bengali - individual insult\n",
      "   Text: ‡¶§‡ßÅ‡¶á ‡¶è‡¶ï‡¶ü‡¶æ ‡¶¨‡ßã‡¶ï‡¶æ\n",
      "   Expected Target: individual\n",
      "   Predicted: individual ‚úÖ\n",
      "   (Hate Type: personal_attack, Severity: low)\n",
      "\n",
      "üìù Bengali - individual insult\n",
      "   Text: ‡¶§‡ßÅ‡¶á ‡¶è‡¶ï‡¶ü‡¶æ ‡¶¨‡ßã‡¶ï‡¶æ\n",
      "   Expected Target: individual\n",
      "   Predicted: individual ‚úÖ\n",
      "   (Hate Type: personal_attack, Severity: low)\n",
      "\n",
      "üìù English - individual\n",
      "   Text: That guy is a complete moron\n",
      "   Expected Target: individual\n",
      "   Predicted: individual ‚úÖ\n",
      "   (Hate Type: personal_attack, Severity: low)\n",
      "\n",
      "üìù English - individual\n",
      "   Text: That guy is a complete moron\n",
      "   Expected Target: individual\n",
      "   Predicted: individual ‚úÖ\n",
      "   (Hate Type: personal_attack, Severity: low)\n",
      "\n",
      "üìù English - organization\n",
      "   Text: This company is full of thieves\n",
      "   Expected Target: organization/group\n",
      "   Predicted: organization/group ‚úÖ\n",
      "   (Hate Type: not_hate/other, Severity: low)\n",
      "\n",
      "üìù English - organization\n",
      "   Text: This company is full of thieves\n",
      "   Expected Target: organization/group\n",
      "   Predicted: organization/group ‚úÖ\n",
      "   (Hate Type: not_hate/other, Severity: low)\n",
      "\n",
      "üìù English - government/org\n",
      "   Text: The government is corrupt and useless\n",
      "   Expected Target: organization/group\n",
      "   Predicted: organization/group ‚úÖ\n",
      "   (Hate Type: personal_attack, Severity: low)\n",
      "\n",
      "üìù English - government/org\n",
      "   Text: The government is corrupt and useless\n",
      "   Expected Target: organization/group\n",
      "   Predicted: organization/group ‚úÖ\n",
      "   (Hate Type: personal_attack, Severity: low)\n",
      "\n",
      "üìù Bengali - political party\n",
      "   Text: ‡¶è‡¶á ‡¶¶‡¶≤ ‡¶∏‡¶¨ ‡¶ö‡ßã‡¶∞\n",
      "   Expected Target: organization/group\n",
      "   Predicted: individual ‚ùå\n",
      "   (Hate Type: personal_attack, Severity: low)\n",
      "\n",
      "üìù Bengali - political party\n",
      "   Text: ‡¶è‡¶á ‡¶¶‡¶≤ ‡¶∏‡¶¨ ‡¶ö‡ßã‡¶∞\n",
      "   Expected Target: organization/group\n",
      "   Predicted: individual ‚ùå\n",
      "   (Hate Type: personal_attack, Severity: low)\n",
      "\n",
      "üìù English - company\n",
      "   Text: Facebook is destroying society\n",
      "   Expected Target: organization/group\n",
      "   Predicted: individual ‚ùå\n",
      "   (Hate Type: not_hate/other, Severity: low)\n",
      "\n",
      "üìù English - company\n",
      "   Text: Facebook is destroying society\n",
      "   Expected Target: organization/group\n",
      "   Predicted: individual ‚ùå\n",
      "   (Hate Type: not_hate/other, Severity: low)\n",
      "\n",
      "üìù English - religious community\n",
      "   Text: All Muslims are terrorists\n",
      "   Expected Target: community\n",
      "   Predicted: organization/group ‚ùå\n",
      "   (Hate Type: not_hate/other, Severity: low)\n",
      "\n",
      "üìù English - religious community\n",
      "   Text: All Muslims are terrorists\n",
      "   Expected Target: community\n",
      "   Predicted: organization/group ‚ùå\n",
      "   (Hate Type: not_hate/other, Severity: low)\n",
      "\n",
      "üìù Bengali - religious community\n",
      "   Text: ‡¶π‡¶ø‡¶®‡ßç‡¶¶‡ßÅ‡¶∞‡¶æ ‡¶∏‡¶¨ ‡¶ñ‡¶æ‡¶∞‡¶æ‡¶™\n",
      "   Expected Target: community\n",
      "   Predicted: community ‚úÖ\n",
      "   (Hate Type: religious, Severity: low)\n",
      "\n",
      "üìù Bengali - religious community\n",
      "   Text: ‡¶π‡¶ø‡¶®‡ßç‡¶¶‡ßÅ‡¶∞‡¶æ ‡¶∏‡¶¨ ‡¶ñ‡¶æ‡¶∞‡¶æ‡¶™\n",
      "   Expected Target: community\n",
      "   Predicted: community ‚úÖ\n",
      "   (Hate Type: religious, Severity: low)\n",
      "\n",
      "üìù English - gender community\n",
      "   Text: Women belong in the kitchen\n",
      "   Expected Target: community\n",
      "   Predicted: organization/group ‚ùå\n",
      "   (Hate Type: not_hate/other, Severity: low)\n",
      "\n",
      "üìù English - gender community\n",
      "   Text: Women belong in the kitchen\n",
      "   Expected Target: community\n",
      "   Predicted: organization/group ‚ùå\n",
      "   (Hate Type: not_hate/other, Severity: low)\n",
      "\n",
      "üìù English - ethnic community\n",
      "   Text: Immigrants are ruining this country\n",
      "   Expected Target: community\n",
      "   Predicted: organization/group ‚ùå\n",
      "   (Hate Type: not_hate/other, Severity: low)\n",
      "\n",
      "üìù English - ethnic community\n",
      "   Text: Immigrants are ruining this country\n",
      "   Expected Target: community\n",
      "   Predicted: organization/group ‚ùå\n",
      "   (Hate Type: not_hate/other, Severity: low)\n",
      "\n",
      "üìù English - occupational group\n",
      "   Text: All politicians are liars\n",
      "   Expected Target: community\n",
      "   Predicted: organization/group ‚ùå\n",
      "   (Hate Type: not_hate/other, Severity: low)\n",
      "\n",
      "üìù English - occupational group\n",
      "   Text: All politicians are liars\n",
      "   Expected Target: community\n",
      "   Predicted: organization/group ‚ùå\n",
      "   (Hate Type: not_hate/other, Severity: low)\n",
      "\n",
      "üìù English - neutral\n",
      "   Text: The weather is nice today\n",
      "   Expected Target: other/none\n",
      "   Predicted: individual ‚ùå\n",
      "   (Hate Type: not_hate/other, Severity: none)\n",
      "\n",
      "üìù English - neutral\n",
      "   Text: The weather is nice today\n",
      "   Expected Target: other/none\n",
      "   Predicted: individual ‚ùå\n",
      "   (Hate Type: not_hate/other, Severity: none)\n",
      "\n",
      "üìù Bengali - neutral\n",
      "   Text: ‡¶Ü‡¶ú‡¶ï‡ßá ‡¶Ü‡¶¨‡¶π‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ ‡¶≠‡¶æ‡¶≤‡ßã\n",
      "   Expected Target: other/none\n",
      "   Predicted: individual ‚ùå\n",
      "   (Hate Type: personal_attack, Severity: low)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üìä Accuracy by Target Group:\n",
      "   individual: 3/3 (100%)\n",
      "   organization/group: 2/4 (50%)\n",
      "   community: 1/5 (20%)\n",
      "   other/none: 0/2 (0%)\n",
      "\n",
      "üìù Bengali - neutral\n",
      "   Text: ‡¶Ü‡¶ú‡¶ï‡ßá ‡¶Ü‡¶¨‡¶π‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ ‡¶≠‡¶æ‡¶≤‡ßã\n",
      "   Expected Target: other/none\n",
      "   Predicted: individual ‚ùå\n",
      "   (Hate Type: personal_attack, Severity: low)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üìä Accuracy by Target Group:\n",
      "   individual: 3/3 (100%)\n",
      "   organization/group: 2/4 (50%)\n",
      "   community: 1/5 (20%)\n",
      "   other/none: 0/2 (0%)\n"
     ]
    }
   ],
   "source": [
    "# 18. üéØ Test Different Target Groups\n",
    "# =====================================================\n",
    "# Testing: individual, organization/group, community targets\n",
    "# =====================================================\n",
    "\n",
    "target_examples = [\n",
    "    # === INDIVIDUAL (target_group=1) ===\n",
    "    (\"You're an idiot\", \"English - individual insult\", \"individual\"),\n",
    "    (\"‡¶§‡ßÅ‡¶á ‡¶è‡¶ï‡¶ü‡¶æ ‡¶¨‡ßã‡¶ï‡¶æ\", \"Bengali - individual insult\", \"individual\"),\n",
    "    (\"That guy is a complete moron\", \"English - individual\", \"individual\"),\n",
    "    \n",
    "    # === ORGANIZATION/GROUP (target_group=2) ===\n",
    "    (\"This company is full of thieves\", \"English - organization\", \"organization/group\"),\n",
    "    (\"The government is corrupt and useless\", \"English - government/org\", \"organization/group\"),\n",
    "    (\"‡¶è‡¶á ‡¶¶‡¶≤ ‡¶∏‡¶¨ ‡¶ö‡ßã‡¶∞\", \"Bengali - political party\", \"organization/group\"),\n",
    "    (\"Facebook is destroying society\", \"English - company\", \"organization/group\"),\n",
    "    \n",
    "    # === COMMUNITY (target_group=3) ===\n",
    "    (\"All Muslims are terrorists\", \"English - religious community\", \"community\"),\n",
    "    (\"‡¶π‡¶ø‡¶®‡ßç‡¶¶‡ßÅ‡¶∞‡¶æ ‡¶∏‡¶¨ ‡¶ñ‡¶æ‡¶∞‡¶æ‡¶™\", \"Bengali - religious community\", \"community\"),\n",
    "    (\"Women belong in the kitchen\", \"English - gender community\", \"community\"),\n",
    "    (\"Immigrants are ruining this country\", \"English - ethnic community\", \"community\"),\n",
    "    (\"All politicians are liars\", \"English - occupational group\", \"community\"),\n",
    "    \n",
    "    # === OTHER/NONE (target_group=0) - Neutral ===\n",
    "    (\"The weather is nice today\", \"English - neutral\", \"other/none\"),\n",
    "    (\"‡¶Ü‡¶ú‡¶ï‡ßá ‡¶Ü‡¶¨‡¶π‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ ‡¶≠‡¶æ‡¶≤‡ßã\", \"Bengali - neutral\", \"other/none\"),\n",
    "]\n",
    "\n",
    "print(\"üéØ Testing Different Target Groups\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Track accuracy per target\n",
    "correct = {'individual': 0, 'organization/group': 0, 'community': 0, 'other/none': 0}\n",
    "total = {'individual': 0, 'organization/group': 0, 'community': 0, 'other/none': 0}\n",
    "\n",
    "for text, description, expected_target in target_examples:\n",
    "    result = predict(text)\n",
    "    \n",
    "    total[expected_target] += 1\n",
    "    is_correct = result['target_group'] == expected_target\n",
    "    if is_correct:\n",
    "        correct[expected_target] += 1\n",
    "    \n",
    "    icon = \"‚úÖ\" if is_correct else \"‚ùå\"\n",
    "    \n",
    "    print(f\"\\nüìù {description}\")\n",
    "    print(f\"   Text: {text}\")\n",
    "    print(f\"   Expected Target: {expected_target}\")\n",
    "    print(f\"   Predicted: {result['target_group']} {icon}\")\n",
    "    print(f\"   (Hate Type: {result['hate_type']}, Severity: {result['severity']})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nüìä Accuracy by Target Group:\")\n",
    "for target in ['individual', 'organization/group', 'community', 'other/none']:\n",
    "    acc = correct[target] / total[target] * 100 if total[target] > 0 else 0\n",
    "    print(f\"   {target}: {correct[target]}/{total[target]} ({acc:.0f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä TRAINING DATA CLASS DISTRIBUTION ANALYSIS\n",
      "\n",
      "============================================================\n",
      "\n",
      "üè∑Ô∏è HATE TYPE (n=11339 samples with labels)\n",
      "----------------------------------------\n",
      "  0 (not_hate/other ):  5331 ( 47.0%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  1 (political      ):   489 (  4.3%) ‚ñà‚ñà\n",
      "  2 (religious      ):   553 (  4.9%) ‚ñà‚ñà\n",
      "  4 (personal_attack):  3932 ( 34.7%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  5 (geopolitical   ):  1034 (  9.1%) ‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "üéØ TARGET GROUP (n=5176 samples with labels)\n",
      "----------------------------------------\n",
      "  0 (other/none        ):   228 (  4.4%) ‚ñà‚ñà\n",
      "  1 (individual        ):  2978 ( 57.5%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  2 (organization/group):  1306 ( 25.2%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  3 (community         ):   664 ( 12.8%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "‚ö†Ô∏è SEVERITY (n=45518 samples with labels)\n",
      "----------------------------------------\n",
      "  0 (none    ): 25592 ( 56.2%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  1 (low     ): 19700 ( 43.3%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  2 (medium  ):   226 (  0.5%) \n",
      "\n",
      "============================================================\n",
      "\n",
      "üìã DIAGNOSIS:\n",
      "  - If one class dominates (>50%), model will over-predict it\n",
      "  - Classes with <5% samples are hard to learn\n",
      "  - Missing labels (-1) reduce effective training data per task\n"
     ]
    }
   ],
   "source": [
    "# 19. üìä Analyze Training Data Distribution (Diagnose Model Issues)\n",
    "# =====================================================\n",
    "\n",
    "print(\"üìä TRAINING DATA CLASS DISTRIBUTION ANALYSIS\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Filter only valid labels (not -1)\n",
    "train_with_ht = train_df[train_df['hate_type'] != -1]\n",
    "train_with_tg = train_df[train_df['target_group'] != -1]\n",
    "train_with_sv = train_df[train_df['severity'] != -1]\n",
    "\n",
    "print(f\"\\nüè∑Ô∏è HATE TYPE (n={len(train_with_ht)} samples with labels)\")\n",
    "print(\"-\" * 40)\n",
    "ht_counts = train_with_ht['hate_type'].value_counts().sort_index()\n",
    "for idx, count in ht_counts.items():\n",
    "    pct = count / len(train_with_ht) * 100\n",
    "    label = HATE_TYPE_LABELS.get(idx, f'unknown_{idx}')\n",
    "    bar = \"‚ñà\" * int(pct / 2)\n",
    "    print(f\"  {idx} ({label:15}): {count:5} ({pct:5.1f}%) {bar}\")\n",
    "\n",
    "print(f\"\\nüéØ TARGET GROUP (n={len(train_with_tg)} samples with labels)\")\n",
    "print(\"-\" * 40)\n",
    "tg_counts = train_with_tg['target_group'].value_counts().sort_index()\n",
    "for idx, count in tg_counts.items():\n",
    "    pct = count / len(train_with_tg) * 100\n",
    "    label = TARGET_GROUP_LABELS.get(idx, f'unknown_{idx}')\n",
    "    bar = \"‚ñà\" * int(pct / 2)\n",
    "    print(f\"  {idx} ({label:18}): {count:5} ({pct:5.1f}%) {bar}\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è SEVERITY (n={len(train_with_sv)} samples with labels)\")\n",
    "print(\"-\" * 40)\n",
    "sv_counts = train_with_sv['severity'].value_counts().sort_index()\n",
    "for idx, count in sv_counts.items():\n",
    "    pct = count / len(train_with_sv) * 100\n",
    "    label = SEVERITY_LABELS.get(idx, f'unknown_{idx}')\n",
    "    bar = \"‚ñà\" * int(pct / 2)\n",
    "    print(f\"  {idx} ({label:8}): {count:5} ({pct:5.1f}%) {bar}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\\nüìã DIAGNOSIS:\")\n",
    "print(\"  - If one class dominates (>50%), model will over-predict it\")\n",
    "print(\"  - Classes with <5% samples are hard to learn\")\n",
    "print(\"  - Missing labels (-1) reduce effective training data per task\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
