{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeeb1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Install dependencies and verify GPU\n",
    "!pip install transformers scikit-learn wandb sentencepiece -q\n",
    "\n",
    "import torch\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')\n",
    "else:\n",
    "    print('‚ö†Ô∏è NO GPU DETECTED! Go to Runtime ‚Üí Change runtime type ‚Üí GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4798ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Mount Google Drive & Setup (RUN THIS FIRST!)\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Setup paths\n",
    "DRIVE_FOLDER = '/content/drive/MyDrive/thesis_training'\n",
    "os.makedirs(DRIVE_FOLDER, exist_ok=True)\n",
    "os.makedirs(f'{DRIVE_FOLDER}/checkpoints_enhanced', exist_ok=True)\n",
    "os.makedirs('dataset', exist_ok=True)\n",
    "\n",
    "# =====================================================\n",
    "# CHOOSE YOUR DATASET:\n",
    "# - FILTERED: High quality labels, smaller size (25K), 86% hate_type coverage\n",
    "# - ENHANCED: Auto-labeled, larger size (75K), 95% hate_type coverage ‚≠ê RECOMMENDED\n",
    "# =====================================================\n",
    "\n",
    "# OPTION 1: Filtered dataset (no toxic_comments) - Fast training\n",
    "# DRIVE_DATASET = f'{DRIVE_FOLDER}/UNIFIED_ALL_SPLIT_FILTERED.csv'\n",
    "\n",
    "# OPTION 2: Enhanced dataset (with auto-labeled toxic_comments) - Best performance ‚≠ê\n",
    "DRIVE_DATASET = f'{DRIVE_FOLDER}/UNIFIED_ALL_SPLIT_ENHANCED.csv'\n",
    "\n",
    "LOCAL_DATASET = 'dataset/UNIFIED_ALL_SPLIT.csv'\n",
    "\n",
    "if os.path.exists(DRIVE_DATASET):\n",
    "    !cp '{DRIVE_DATASET}' '{LOCAL_DATASET}'\n",
    "    print('‚úÖ Dataset loaded from Google Drive!')\n",
    "    print(f'   Source: {DRIVE_DATASET.split(\"/\")[-1]}')\n",
    "else:\n",
    "    print(f'‚ùå Dataset not found! Please upload to:')\n",
    "    print(f'   Google Drive ‚Üí My Drive ‚Üí thesis_training/')\n",
    "    print(f'   File needed: UNIFIED_ALL_SPLIT_ENHANCED.csv')\n",
    "    print(f'   (or UNIFIED_ALL_SPLIT_FILTERED.csv if using filtered option)')\n",
    "\n",
    "# Checkpoint directory (saves to Drive - survives disconnects!)\n",
    "CHECKPOINT_DIR = f'{DRIVE_FOLDER}/checkpoints_enhanced/'\n",
    "print(f'‚úÖ Checkpoints will save to: {CHECKPOINT_DIR}')\n",
    "\n",
    "!ls -lh dataset/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e55b4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load and verify dataset\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('dataset/UNIFIED_ALL_SPLIT.csv')\n",
    "\n",
    "print(f'üìä Dataset: {len(df)} samples from {df[\"source_dataset\"].nunique()} sources')\n",
    "print('='*60)\n",
    "\n",
    "# Split sizes\n",
    "train_df = df[df['split'] == 'train']\n",
    "val_df = df[df['split'] == 'val']\n",
    "test_df = df[df['split'] == 'test']\n",
    "print(f'Train: {len(train_df)} | Val: {len(val_df)} | Test: {len(test_df)}\\n')\n",
    "\n",
    "# Label coverage\n",
    "ht_valid = df[df['hate_type'] != -1]\n",
    "tg_valid = df[df['target_group'] != -1]\n",
    "print(f'Label Coverage:')\n",
    "print(f'  Hate Type:    {len(ht_valid)/len(df)*100:.1f}% ({len(ht_valid)}/{len(df)})')\n",
    "print(f'  Target Group: {len(tg_valid)/len(df)*100:.1f}% ({len(tg_valid)}/{len(df)})')\n",
    "\n",
    "# Key stats\n",
    "print(f'\\nClass Distribution:')\n",
    "print(f'  Hate Type:    {ht_valid[\"hate_type\"].value_counts().to_dict()}')\n",
    "print(f'  Target Group: {tg_valid[\"target_group\"].value_counts().to_dict()}')\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "if 'toxic_comments_labeled' in df['source_dataset'].values:\n",
    "    print('‚úÖ ENHANCED dataset loaded (auto-labeled toxic_comments)')\n",
    "    print('   ‚Üí 95% hate_type | 77% target_group coverage')\n",
    "elif len(df) < 30000:\n",
    "    print('‚úÖ FILTERED dataset loaded (no toxic_comments)')\n",
    "    print('   ‚Üí 86% hate_type | 34% target_group coverage')\n",
    "print('='*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8ce421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. HateDataset: PyTorch Dataset with tokenization and masking\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import XLMRobertaTokenizer\n",
    "\n",
    "class HateDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=160):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        text = str(row['text'])\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        hate_type = int(row['hate_type'])\n",
    "        target_group = int(row['target_group'])\n",
    "        severity = int(row['severity'])\n",
    "        \n",
    "        hate_type_mask = hate_type != -1\n",
    "        target_group_mask = target_group != -1\n",
    "        severity_mask = severity != -1\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'hate_type': torch.tensor(max(0, hate_type), dtype=torch.long),\n",
    "            'target_group': torch.tensor(max(0, target_group), dtype=torch.long),\n",
    "            'severity': torch.tensor(max(0, severity), dtype=torch.long),\n",
    "            'hate_type_mask': torch.tensor(hate_type_mask, dtype=torch.bool),\n",
    "            'target_group_mask': torch.tensor(target_group_mask, dtype=torch.bool),\n",
    "            'severity_mask': torch.tensor(severity_mask, dtype=torch.bool),\n",
    "        }\n",
    "\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-large')\n",
    "print('Tokenizer loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d42467d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. MultiTaskXLMRRoberta Model\n",
    "import torch.nn as nn\n",
    "from transformers import XLMRobertaModel\n",
    "\n",
    "class MultiTaskXLMRRoberta(nn.Module):\n",
    "    def __init__(self, model_name='xlm-roberta-large', dropout=0.2,\n",
    "                 n_hate_type=6, n_target_group=4, n_severity=4):\n",
    "        super().__init__()\n",
    "        self.backbone = XLMRobertaModel.from_pretrained(model_name)\n",
    "        hidden_size = self.backbone.config.hidden_size\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.hate_type_head = nn.Linear(hidden_size, n_hate_type)\n",
    "        self.target_group_head = nn.Linear(hidden_size, n_target_group)\n",
    "        self.severity_head = nn.Linear(hidden_size, n_severity)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        cls_output = self.dropout(cls_output)\n",
    "        \n",
    "        return (\n",
    "            self.hate_type_head(cls_output),\n",
    "            self.target_group_head(cls_output),\n",
    "            self.severity_head(cls_output)\n",
    "        )\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a9dccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Loss and evaluation functions WITH CLASS WEIGHTS\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "def compute_class_weights(df, column, n_classes, smoothing=0.1):\n",
    "    \"\"\"Compute inverse frequency class weights with smoothing.\"\"\"\n",
    "    valid = df[df[column] != -1][column]\n",
    "    counts = valid.value_counts().reindex(range(n_classes), fill_value=1).values\n",
    "    weights = 1.0 / (counts + smoothing * len(valid))\n",
    "    weights = weights / weights.sum() * n_classes\n",
    "    return torch.tensor(weights, dtype=torch.float32)\n",
    "\n",
    "def multitask_loss(hate_type_logits, target_group_logits, severity_logits,\n",
    "                   targets, masks, task_weights=(1.0, 1.0, 1.0),\n",
    "                   ht_class_weights=None, tg_class_weights=None, sv_class_weights=None):\n",
    "    \"\"\"Masked cross-entropy loss with optional class weights.\"\"\"\n",
    "    total_loss = 0.0\n",
    "    n_tasks = 0\n",
    "    \n",
    "    ht_mask = masks['hate_type'].bool()\n",
    "    if ht_mask.any():\n",
    "        loss_ht = F.cross_entropy(hate_type_logits[ht_mask], targets['hate_type'][ht_mask], weight=ht_class_weights)\n",
    "        total_loss += task_weights[0] * loss_ht\n",
    "        n_tasks += 1\n",
    "    \n",
    "    tg_mask = masks['target_group'].bool()\n",
    "    if tg_mask.any():\n",
    "        loss_tg = F.cross_entropy(target_group_logits[tg_mask], targets['target_group'][tg_mask], weight=tg_class_weights)\n",
    "        total_loss += task_weights[1] * loss_tg\n",
    "        n_tasks += 1\n",
    "    \n",
    "    sv_mask = masks['severity'].bool()\n",
    "    if sv_mask.any():\n",
    "        loss_sv = F.cross_entropy(severity_logits[sv_mask], targets['severity'][sv_mask], weight=sv_class_weights)\n",
    "        total_loss += task_weights[2] * loss_sv\n",
    "        n_tasks += 1\n",
    "    \n",
    "    return total_loss / max(1, n_tasks)\n",
    "\n",
    "def move_batch_to_device(batch):\n",
    "    return {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "def evaluate(model, data_loader, task_weights=(1.0, 1.0, 1.0),\n",
    "             ht_class_weights=None, tg_class_weights=None, sv_class_weights=None, verbose=False):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    n_batches = 0\n",
    "    all_preds = {'hate_type': [], 'target_group': [], 'severity': []}\n",
    "    all_labels = {'hate_type': [], 'target_group': [], 'severity': []}\n",
    "    all_masks = {'hate_type': [], 'target_group': [], 'severity': []}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            batch = move_batch_to_device(batch)\n",
    "            ht_logits, tg_logits, sv_logits = model(batch['input_ids'], batch['attention_mask'])\n",
    "            targets = {k: batch[k] for k in ['hate_type', 'target_group', 'severity']}\n",
    "            masks = {k: batch[f'{k}_mask'] for k in targets.keys()}\n",
    "            loss = multitask_loss(ht_logits, tg_logits, sv_logits, targets, masks, task_weights,\n",
    "                                  ht_class_weights, tg_class_weights, sv_class_weights)\n",
    "            total_loss += loss.item()\n",
    "            n_batches += 1\n",
    "            all_preds['hate_type'].extend(ht_logits.argmax(dim=1).cpu().numpy())\n",
    "            all_preds['target_group'].extend(tg_logits.argmax(dim=1).cpu().numpy())\n",
    "            all_preds['severity'].extend(sv_logits.argmax(dim=1).cpu().numpy())\n",
    "            for task in ['hate_type', 'target_group', 'severity']:\n",
    "                all_labels[task].extend(targets[task].cpu().numpy())\n",
    "                all_masks[task].extend(masks[task].cpu().numpy())\n",
    "    \n",
    "    metrics = {'loss': total_loss / max(1, n_batches)}\n",
    "    for task in ['hate_type', 'target_group', 'severity']:\n",
    "        mask = np.array(all_masks[task]).astype(bool)\n",
    "        if mask.sum() > 0:\n",
    "            preds = np.array(all_preds[task])[mask]\n",
    "            labels = np.array(all_labels[task])[mask]\n",
    "            metrics[f'{task}_macro_f1'] = f1_score(labels, preds, average='macro', zero_division=0)\n",
    "            metrics[f'{task}_micro_f1'] = f1_score(labels, preds, average='micro', zero_division=0)\n",
    "            if verbose:\n",
    "                print(f'\\n{task.upper()} Classification Report:')\n",
    "                print(classification_report(labels, preds, zero_division=0))\n",
    "        else:\n",
    "            metrics[f'{task}_macro_f1'] = None\n",
    "            metrics[f'{task}_micro_f1'] = None\n",
    "    return metrics\n",
    "\n",
    "print('‚úÖ Loss and evaluation functions defined with CLASS WEIGHTS support.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961e79d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Create data loaders + class weights\n",
    "SEED = 1337\n",
    "MAX_LENGTH = 160\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Already loaded in Cell 2\n",
    "print(f'Splits: Train={len(train_df)} | Val={len(val_df)} | Test={len(test_df)}')\n",
    "\n",
    "# Compute class weights (handles imbalanced classes)\n",
    "ht_weights = compute_class_weights(train_df, 'hate_type', 6).to(device)\n",
    "tg_weights = compute_class_weights(train_df, 'target_group', 4).to(device)\n",
    "sv_weights = compute_class_weights(train_df, 'severity', 4).to(device)\n",
    "\n",
    "print(f'\\nüìä Class Weights:')\n",
    "print(f'  hate_type:    {[f\"{w:.2f}\" for w in ht_weights.tolist()]}')\n",
    "print(f'  target_group: {[f\"{w:.2f}\" for w in tg_weights.tolist()]}')\n",
    "print(f'  severity:     {[f\"{w:.2f}\" for w in sv_weights.tolist()]}')\n",
    "\n",
    "# Create datasets and loaders\n",
    "train_dataset = HateDataset(train_df, tokenizer, max_length=MAX_LENGTH)\n",
    "val_dataset = HateDataset(val_df, tokenizer, max_length=MAX_LENGTH)\n",
    "test_dataset = HateDataset(test_df, tokenizer, max_length=MAX_LENGTH)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(f'\\n‚úÖ Data loaders ready: {len(train_loader)} train batches | {len(val_loader)} val batches')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb54c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Training function with SPACE-SAVING checkpoint strategy\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(train_loader, val_loader, config, run_name='xlmr',\n",
    "                use_wandb=False, resume_from=None,\n",
    "                ht_class_weights=None, tg_class_weights=None, sv_class_weights=None):\n",
    "    \n",
    "    model = MultiTaskXLMRRoberta(dropout=config.get('dropout', 0.3)).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
    "    \n",
    "    total_steps = len(train_loader) * config['epochs']\n",
    "    warmup_steps = int(total_steps * config['warmup_ratio'])\n",
    "    scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=0.1, total_iters=warmup_steps)\n",
    "    \n",
    "    if use_wandb:\n",
    "        import wandb\n",
    "        wandb.init(project='multilingual-hate-detection', name=run_name, config=config)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_macro_f1 = 0.0\n",
    "    patience_counter = 0\n",
    "    history = []\n",
    "    start_epoch = 1\n",
    "    \n",
    "    best_ckpt_path = os.path.join(CHECKPOINT_DIR, f'{run_name}_best.pt')\n",
    "    \n",
    "    if resume_from and os.path.exists(resume_from):\n",
    "        print(f'Resuming from checkpoint: {resume_from}')\n",
    "        checkpoint = torch.load(resume_from, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n",
    "        best_macro_f1 = checkpoint.get('best_macro_f1', 0.0)\n",
    "        patience_counter = checkpoint.get('patience_counter', 0)\n",
    "        history = checkpoint.get('history', [])\n",
    "        print(f'Resumed from epoch {checkpoint[\"epoch\"]}. Starting epoch {start_epoch}.')\n",
    "    \n",
    "    for epoch in range(start_epoch, config['epochs'] + 1):\n",
    "        model.train()\n",
    "        start = time.time()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch}/{config[\"epochs\"]}', leave=True)\n",
    "        for batch_idx, batch in enumerate(pbar):\n",
    "            batch = move_batch_to_device(batch)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(batch['input_ids'], batch['attention_mask'])\n",
    "            targets = {k: batch[k] for k in ['hate_type', 'target_group', 'severity']}\n",
    "            masks = {k: batch[f'{k}_mask'] for k in targets.keys()}\n",
    "            loss = multitask_loss(*logits, targets, masks, config['task_weights'],\n",
    "                                  ht_class_weights, tg_class_weights, sv_class_weights)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config['grad_clip'])\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            running_loss += loss.item()\n",
    "            avg_loss = running_loss / (batch_idx + 1)\n",
    "            pbar.set_postfix({'loss': f'{avg_loss:.4f}'})\n",
    "        \n",
    "        train_loss = running_loss / max(1, len(train_loader))\n",
    "        \n",
    "        print(f'Evaluating on validation set...')\n",
    "        val_metrics = evaluate(model, val_loader, config['task_weights'],\n",
    "                               ht_class_weights, tg_class_weights, sv_class_weights)\n",
    "        val_loss = val_metrics['loss']\n",
    "        \n",
    "        # Compute average macro F1 across tasks\n",
    "        macro_f1s = [val_metrics.get(f'{t}_macro_f1', 0) or 0 for t in ['hate_type', 'target_group', 'severity']]\n",
    "        avg_macro_f1 = sum(macro_f1s) / len(macro_f1s)\n",
    "        \n",
    "        epoch_time = time.time() - start\n",
    "        log_payload = {'epoch': epoch, 'train_loss': train_loss, 'val_loss': val_loss,\n",
    "                       'avg_macro_f1': avg_macro_f1, 'epoch_time': epoch_time, **val_metrics}\n",
    "        history.append(log_payload)\n",
    "        if use_wandb: wandb.log(log_payload)\n",
    "        \n",
    "        print(f'Epoch {epoch}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}, avg_macro_f1={avg_macro_f1:.4f}, time={epoch_time:.1f}s')\n",
    "        print(f'  hate_type_macro_f1={val_metrics.get(\"hate_type_macro_f1\", 0):.4f}, target_group_macro_f1={val_metrics.get(\"target_group_macro_f1\", 0):.4f}, severity_macro_f1={val_metrics.get(\"severity_macro_f1\", 0):.4f}')\n",
    "        \n",
    "        # ‚ö° SPACE-SAVING: Save epoch checkpoint ONLY if needed for resume\n",
    "        epoch_ckpt_path = os.path.join(CHECKPOINT_DIR, f'{run_name}_epoch{epoch}.pt')\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'best_macro_f1': best_macro_f1,\n",
    "            'patience_counter': patience_counter,\n",
    "            'history': history,\n",
    "            'config': config\n",
    "        }, epoch_ckpt_path)\n",
    "        print(f'  üíæ Epoch checkpoint saved to {epoch_ckpt_path}')\n",
    "        \n",
    "        # Save best model based on MACRO F1 (better for imbalanced data)\n",
    "        if avg_macro_f1 > best_macro_f1:\n",
    "            best_macro_f1 = avg_macro_f1\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), best_ckpt_path)\n",
    "            print(f'  ‚úì New best checkpoint saved! (avg_macro_f1={avg_macro_f1:.4f})')\n",
    "            \n",
    "            # ‚ö° DELETE OLD EPOCH CHECKPOINT after saving best (saves 7GB per epoch!)\n",
    "            # Keep only the most recent epoch checkpoint for resume capability\n",
    "            if epoch > 1:\n",
    "                old_epoch_ckpt = os.path.join(CHECKPOINT_DIR, f'{run_name}_epoch{epoch-1}.pt')\n",
    "                if os.path.exists(old_epoch_ckpt):\n",
    "                    os.remove(old_epoch_ckpt)\n",
    "                    print(f'  üóëÔ∏è Deleted old checkpoint: {old_epoch_ckpt}')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f'  No improvement. Patience: {patience_counter}/{config[\"patience\"]}')\n",
    "            if patience_counter >= config['patience']:\n",
    "                print('Early stopping triggered.')\n",
    "                break\n",
    "    \n",
    "    # ‚ö° FINAL CLEANUP: Delete last epoch checkpoint, keep only best\n",
    "    final_epoch_ckpt = os.path.join(CHECKPOINT_DIR, f'{run_name}_epoch{epoch}.pt')\n",
    "    if os.path.exists(final_epoch_ckpt):\n",
    "        os.remove(final_epoch_ckpt)\n",
    "        print(f'üóëÔ∏è Training complete. Deleted final epoch checkpoint. Only keeping: {best_ckpt_path}')\n",
    "    \n",
    "    if use_wandb: wandb.finish()\n",
    "    return best_ckpt_path, history\n",
    "\n",
    "print('‚úÖ Training function defined with SPACE-SAVING checkpoint strategy!')\n",
    "print('üíæ Saves: Best model (~2.5GB) + Latest epoch for resume (~7GB)')\n",
    "print('üóëÔ∏è Auto-deletes old epoch checkpoints after each epoch')\n",
    "print('üìä Total space needed: ~10GB max (vs 35GB for 5 epochs)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b484f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. START TRAINING (Run this cell to begin full 5-epoch training)\n",
    "# =====================================================\n",
    "# Training Configuration - Optimized for Enhanced Dataset\n",
    "# =====================================================\n",
    "\n",
    "full_training_config = {\n",
    "    'epochs': 5,\n",
    "    'learning_rate': 1e-5,      # Stable for auto-labeled data\n",
    "    'weight_decay': 1e-2,\n",
    "    'warmup_ratio': 0.1,\n",
    "    'grad_clip': 1.0,\n",
    "    'patience': 3,              # Early stopping after 3 epochs no improvement\n",
    "    'dropout': 0.3,\n",
    "    'task_weights': (1.0, 1.0, 1.0),\n",
    "    'use_class_weights': True\n",
    "}\n",
    "\n",
    "print('üöÄ Starting training with ENHANCED dataset...')\n",
    "print(f'üìç Device: {device}')\n",
    "print(f'üìÅ Checkpoints save to: {CHECKPOINT_DIR}')\n",
    "print(f'‚è±Ô∏è Estimated time: 45-60 min (T4 GPU)\\n')\n",
    "\n",
    "best_checkpoint_full, history_full = train_model(\n",
    "    train_loader, val_loader, \n",
    "    config=full_training_config,\n",
    "    run_name='xlmr_enhanced',\n",
    "    use_wandb=False,  # Set to True if you want W&B tracking\n",
    "    ht_class_weights=ht_weights,\n",
    "    tg_class_weights=tg_weights,\n",
    "    sv_class_weights=sv_weights\n",
    ")\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('‚úÖ TRAINING COMPLETE!')\n",
    "print(f'üìÅ Best model: {best_checkpoint_full}')\n",
    "print(f'üìä Final metrics: {history_full[-1]}')\n",
    "print('='*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8507ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. DOWNLOAD MODEL - Download trained checkpoint to your computer\n",
    "from google.colab import files\n",
    "\n",
    "print('üì• Preparing checkpoint for download...')\n",
    "print(f'File: {best_checkpoint_full}')\n",
    "print(f'Size: ~2.5GB')\n",
    "\n",
    "# Download the best checkpoint\n",
    "files.download(best_checkpoint_full)\n",
    "\n",
    "print('''\n",
    "‚úÖ Download started!\n",
    "\n",
    "The model is also saved permanently in Google Drive at:\n",
    "  My Drive/thesis_training/checkpoints_enhanced/xlmr_enhanced_best.pt\n",
    "\n",
    "You can access it anytime!\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ae8efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. EVALUATE MODEL - Load best checkpoint and test\n",
    "print('üìä Loading best model for evaluation...')\n",
    "\n",
    "best_model = MultiTaskXLMRRoberta(dropout=0.3).to(device)\n",
    "best_model.load_state_dict(torch.load(best_checkpoint_full, map_location=device))\n",
    "\n",
    "print('\\nüîç Evaluating on TEST SET...')\n",
    "test_results = evaluate(best_model, test_loader, \n",
    "                       task_weights=(1.0, 1.0, 1.0),\n",
    "                       ht_class_weights=ht_weights,\n",
    "                       tg_class_weights=tg_weights,\n",
    "                       sv_class_weights=sv_weights,\n",
    "                       verbose=True)\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('üìä TEST SET RESULTS')\n",
    "print('='*60)\n",
    "print(f\"Loss: {test_results['loss']:.4f}\")\n",
    "print(f\"\\nHate Type:\")\n",
    "print(f\"  Macro F1: {test_results['hate_type_macro_f1']:.4f}\")\n",
    "print(f\"  Micro F1: {test_results['hate_type_micro_f1']:.4f}\")\n",
    "print(f\"\\nTarget Group:\")\n",
    "print(f\"  Macro F1: {test_results['target_group_macro_f1']:.4f}\")\n",
    "print(f\"  Micro F1: {test_results['target_group_micro_f1']:.4f}\")\n",
    "print(f\"\\nSeverity:\")\n",
    "print(f\"  Macro F1: {test_results['severity_macro_f1']:.4f}\")\n",
    "print(f\"  Micro F1: {test_results['severity_micro_f1']:.4f}\")\n",
    "print('='*60)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
