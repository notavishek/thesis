{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeeb1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Install dependencies and verify GPU\n",
    "!pip install transformers scikit-learn wandb sentencepiece -q\n",
    "\n",
    "import torch\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')\n",
    "else:\n",
    "    print('‚ö†Ô∏è NO GPU DETECTED! Go to Runtime ‚Üí Change runtime type ‚Üí GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4798ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Mount Google Drive & Setup (RUN THIS FIRST!)\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Setup paths\n",
    "DRIVE_FOLDER = '/content/drive/MyDrive/thesis_training'\n",
    "os.makedirs(DRIVE_FOLDER, exist_ok=True)\n",
    "os.makedirs(f'{DRIVE_FOLDER}/checkpoints', exist_ok=True)\n",
    "os.makedirs('dataset', exist_ok=True)\n",
    "\n",
    "# Copy dataset from Google Drive\n",
    "DRIVE_DATASET = f'{DRIVE_FOLDER}/UNIFIED_ALL_SPLIT.csv'\n",
    "LOCAL_DATASET = 'dataset/UNIFIED_ALL_SPLIT.csv'\n",
    "\n",
    "if os.path.exists(DRIVE_DATASET):\n",
    "    !cp '{DRIVE_DATASET}' '{LOCAL_DATASET}'\n",
    "    print('‚úÖ Dataset loaded from Google Drive!')\n",
    "else:\n",
    "    print(f'‚ùå Dataset not found! Please upload UNIFIED_ALL_SPLIT.csv to:')\n",
    "    print(f'   Google Drive ‚Üí My Drive ‚Üí thesis_training/')\n",
    "    print(f'   Then re-run this cell.')\n",
    "\n",
    "# Checkpoint directory (saves to Drive - survives disconnects!)\n",
    "CHECKPOINT_DIR = f'{DRIVE_FOLDER}/checkpoints/'\n",
    "print(f'‚úÖ Checkpoints will save to: {CHECKPOINT_DIR}')\n",
    "\n",
    "!ls -la dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e55b4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. EDA: Sanity check splits and class balance\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('dataset/UNIFIED_ALL_SPLIT.csv')\n",
    "\n",
    "print('Split counts:')\n",
    "print(df['split'].value_counts(), '\\n')\n",
    "\n",
    "print('Per split language distribution:')\n",
    "print(df.groupby('split')['language'].value_counts(), '\\n')\n",
    "\n",
    "print('hate_type distribution:')\n",
    "print(df['hate_type'].value_counts(), '\\n')\n",
    "\n",
    "print('source_dataset distribution:')\n",
    "print(df['source_dataset'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8ce421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. HateDataset: PyTorch Dataset with tokenization and masking\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import XLMRobertaTokenizer\n",
    "\n",
    "class HateDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=160):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        text = str(row['text'])\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        hate_type = int(row['hate_type'])\n",
    "        target_group = int(row['target_group'])\n",
    "        severity = int(row['severity'])\n",
    "        \n",
    "        hate_type_mask = hate_type != -1\n",
    "        target_group_mask = target_group != -1\n",
    "        severity_mask = severity != -1\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'hate_type': torch.tensor(max(0, hate_type), dtype=torch.long),\n",
    "            'target_group': torch.tensor(max(0, target_group), dtype=torch.long),\n",
    "            'severity': torch.tensor(max(0, severity), dtype=torch.long),\n",
    "            'hate_type_mask': torch.tensor(hate_type_mask, dtype=torch.bool),\n",
    "            'target_group_mask': torch.tensor(target_group_mask, dtype=torch.bool),\n",
    "            'severity_mask': torch.tensor(severity_mask, dtype=torch.bool),\n",
    "        }\n",
    "\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-large')\n",
    "print('Tokenizer loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d42467d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. MultiTaskXLMRRoberta Model\n",
    "import torch.nn as nn\n",
    "from transformers import XLMRobertaModel\n",
    "\n",
    "class MultiTaskXLMRRoberta(nn.Module):\n",
    "    def __init__(self, model_name='xlm-roberta-large', dropout=0.2,\n",
    "                 n_hate_type=6, n_target_group=4, n_severity=4):\n",
    "        super().__init__()\n",
    "        self.backbone = XLMRobertaModel.from_pretrained(model_name)\n",
    "        hidden_size = self.backbone.config.hidden_size\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.hate_type_head = nn.Linear(hidden_size, n_hate_type)\n",
    "        self.target_group_head = nn.Linear(hidden_size, n_target_group)\n",
    "        self.severity_head = nn.Linear(hidden_size, n_severity)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        cls_output = self.dropout(cls_output)\n",
    "        \n",
    "        return (\n",
    "            self.hate_type_head(cls_output),\n",
    "            self.target_group_head(cls_output),\n",
    "            self.severity_head(cls_output)\n",
    "        )\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a9dccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Loss and evaluation functions WITH CLASS WEIGHTS\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "def compute_class_weights(df, column, n_classes, smoothing=0.1):\n",
    "    \"\"\"Compute inverse frequency class weights with smoothing.\"\"\"\n",
    "    valid = df[df[column] != -1][column]\n",
    "    counts = valid.value_counts().reindex(range(n_classes), fill_value=1).values\n",
    "    weights = 1.0 / (counts + smoothing * len(valid))\n",
    "    weights = weights / weights.sum() * n_classes\n",
    "    return torch.tensor(weights, dtype=torch.float32)\n",
    "\n",
    "def multitask_loss(hate_type_logits, target_group_logits, severity_logits,\n",
    "                   targets, masks, task_weights=(1.0, 1.0, 1.0),\n",
    "                   ht_class_weights=None, tg_class_weights=None, sv_class_weights=None):\n",
    "    \"\"\"Masked cross-entropy loss with optional class weights.\"\"\"\n",
    "    total_loss = 0.0\n",
    "    n_tasks = 0\n",
    "    \n",
    "    ht_mask = masks['hate_type'].bool()\n",
    "    if ht_mask.any():\n",
    "        loss_ht = F.cross_entropy(hate_type_logits[ht_mask], targets['hate_type'][ht_mask], weight=ht_class_weights)\n",
    "        total_loss += task_weights[0] * loss_ht\n",
    "        n_tasks += 1\n",
    "    \n",
    "    tg_mask = masks['target_group'].bool()\n",
    "    if tg_mask.any():\n",
    "        loss_tg = F.cross_entropy(target_group_logits[tg_mask], targets['target_group'][tg_mask], weight=tg_class_weights)\n",
    "        total_loss += task_weights[1] * loss_tg\n",
    "        n_tasks += 1\n",
    "    \n",
    "    sv_mask = masks['severity'].bool()\n",
    "    if sv_mask.any():\n",
    "        loss_sv = F.cross_entropy(severity_logits[sv_mask], targets['severity'][sv_mask], weight=sv_class_weights)\n",
    "        total_loss += task_weights[2] * loss_sv\n",
    "        n_tasks += 1\n",
    "    \n",
    "    return total_loss / max(1, n_tasks)\n",
    "\n",
    "def move_batch_to_device(batch):\n",
    "    return {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "def evaluate(model, data_loader, task_weights=(1.0, 1.0, 1.0),\n",
    "             ht_class_weights=None, tg_class_weights=None, sv_class_weights=None, verbose=False):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    n_batches = 0\n",
    "    all_preds = {'hate_type': [], 'target_group': [], 'severity': []}\n",
    "    all_labels = {'hate_type': [], 'target_group': [], 'severity': []}\n",
    "    all_masks = {'hate_type': [], 'target_group': [], 'severity': []}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            batch = move_batch_to_device(batch)\n",
    "            ht_logits, tg_logits, sv_logits = model(batch['input_ids'], batch['attention_mask'])\n",
    "            targets = {k: batch[k] for k in ['hate_type', 'target_group', 'severity']}\n",
    "            masks = {k: batch[f'{k}_mask'] for k in targets.keys()}\n",
    "            loss = multitask_loss(ht_logits, tg_logits, sv_logits, targets, masks, task_weights,\n",
    "                                  ht_class_weights, tg_class_weights, sv_class_weights)\n",
    "            total_loss += loss.item()\n",
    "            n_batches += 1\n",
    "            all_preds['hate_type'].extend(ht_logits.argmax(dim=1).cpu().numpy())\n",
    "            all_preds['target_group'].extend(tg_logits.argmax(dim=1).cpu().numpy())\n",
    "            all_preds['severity'].extend(sv_logits.argmax(dim=1).cpu().numpy())\n",
    "            for task in ['hate_type', 'target_group', 'severity']:\n",
    "                all_labels[task].extend(targets[task].cpu().numpy())\n",
    "                all_masks[task].extend(masks[task].cpu().numpy())\n",
    "    \n",
    "    metrics = {'loss': total_loss / max(1, n_batches)}\n",
    "    for task in ['hate_type', 'target_group', 'severity']:\n",
    "        mask = np.array(all_masks[task]).astype(bool)\n",
    "        if mask.sum() > 0:\n",
    "            preds = np.array(all_preds[task])[mask]\n",
    "            labels = np.array(all_labels[task])[mask]\n",
    "            metrics[f'{task}_macro_f1'] = f1_score(labels, preds, average='macro', zero_division=0)\n",
    "            metrics[f'{task}_micro_f1'] = f1_score(labels, preds, average='micro', zero_division=0)\n",
    "            if verbose:\n",
    "                print(f'\\n{task.upper()} Classification Report:')\n",
    "                print(classification_report(labels, preds, zero_division=0))\n",
    "        else:\n",
    "            metrics[f'{task}_macro_f1'] = None\n",
    "            metrics[f'{task}_micro_f1'] = None\n",
    "    return metrics\n",
    "\n",
    "print('‚úÖ Loss and evaluation functions defined with CLASS WEIGHTS support.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961e79d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Setup data loaders + COMPUTE CLASS WEIGHTS\n",
    "import os\n",
    "\n",
    "SEED = 1337\n",
    "MAX_LENGTH = 160\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "train_df = df[df['split'] == 'train'].reset_index(drop=True)\n",
    "val_df = df[df['split'] == 'val'].reset_index(drop=True)\n",
    "test_df = df[df['split'] == 'test'].reset_index(drop=True)\n",
    "print(f'Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}')\n",
    "\n",
    "# Compute class weights from training data (helps with imbalanced classes!)\n",
    "ht_weights = compute_class_weights(train_df, 'hate_type', 6).to(device)\n",
    "tg_weights = compute_class_weights(train_df, 'target_group', 4).to(device)\n",
    "sv_weights = compute_class_weights(train_df, 'severity', 4).to(device)\n",
    "\n",
    "print(f'\\nüìä Class Weights (higher = more focus on that class):')\n",
    "print(f'  hate_type:    {[f\"{w:.2f}\" for w in ht_weights.tolist()]}')\n",
    "print(f'  target_group: {[f\"{w:.2f}\" for w in tg_weights.tolist()]}')\n",
    "print(f'  severity:     {[f\"{w:.2f}\" for w in sv_weights.tolist()]}')\n",
    "\n",
    "train_dataset = HateDataset(train_df, tokenizer, max_length=MAX_LENGTH)\n",
    "val_dataset = HateDataset(val_df, tokenizer, max_length=MAX_LENGTH)\n",
    "test_dataset = HateDataset(test_df, tokenizer, max_length=MAX_LENGTH)\n",
    "\n",
    "# Use num_workers=2 for faster data loading on Colab\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "print(f'\\nData loaders created. Batches - Train: {len(train_loader)}, Val: {len(val_loader)}, Test: {len(test_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb54c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Training function with CLASS WEIGHTS + macro F1 early stopping\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "training_config = {\n",
    "    'epochs': 5,\n",
    "    'learning_rate': 1e-5,    # Lower LR for stability\n",
    "    'weight_decay': 1e-2,\n",
    "    'warmup_ratio': 0.1,\n",
    "    'grad_clip': 1.0,\n",
    "    'patience': 3,\n",
    "    'dropout': 0.3,           # Slightly higher dropout\n",
    "    'task_weights': (1.0, 1.0, 1.0),\n",
    "    'use_class_weights': True\n",
    "}\n",
    "\n",
    "def train_model(train_loader, val_loader, config=None, run_name='xlmr_run', use_wandb=False, resume_from=None,\n",
    "                ht_class_weights=None, tg_class_weights=None, sv_class_weights=None):\n",
    "    if config is None: config = training_config\n",
    "    if use_wandb:\n",
    "        import wandb\n",
    "        wandb.init(project='multilingual-hate-detection', name=run_name, config=config, resume='allow')\n",
    "    \n",
    "    model = MultiTaskXLMRRoberta(dropout=config['dropout']).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
    "    total_steps = len(train_loader) * config['epochs']\n",
    "    warmup_steps = int(total_steps * config['warmup_ratio'])\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_macro_f1 = 0.0\n",
    "    patience_counter = 0\n",
    "    start_epoch = 1\n",
    "    history = []\n",
    "    \n",
    "    best_ckpt_path = os.path.join(CHECKPOINT_DIR, f'{run_name}_best.pt')\n",
    "    \n",
    "    if resume_from and os.path.exists(resume_from):\n",
    "        print(f'Resuming from checkpoint: {resume_from}')\n",
    "        checkpoint = torch.load(resume_from, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n",
    "        best_macro_f1 = checkpoint.get('best_macro_f1', 0.0)\n",
    "        patience_counter = checkpoint.get('patience_counter', 0)\n",
    "        history = checkpoint.get('history', [])\n",
    "        print(f'Resumed from epoch {checkpoint[\"epoch\"]}. Starting epoch {start_epoch}.')\n",
    "    \n",
    "    for epoch in range(start_epoch, config['epochs'] + 1):\n",
    "        model.train()\n",
    "        start = time.time()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch}/{config[\"epochs\"]}', leave=True)\n",
    "        for batch_idx, batch in enumerate(pbar):\n",
    "            batch = move_batch_to_device(batch)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(batch['input_ids'], batch['attention_mask'])\n",
    "            targets = {k: batch[k] for k in ['hate_type', 'target_group', 'severity']}\n",
    "            masks = {k: batch[f'{k}_mask'] for k in targets.keys()}\n",
    "            loss = multitask_loss(*logits, targets, masks, config['task_weights'],\n",
    "                                  ht_class_weights, tg_class_weights, sv_class_weights)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config['grad_clip'])\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            running_loss += loss.item()\n",
    "            avg_loss = running_loss / (batch_idx + 1)\n",
    "            pbar.set_postfix({'loss': f'{avg_loss:.4f}'})\n",
    "        \n",
    "        train_loss = running_loss / max(1, len(train_loader))\n",
    "        \n",
    "        print(f'Evaluating on validation set...')\n",
    "        val_metrics = evaluate(model, val_loader, config['task_weights'],\n",
    "                               ht_class_weights, tg_class_weights, sv_class_weights)\n",
    "        val_loss = val_metrics['loss']\n",
    "        \n",
    "        # Compute average macro F1 across tasks\n",
    "        macro_f1s = [val_metrics.get(f'{t}_macro_f1', 0) or 0 for t in ['hate_type', 'target_group', 'severity']]\n",
    "        avg_macro_f1 = sum(macro_f1s) / len(macro_f1s)\n",
    "        \n",
    "        epoch_time = time.time() - start\n",
    "        log_payload = {'epoch': epoch, 'train_loss': train_loss, 'val_loss': val_loss,\n",
    "                       'avg_macro_f1': avg_macro_f1, 'epoch_time': epoch_time, **val_metrics}\n",
    "        history.append(log_payload)\n",
    "        if use_wandb: wandb.log(log_payload)\n",
    "        \n",
    "        print(f'Epoch {epoch}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}, avg_macro_f1={avg_macro_f1:.4f}, time={epoch_time:.1f}s')\n",
    "        print(f'  hate_type_macro_f1={val_metrics.get(\"hate_type_macro_f1\", 0):.4f}, target_group_macro_f1={val_metrics.get(\"target_group_macro_f1\", 0):.4f}, severity_macro_f1={val_metrics.get(\"severity_macro_f1\", 0):.4f}')\n",
    "        \n",
    "        # Save epoch checkpoint\n",
    "        epoch_ckpt_path = os.path.join(CHECKPOINT_DIR, f'{run_name}_epoch{epoch}.pt')\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'best_macro_f1': best_macro_f1,\n",
    "            'patience_counter': patience_counter,\n",
    "            'history': history,\n",
    "            'config': config\n",
    "        }, epoch_ckpt_path)\n",
    "        print(f'  üíæ Epoch checkpoint saved to {epoch_ckpt_path}')\n",
    "        \n",
    "        # Save best model based on MACRO F1 (better for imbalanced data)\n",
    "        if avg_macro_f1 > best_macro_f1:\n",
    "            best_macro_f1 = avg_macro_f1\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), best_ckpt_path)\n",
    "            print(f'  ‚úì New best checkpoint saved! (avg_macro_f1={avg_macro_f1:.4f})')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f'  No improvement. Patience: {patience_counter}/{config[\"patience\"]}')\n",
    "            if patience_counter >= config['patience']:\n",
    "                print('Early stopping triggered.')\n",
    "                break\n",
    "    \n",
    "    if use_wandb: wandb.finish()\n",
    "    return best_ckpt_path, history\n",
    "\n",
    "print('‚úÖ Training function defined with CLASS WEIGHTS + macro F1 early stopping.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dbff92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. (Optional) W&B Login - uncomment if you want experiment tracking\n",
    "# import wandb\n",
    "# wandb.login(key='YOUR_WANDB_KEY')  # Or just run wandb.login() to authenticate interactively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f692c34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. üöÄ FULL TRAINING WITH CLASS WEIGHTS - Run this cell!\n",
    "print(f'üöÄ Starting IMPROVED training on {len(train_dataset)} samples...')\n",
    "print(f'Device: {device}')\n",
    "print(f'üìÅ Checkpoints saving to: {CHECKPOINT_DIR}')\n",
    "print(f'\\nüìä Using class weights to handle imbalance!')\n",
    "print(f'  hate_type weights:    {[f\"{w:.2f}\" for w in ht_weights.tolist()]}')\n",
    "print(f'  target_group weights: {[f\"{w:.2f}\" for w in tg_weights.tolist()]}')\n",
    "print(f'  severity weights:     {[f\"{w:.2f}\" for w in sv_weights.tolist()]}')\n",
    "print('=' * 50)\n",
    "\n",
    "# To resume from a previous epoch, set this:\n",
    "# resume_checkpoint = '/content/drive/MyDrive/thesis_training/checkpoints/xlmr_v2_epoch2.pt'\n",
    "resume_checkpoint = None\n",
    "\n",
    "best_checkpoint_full, history_full = train_model(\n",
    "    train_loader, val_loader, \n",
    "    config=training_config,\n",
    "    run_name='xlmr_v2_classweights',\n",
    "    use_wandb=False,\n",
    "    resume_from=resume_checkpoint,\n",
    "    ht_class_weights=ht_weights,\n",
    "    tg_class_weights=tg_weights,\n",
    "    sv_class_weights=sv_weights\n",
    ")\n",
    "print('\\n‚úÖ Training complete!')\n",
    "print(f'üìÅ Best checkpoint: {best_checkpoint_full}')\n",
    "print('History:', history_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b484f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Final Evaluation with detailed per-class metrics\n",
    "print('Loading best checkpoint and evaluating...')\n",
    "best_model_full = MultiTaskXLMRRoberta().to(device)\n",
    "best_model_full.load_state_dict(torch.load(best_checkpoint_full, map_location=device))\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('=== VALIDATION SET RESULTS ===')\n",
    "print('='*60)\n",
    "val_results_full = evaluate(best_model_full, val_loader, verbose=True,\n",
    "                            ht_class_weights=ht_weights, tg_class_weights=tg_weights, sv_class_weights=sv_weights)\n",
    "print('\\nSummary:')\n",
    "for k, v in val_results_full.items():\n",
    "    if v is not None: print(f'  {k}: {v:.4f}')\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('=== TEST SET RESULTS ===')\n",
    "print('='*60)\n",
    "test_results_full = evaluate(best_model_full, test_loader, verbose=True,\n",
    "                             ht_class_weights=ht_weights, tg_class_weights=tg_weights, sv_class_weights=sv_weights)\n",
    "print('\\nSummary:')\n",
    "for k, v in test_results_full.items():\n",
    "    if v is not None: print(f'  {k}: {v:.4f}')\n",
    "\n",
    "print(f'\\nüìÅ Best checkpoint: {best_checkpoint_full}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7e376b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.5 üîÆ Inference: Predict on new text\n",
    "# CORRECTED LABELS based on main.py mapping!\n",
    "HATE_TYPE_LABELS = {\n",
    "    0: 'not_hate/other',\n",
    "    1: 'political',\n",
    "    2: 'religious',\n",
    "    3: 'gender',\n",
    "    4: 'personal_attack',\n",
    "    5: 'geopolitical'\n",
    "}\n",
    "TARGET_GROUP_LABELS = {0: 'other/none', 1: 'individual', 2: 'organization/group', 3: 'community'}\n",
    "SEVERITY_LABELS = {0: 'none', 1: 'low', 2: 'medium', 3: 'high'}\n",
    "\n",
    "def predict(text, model=None, return_probs=False):\n",
    "    \"\"\"Predict hate type, target group, and severity for a given text.\"\"\"\n",
    "    if model is None:\n",
    "        model = best_model_full\n",
    "    \n",
    "    model.eval()\n",
    "    encoding = tokenizer(text, max_length=160, padding='max_length', truncation=True, return_tensors='pt')\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        ht_logits, tg_logits, sv_logits = model(input_ids, attention_mask)\n",
    "    \n",
    "    ht_pred = ht_logits.argmax(dim=1).item()\n",
    "    tg_pred = tg_logits.argmax(dim=1).item()\n",
    "    sv_pred = sv_logits.argmax(dim=1).item()\n",
    "    \n",
    "    result = {\n",
    "        'text': text[:100] + '...' if len(text) > 100 else text,\n",
    "        'hate_type': HATE_TYPE_LABELS[ht_pred],\n",
    "        'target_group': TARGET_GROUP_LABELS[tg_pred],\n",
    "        'severity': SEVERITY_LABELS[sv_pred],\n",
    "    }\n",
    "    \n",
    "    if return_probs:\n",
    "        result['hate_type_probs'] = torch.softmax(ht_logits, dim=1).cpu().numpy()[0]\n",
    "        result['target_group_probs'] = torch.softmax(tg_logits, dim=1).cpu().numpy()[0]\n",
    "        result['severity_probs'] = torch.softmax(sv_logits, dim=1).cpu().numpy()[0]\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test examples\n",
    "print('üîÆ Testing predictions...\\n')\n",
    "test_texts = [\n",
    "    \"‡¶§‡ßÅ‡¶á ‡¶è‡¶ï‡¶ü‡¶æ ‡¶¨‡ßã‡¶ï‡¶æ\",           # Bengali - personal attack\n",
    "    \"You're such an idiot\",   # English - personal attack  \n",
    "    \"tui ekta pagol\",         # Banglish - personal attack\n",
    "    \"Have a nice day!\",       # English - not hate\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    result = predict(text)\n",
    "    print(f\"Text: {result['text']}\")\n",
    "    print(f\"  ‚Üí Hate: {result['hate_type']}, Target: {result['target_group']}, Severity: {result['severity']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5379b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Download the trained model from Google Drive\n",
    "from google.colab import files\n",
    "import shutil\n",
    "\n",
    "# Checkpoints are already in Google Drive! Just download them.\n",
    "DRIVE_CHECKPOINT_DIR = '/content/drive/MyDrive/thesis_training/checkpoints'\n",
    "\n",
    "print('üìÅ Checkpoints saved in Google Drive:')\n",
    "!ls -la {DRIVE_CHECKPOINT_DIR}\n",
    "\n",
    "# Create a zip for download\n",
    "shutil.make_archive('trained_model', 'zip', DRIVE_CHECKPOINT_DIR)\n",
    "print('\\nüì• Downloading trained_model.zip...')\n",
    "files.download('trained_model.zip')\n",
    "\n",
    "print('''\n",
    "‚úÖ Download complete!\n",
    "\n",
    "Your checkpoints are ALSO permanently saved in Google Drive at:\n",
    "  My Drive/thesis_training/checkpoints/\n",
    "\n",
    "You can access them anytime, even after runtime ends!\n",
    "''')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
