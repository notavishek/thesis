{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6a82a80",
   "metadata": {},
   "source": [
    "# ðŸš€ Final Training Notebook for Multilingual Hate Detection\n",
    "This notebook is optimized for Google Colab (T4/A100 GPU).\n",
    "It uses the **ENHANCED** dataset (`UNIFIED_ALL_SPLIT_ENHANCED.csv`) which includes:\n",
    "- 14,000+ recovered neutral Bengali samples.\n",
    "- Banglish keyword fixes.\n",
    "- Strict consistency checks (No Safe=Hate errors).\n",
    "\n",
    "**Steps:**\n",
    "1. Mount Drive.\n",
    "2. Install Dependencies.\n",
    "3. Load Data.\n",
    "4. Train with Mixed Precision (AMP).\n",
    "5. Save Checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab86167b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create checkpoint directory\n",
    "import os\n",
    "CHECKPOINT_DIR = '/content/drive/MyDrive/thesis/checkpoints/'\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "print(f\"âœ… Checkpoints will be saved to: {CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e017cb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Install Dependencies & Check GPU\n",
    "!pip install transformers wandb scikit-learn\n",
    "\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"âœ… GPU Available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"âŒ No GPU detected. Please enable GPU in Runtime > Change runtime type.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e9007b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Path to the uploaded AUGMENTED dataset\n",
    "DATASET_PATH = '/content/drive/MyDrive/thesis/dataset/UNIFIED_ALL_SPLIT_AUGMENTED.csv'\n",
    "\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    print(f\"âŒ Error: Dataset not found at {DATASET_PATH}\")\n",
    "    print(\"Please upload 'UNIFIED_ALL_SPLIT_AUGMENTED.csv' to 'thesis/dataset/' in your Drive.\")\n",
    "else:\n",
    "    df = pd.read_csv(DATASET_PATH)\n",
    "    print(f\"âœ… Loaded dataset: {len(df)} rows\")\n",
    "    print(\"\\nSplit distribution:\")\n",
    "    print(df['split'].value_counts())\n",
    "    print(\"\\nLanguage distribution:\")\n",
    "    print(df['language'].value_counts())\n",
    "    print(\"\\nSeverity distribution:\")\n",
    "    print(df['severity'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5ae443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Define Dataset Class\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import XLMRobertaTokenizer\n",
    "\n",
    "class HateDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=160):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        text = str(row['text'])\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Extract labels (use -1 for missing)\n",
    "        hate_type = int(row['hate_type'])\n",
    "        target_group = int(row['target_group'])\n",
    "        severity = int(row['severity'])\n",
    "        \n",
    "        # Create masks: True if label is valid (not -1)\n",
    "        hate_type_mask = hate_type != -1\n",
    "        target_group_mask = target_group != -1\n",
    "        severity_mask = severity != -1\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'hate_type': torch.tensor(max(0, hate_type), dtype=torch.long),\n",
    "            'target_group': torch.tensor(max(0, target_group), dtype=torch.long),\n",
    "            'severity': torch.tensor(max(0, severity), dtype=torch.long),\n",
    "            'hate_type_mask': torch.tensor(hate_type_mask, dtype=torch.bool),\n",
    "            'target_group_mask': torch.tensor(target_group_mask, dtype=torch.bool),\n",
    "            'severity_mask': torch.tensor(severity_mask, dtype=torch.bool),\n",
    "        }\n",
    "\n",
    "# Initialize Tokenizer\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-large')\n",
    "print(\"âœ… Tokenizer loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0f9b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Define Model Architecture\n",
    "import torch.nn as nn\n",
    "from transformers import XLMRobertaModel\n",
    "\n",
    "class MultiTaskXLMRRoberta(nn.Module):\n",
    "    def __init__(self, model_name='xlm-roberta-large', dropout=0.2,\n",
    "                 n_hate_type=6, n_target_group=4, n_severity=4):\n",
    "        super().__init__()\n",
    "        self.backbone = XLMRobertaModel.from_pretrained(model_name)\n",
    "        hidden_size = self.backbone.config.hidden_size  # 1024 for large\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Task-specific classification heads\n",
    "        self.hate_type_head = nn.Linear(hidden_size, n_hate_type)\n",
    "        self.target_group_head = nn.Linear(hidden_size, n_target_group)\n",
    "        self.severity_head = nn.Linear(hidden_size, n_severity)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Use CLS token representation\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        cls_output = self.dropout(cls_output)\n",
    "        \n",
    "        hate_type_logits = self.hate_type_head(cls_output)\n",
    "        target_group_logits = self.target_group_head(cls_output)\n",
    "        severity_logits = self.severity_head(cls_output)\n",
    "        \n",
    "        return hate_type_logits, target_group_logits, severity_logits\n",
    "\n",
    "print(\"âœ… Model class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e3d438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Loss Function & Class Weights\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_class_weights(df, column, n_classes, smoothing=0.1):\n",
    "    \"\"\"Compute inverse frequency class weights with smoothing.\"\"\"\n",
    "    valid = df[df[column] != -1][column]\n",
    "    counts = valid.value_counts().reindex(range(n_classes), fill_value=1).values\n",
    "    weights = 1.0 / (counts + smoothing * len(valid))\n",
    "    weights = weights / weights.sum() * n_classes  # Normalize to sum to n_classes\n",
    "    return torch.tensor(weights, dtype=torch.float32)\n",
    "\n",
    "def multitask_loss(hate_type_logits, target_group_logits, severity_logits,\n",
    "                   targets, masks, task_weights=(1.0, 1.0, 1.0),\n",
    "                   ht_class_weights=None, tg_class_weights=None, sv_class_weights=None):\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    \n",
    "    # Hate type loss\n",
    "    ht_mask = masks['hate_type'].bool()\n",
    "    if ht_mask.any():\n",
    "        loss_ht = F.cross_entropy(\n",
    "            hate_type_logits[ht_mask], \n",
    "            targets['hate_type'][ht_mask],\n",
    "            weight=ht_class_weights.to(hate_type_logits.device) if ht_class_weights is not None else None\n",
    "        )\n",
    "        total_loss += task_weights[0] * loss_ht\n",
    "        \n",
    "    # Target group loss\n",
    "    tg_mask = masks['target_group'].bool()\n",
    "    if tg_mask.any():\n",
    "        loss_tg = F.cross_entropy(\n",
    "            target_group_logits[tg_mask], \n",
    "            targets['target_group'][tg_mask],\n",
    "            weight=tg_class_weights.to(target_group_logits.device) if tg_class_weights is not None else None\n",
    "        )\n",
    "        total_loss += task_weights[1] * loss_tg\n",
    "        \n",
    "    # Severity loss\n",
    "    sv_mask = masks['severity'].bool()\n",
    "    if sv_mask.any():\n",
    "        loss_sv = F.cross_entropy(\n",
    "            severity_logits[sv_mask], \n",
    "            targets['severity'][sv_mask],\n",
    "            weight=sv_class_weights.to(severity_logits.device) if sv_class_weights is not None else None\n",
    "        )\n",
    "        total_loss += task_weights[2] * loss_sv\n",
    "        \n",
    "    return total_loss\n",
    "\n",
    "print(\"âœ… Loss function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c02833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Prepare Data Loaders\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Compute class weights\n",
    "ht_weights = compute_class_weights(df, 'hate_type', 6)\n",
    "tg_weights = compute_class_weights(df, 'target_group', 4)\n",
    "sv_weights = compute_class_weights(df, 'severity', 4)\n",
    "\n",
    "# ðŸ”§ FIX: Aggressively boost Severity weights\n",
    "# The model is \"lazy\" and defaults to Low (0/1) because High (3) is rare.\n",
    "# We drastically increase the penalty for missing High/Medium cases.\n",
    "print(f\"Original Severity Weights: {sv_weights}\")\n",
    "sv_weights[3] *= 5.0  # 5x penalty for missing High Severity\n",
    "sv_weights[2] *= 2.0  # 2x penalty for missing Medium Severity\n",
    "print(f\"ðŸ”¥ Boosted Severity Weights: {sv_weights}\")\n",
    "\n",
    "print(\"Class weights computed.\")\n",
    "\n",
    "# Split Data\n",
    "train_df = df[df['split'] == 'train']\n",
    "val_df = df[df['split'] == 'val']\n",
    "\n",
    "train_dataset = HateDataset(train_df, tokenizer)\n",
    "val_dataset = HateDataset(val_df, tokenizer)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525d8637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Training Loop (Fine-Tuning on Augmented Data)\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.cuda.amp as amp\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "\n",
    "def train_model(total_epochs=5):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = MultiTaskXLMRRoberta().to(device)\n",
    "    \n",
    "    # Initialize Optimizer & Scheduler\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "    total_steps = len(train_loader) * total_epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(0.1*total_steps), num_training_steps=total_steps)\n",
    "    scaler = amp.GradScaler() # For Mixed Precision\n",
    "    \n",
    "    start_epoch = 0\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    # 1. Try to resume from an ongoing augmented run\n",
    "    last_ckpt_path = os.path.join(CHECKPOINT_DIR, 'xlmr_augmented_last.pt')\n",
    "    best_ckpt_path = os.path.join(CHECKPOINT_DIR, 'xlmr_augmented_best.pt')\n",
    "    \n",
    "    # 2. If no augmented run exists, load the previous \"Enhanced\" best model as starting point\n",
    "    prev_best_path = os.path.join(CHECKPOINT_DIR, 'xlmr_enhanced_best.pt')\n",
    "    \n",
    "    if os.path.exists(last_ckpt_path):\n",
    "        print(f\"ðŸ”„ Found interrupted augmented training at {last_ckpt_path}. Resuming...\")\n",
    "        checkpoint = torch.load(last_ckpt_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n",
    "        print(f\"âœ… Resuming from Epoch {start_epoch+1}\")\n",
    "        \n",
    "    elif os.path.exists(prev_best_path):\n",
    "        print(f\"ðŸš€ Loading previous best model from {prev_best_path} to fine-tune on Banglish...\")\n",
    "        model.load_state_dict(torch.load(prev_best_path))\n",
    "        print(\"âœ… Weights loaded. Starting fresh fine-tuning for 5 epochs.\")\n",
    "        \n",
    "    else:\n",
    "        print(\"âš ï¸ No previous model found. Training from scratch (Not recommended for fine-tuning).\")\n",
    "    \n",
    "    if start_epoch >= total_epochs:\n",
    "        print(\"âœ… Training already completed!\")\n",
    "        return\n",
    "\n",
    "    print(f\"ðŸš€ Starting training from Epoch {start_epoch+1} to {total_epochs}...\")\n",
    "    print(f\"Estimated time per epoch (T4 GPU): ~25-30 minutes (Larger dataset)\")\n",
    "    \n",
    "    for epoch in range(start_epoch, total_epochs):\n",
    "        start_time = time.time()\n",
    "        print(f\"\\nEpoch {epoch+1}/{total_epochs}\")\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        loop = tqdm(train_loader, leave=True)\n",
    "        for batch in loop:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            targets = {k: batch[k].to(device) for k in ['hate_type', 'target_group', 'severity']}\n",
    "            masks = {k: batch[f'{k}_mask'].to(device) for k in ['hate_type', 'target_group', 'severity']}\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Mixed Precision Forward Pass\n",
    "            with amp.autocast():\n",
    "                ht_logits, tg_logits, sv_logits = model(input_ids, attention_mask)\n",
    "                loss = multitask_loss(ht_logits, tg_logits, sv_logits, targets, masks, \n",
    "                                      ht_class_weights=ht_weights, \n",
    "                                      tg_class_weights=tg_weights, \n",
    "                                      sv_class_weights=sv_weights)\n",
    "            \n",
    "            # Mixed Precision Backward Pass\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            loop.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "            \n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        print(f\"Average Train Loss: {avg_train_loss:.4f}\")\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                targets = {k: batch[k].to(device) for k in ['hate_type', 'target_group', 'severity']}\n",
    "                masks = {k: batch[f'{k}_mask'].to(device) for k in ['hate_type', 'target_group', 'severity']}\n",
    "                \n",
    "                ht_logits, tg_logits, sv_logits = model(input_ids, attention_mask)\n",
    "                loss = multitask_loss(ht_logits, tg_logits, sv_logits, targets, masks,\n",
    "                                      ht_class_weights=ht_weights, \n",
    "                                      tg_class_weights=tg_weights, \n",
    "                                      sv_class_weights=sv_weights)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        epoch_time = (time.time() - start_time) / 60\n",
    "        print(f\"Validation Loss: {avg_val_loss:.4f} | Time: {epoch_time:.1f} min\")\n",
    "        \n",
    "        # Save Best Checkpoint (New Name)\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            save_path = os.path.join(CHECKPOINT_DIR, 'xlmr_augmented_best.pt')\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"ðŸ”¥ New Best Model Saved: {save_path}\")\n",
    "            \n",
    "        # Save Last Checkpoint (New Name)\n",
    "        last_save_path = os.path.join(CHECKPOINT_DIR, 'xlmr_augmented_last.pt')\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'scaler_state_dict': scaler.state_dict(),\n",
    "            'best_val_loss': best_val_loss\n",
    "        }, last_save_path)\n",
    "        print(f\"ðŸ’¾ Checkpoint saved: {last_save_path}\")\n",
    "\n",
    "# Run Training (5 epochs is enough for fine-tuning)\n",
    "train_model(total_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01989874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Final Evaluation on Test Set (Augmented Model)\n",
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def evaluate_test_set():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    print(\"â³ Loading best AUGMENTED model for evaluation...\")\n",
    "    model = MultiTaskXLMRRoberta()\n",
    "    # Load best checkpoint\n",
    "    ckpt_path = os.path.join(CHECKPOINT_DIR, 'xlmr_augmented_best.pt')\n",
    "    if not os.path.exists(ckpt_path):\n",
    "        print(\"âŒ No augmented checkpoint found. Run training first.\")\n",
    "        return\n",
    "        \n",
    "    model.load_state_dict(torch.load(ckpt_path))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Prepare Test Loader\n",
    "    test_df = df[df['split'] == 'test']\n",
    "    test_dataset = HateDataset(test_df, tokenizer)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
    "    \n",
    "    print(f\"ðŸš€ Evaluating on {len(test_df)} test samples...\")\n",
    "    \n",
    "    # Storage for predictions and labels\n",
    "    task_metrics = {\n",
    "        'hate_type': {'preds': [], 'labels': []},\n",
    "        'target_group': {'preds': [], 'labels': []},\n",
    "        'severity': {'preds': [], 'labels': []}\n",
    "    }\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            ht_logits, tg_logits, sv_logits = model(input_ids, attention_mask)\n",
    "            \n",
    "            # Helper to collect valid predictions\n",
    "            def collect(logits, task_name):\n",
    "                preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "                labels = batch[task_name].cpu().numpy()\n",
    "                masks = batch[f'{task_name}_mask'].cpu().numpy()\n",
    "                \n",
    "                for p, l, m in zip(preds, labels, masks):\n",
    "                    if m: # Only keep valid labels\n",
    "                        task_metrics[task_name]['preds'].append(p)\n",
    "                        task_metrics[task_name]['labels'].append(l)\n",
    "\n",
    "            collect(ht_logits, 'hate_type')\n",
    "            collect(tg_logits, 'target_group')\n",
    "            collect(sv_logits, 'severity')\n",
    "            \n",
    "    # Print Reports\n",
    "    for task, data in task_metrics.items():\n",
    "        print(f\"\\nðŸ“Š --- {task.upper().replace('_', ' ')} Report ---\")\n",
    "        if len(data['labels']) > 0:\n",
    "            print(classification_report(data['labels'], data['preds'], digits=4))\n",
    "        else:\n",
    "            print(\"No valid labels found for this task in test set.\")\n",
    "\n",
    "evaluate_test_set()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
