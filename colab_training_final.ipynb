{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6a82a80",
   "metadata": {},
   "source": [
    "# üöÄ Final Training Notebook for Multilingual Hate Detection (XLM-RoBERTa Base)\n",
    "This notebook is optimized for Google Colab (T4 GPU recommended).\n",
    "It uses the **FINAL** dataset (`Final_Dataset.csv`) which includes:\n",
    "- 90,710 deduplicated samples (54k train, 36k val/test)\n",
    "- Zero data leakage between splits\n",
    "- Balanced class distributions across all tasks\n",
    "- Cleaned text with artifacts removed\n",
    "\n",
    "**Model:** XLM-RoBERTa Base (270M params) - Optimal for dataset size\n",
    "\n",
    "**Steps:**\n",
    "1. Mount Drive.\n",
    "2. Install Dependencies.\n",
    "\n",
    "3. Load Data.5. Save Checkpoints.\n",
    "4. Train with Mixed Precision (AMP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab86167b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create checkpoint directory\n",
    "import os\n",
    "CHECKPOINT_DIR = '/content/drive/MyDrive/thesis/checkpoints/'\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "print(f\"‚úÖ Checkpoints will be saved to: {CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e017cb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Install Dependencies & Check GPU\n",
    "# Force upgrade to fix 'GenerationMixin' error and ensure compatibility\n",
    "!pip install -U transformers accelerate wandb scikit-learn seaborn matplotlib\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU Available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    # üöÄ Performance Optimization\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "else:\n",
    "    print(\"‚ùå No GPU detected. Please enable GPU in Runtime > Change runtime type.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7fbfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Pre-download Model (Turbo Mode üöÄ)\n",
    "import os\n",
    "\n",
    "print(\"‚è≥ Downloading XLM-RoBERTa Base (High Speed)...\")\n",
    "\n",
    "# 1. Install hf_transfer (Rust-based accelerator)\n",
    "!pip install -q huggingface_hub hf_transfer\n",
    "\n",
    "# 2. Enable the accelerator\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "\n",
    "# 3. Download\n",
    "# Note: If this fails with \"command not found\", restart the runtime once.\n",
    "!huggingface-cli download xlm-roberta-base --local-dir ./xlm-roberta-base --exclude \"*.msgpack\" \"*.h5\" \"*.ot\"\n",
    "\n",
    "print(\"‚úÖ Model downloaded to ./xlm-roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e9007b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Path to the uploaded FINAL dataset\n",
    "DATASET_PATH = '/content/drive/MyDrive/thesis/dataset/Final_Dataset.csv'\n",
    "\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    print(f\"‚ùå Error: Dataset not found at {DATASET_PATH}\")\n",
    "    print(\"Please upload 'Final_Dataset.csv' to 'thesis/dataset/' in your Drive.\")\n",
    "else:\n",
    "    df = pd.read_csv(DATASET_PATH)\n",
    "    print(f\"‚úÖ Loaded dataset: {len(df)} rows\")\n",
    "    print(\"\\nSplit distribution:\")\n",
    "    print(df['split'].value_counts())\n",
    "    print(\"\\nLanguage distribution:\")\n",
    "    print(df['language'].value_counts())\n",
    "    print(\"\\nSeverity distribution:\")\n",
    "    print(df['severity'].value_counts().sort_index())\n",
    "    print(\"\\nHate Type distribution:\")\n",
    "    print(df['hate_type'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5ae443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Define Dataset Class\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import XLMRobertaTokenizer\n",
    "\n",
    "class HateDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=160):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        text = str(row['text'])\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Extract labels (use -1 for missing)\n",
    "        hate_type = int(row['hate_type'])\n",
    "        target_group = int(row['target_group'])\n",
    "        severity = int(row['severity'])\n",
    "        \n",
    "        # Create masks: True if label is valid (not -1)\n",
    "        hate_type_mask = hate_type != -1\n",
    "        target_group_mask = target_group != -1\n",
    "        severity_mask = severity != -1\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'hate_type': torch.tensor(max(0, hate_type), dtype=torch.long),\n",
    "            'target_group': torch.tensor(max(0, target_group), dtype=torch.long),\n",
    "            'severity': torch.tensor(max(0, severity), dtype=torch.long),\n",
    "            'hate_type_mask': torch.tensor(hate_type_mask, dtype=torch.bool),\n",
    "            'target_group_mask': torch.tensor(target_group_mask, dtype=torch.bool),\n",
    "            'severity_mask': torch.tensor(severity_mask, dtype=torch.bool),\n",
    "        }\n",
    "\n",
    "# Initialize Tokenizer\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
    "print(\"‚úÖ Tokenizer loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e3d438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none', weight=self.alpha)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "# üî• OPTIMIZED WEIGHTS FOR BALANCED PERFORMANCE\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 1. Hate Type Weights (6 Classes)\n",
    "# Balanced to handle the 3:1 ratio of Not-Hate vs Hate\n",
    "ht_weights = torch.tensor([1.0, 2.5, 2.5, 2.5, 2.0, 2.5], device=device, dtype=torch.float)\n",
    "\n",
    "# 2. Target Group Weights (4 Classes)\n",
    "# Community is dominant, so we boost Individual/Org\n",
    "tg_weights = torch.tensor([1.0, 3.0, 3.0, 1.0], device=device, dtype=torch.float) \n",
    "\n",
    "# 3. Severity Weights (4 Classes) - CRITICAL FIX üõ†Ô∏è\n",
    "# Previous issue: Model predicted \"High\" for everything.\n",
    "# Fix: Equalize weights for Low(1), Medium(2), High(3) to force feature learning.\n",
    "# We boost all hate severities to 3.0 to prioritize them over None(0).\n",
    "sv_weights = torch.tensor([1.0, 3.0, 3.0, 3.0], device=device, dtype=torch.float)\n",
    "\n",
    "# Initialize Weighted Focal Loss\n",
    "loss_fn_ht = FocalLoss(alpha=ht_weights, gamma=2.0)\n",
    "loss_fn_tg = FocalLoss(alpha=tg_weights, gamma=2.0)\n",
    "loss_fn_sv = FocalLoss(alpha=sv_weights, gamma=2.0)\n",
    "\n",
    "def multitask_loss(hate_type_logits, target_group_logits, severity_logits,\n",
    "                   targets, masks, task_weights=(1.0, 1.0, 1.5)): \n",
    "    # ‚¨ÜÔ∏è NOTE: Severity task weight boosted to 1.5 to prioritize learning it\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    \n",
    "    # Hate type loss\n",
    "    ht_mask = masks['hate_type'].bool()\n",
    "    if ht_mask.any():\n",
    "        loss_ht = loss_fn_ht(hate_type_logits[ht_mask], targets['hate_type'][ht_mask])\n",
    "        total_loss += task_weights[0] * loss_ht\n",
    "        \n",
    "    # Target group loss\n",
    "    tg_mask = masks['target_group'].bool()\n",
    "    if tg_mask.any():\n",
    "        loss_tg = loss_fn_tg(target_group_logits[tg_mask], targets['target_group'][tg_mask])\n",
    "        total_loss += task_weights[1] * loss_tg\n",
    "        \n",
    "    # Severity loss\n",
    "    sv_mask = masks['severity'].bool()\n",
    "    if sv_mask.any():\n",
    "        loss_sv = loss_fn_sv(severity_logits[sv_mask], targets['severity'][sv_mask])\n",
    "        total_loss += task_weights[2] * loss_sv\n",
    "        \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5651be54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Define Model Architecture\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from transformers import XLMRobertaModel\n",
    "\n",
    "class MultiTaskXLMRRoberta(nn.Module):\n",
    "    def __init__(self, model_name='xlm-roberta-base', dropout=0.2):\n",
    "        super(MultiTaskXLMRRoberta, self).__init__()\n",
    "        \n",
    "        # Check if local cache exists (from CLI download)\n",
    "        if model_name == 'xlm-roberta-base' and os.path.exists('./xlm-roberta-base'):\n",
    "            print(f\"üìÇ Loading model from local cache: ./xlm-roberta-base\")\n",
    "            model_name = './xlm-roberta-base'\n",
    "            \n",
    "        self.backbone = XLMRobertaModel.from_pretrained(model_name)\n",
    "        self.hidden_size = self.backbone.config.hidden_size\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Classification Heads\n",
    "        self.hate_type_head = nn.Linear(self.hidden_size, 6)    # 0-5\n",
    "        self.target_group_head = nn.Linear(self.hidden_size, 4) # 0-3\n",
    "        self.severity_head = nn.Linear(self.hidden_size, 4)     # 0-3\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Use CLS token representation (first token)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        hate_type_logits = self.hate_type_head(pooled_output)\n",
    "        target_group_logits = self.target_group_head(pooled_output)\n",
    "        severity_logits = self.severity_head(pooled_output)\n",
    "        \n",
    "        return hate_type_logits, target_group_logits, severity_logits\n",
    "\n",
    "print(\"‚úÖ MultiTaskXLMRRoberta model defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee878d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Training Loop with Smart LR Adjustment\n",
    "# We need to lower LR when unfreezing to prevent divergence\n",
    "\n",
    "def train_model(model, train_loader, val_loader, total_epochs=8):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=1e-2)\n",
    "    \n",
    "    # Scheduler: Linear warmup then decay\n",
    "    total_steps = len(train_loader) * total_epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                num_warmup_steps=int(0.1 * total_steps), \n",
    "                                                num_training_steps=total_steps)\n",
    "    \n",
    "    scaler = amp.GradScaler()\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    # Checkpoint names\n",
    "    last_ckpt_path = os.path.join(CHECKPOINT_DIR, 'xlmr_base_final_last.pt')\n",
    "    best_ckpt_path = os.path.join(CHECKPOINT_DIR, 'xlmr_base_final_best.pt')\n",
    "    \n",
    "    start_epoch = 0\n",
    "    \n",
    "    print(f\"üöÄ Starting SMART Training Strategy\")\n",
    "    print(f\"   Phase 1 (Epoch 1-2): Backbone Frozen (Heads only)\")\n",
    "    print(f\"   Phase 2 (Epoch 3+):  Full Fine-Tuning (Lower LR)\")\n",
    "    \n",
    "    for epoch in range(start_epoch, total_epochs):\n",
    "        start_time = time.time()\n",
    "        print(f\"\\nEpoch {epoch+1}/{total_epochs}\")\n",
    "        \n",
    "        # SMART FREEZING & LR LOGIC\n",
    "        if epoch < 2:\n",
    "            freeze_backbone(model, freeze=True)\n",
    "            # Standard LR for heads\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = 2e-5\n",
    "        else:\n",
    "            freeze_backbone(model, freeze=False)\n",
    "            # üî• CRITICAL FIX: Lower LR by 10x when unfreezing backbone\n",
    "            # This prevents the \"Epoch 4 Divergence\" we saw earlier\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = 2e-6 \n",
    "            \n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        loop = tqdm(train_loader, leave=True)\n",
    "        for batch in loop:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            targets = {k: batch[k].to(device) for k in ['hate_type', 'target_group', 'severity']}\n",
    "            masks = {k: batch[f'{k}_mask'].to(device) for k in ['hate_type', 'target_group', 'severity']}\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with amp.autocast():\n",
    "                ht_logits, tg_logits, sv_logits = model(input_ids, attention_mask)\n",
    "                loss = multitask_loss(ht_logits, tg_logits, sv_logits, targets, masks)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            # scheduler.step() # Disable scheduler since we manually control LR\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            loop.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "            \n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        print(f\"Average Train Loss: {avg_train_loss:.4f}\")\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                targets = {k: batch[k].to(device) for k in ['hate_type', 'target_group', 'severity']}\n",
    "                masks = {k: batch[f'{k}_mask'].to(device) for k in ['hate_type', 'target_group', 'severity']}\n",
    "                \n",
    "                ht_logits, tg_logits, sv_logits = model(input_ids, attention_mask)\n",
    "                loss = multitask_loss(ht_logits, tg_logits, sv_logits, targets, masks)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        epoch_time = (time.time() - start_time) / 60\n",
    "        print(f\"Validation Loss: {avg_val_loss:.4f} | Time: {epoch_time:.1f} min\")\n",
    "        \n",
    "        # Save Best Checkpoint\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), best_ckpt_path)\n",
    "            print(f\"üî• New Best Model Saved: {best_ckpt_path}\")\n",
    "            \n",
    "        # Save Last Checkpoint\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_val_loss': best_val_loss\n",
    "        }, last_ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c02833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Prepare Data Loaders\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "# Split Data\n",
    "train_df = df[df['split'] == 'train']\n",
    "val_df = df[df['split'] == 'val']\n",
    "\n",
    "train_dataset = HateDataset(train_df, tokenizer)\n",
    "val_dataset = HateDataset(val_df, tokenizer)\n",
    "\n",
    "# üöÄ BATCH SIZE CONFIGURATION\n",
    "# User requested fixed batch size of 16\n",
    "BATCH_SIZE = 16\n",
    "print(f\"üöÄ Using Fixed Batch Size: {BATCH_SIZE}\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525d8637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Training Loop (Smart Layer-Wise Learning)\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.cuda.amp as amp\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# üõ†Ô∏è Memory Optimization\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "def freeze_backbone(model, freeze=True):\n",
    "    for param in model.backbone.parameters():\n",
    "        param.requires_grad = not freeze\n",
    "    status = \"‚ùÑÔ∏è FROZEN\" if freeze else \"üî• UNFROZEN\"\n",
    "    print(f\"Model Backbone is now {status}\")\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    all_preds = {'hate_type': [], 'target_group': [], 'severity': []}\n",
    "    all_labels = {'hate_type': [], 'target_group': [], 'severity': []}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            targets = {k: batch[k].to(device) for k in ['hate_type', 'target_group', 'severity']}\n",
    "            \n",
    "            # Fix masks for validation\n",
    "            masks = {\n",
    "                'hate_type': batch['hate_type_mask'].to(device),\n",
    "                'target_group': batch['target_group_mask'].to(device),\n",
    "                'severity': batch['severity_mask'].to(device)\n",
    "            }\n",
    "            \n",
    "            ht_logits, tg_logits, sv_logits = model(input_ids, attention_mask)\n",
    "            loss = multitask_loss(ht_logits, tg_logits, sv_logits, targets, masks)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Collect preds for F1\n",
    "            for task, logits in zip(['hate_type', 'target_group', 'severity'], [ht_logits, tg_logits, sv_logits]):\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                mask = masks[task].bool()\n",
    "                \n",
    "                if mask.any():\n",
    "                    all_preds[task].extend(preds[mask].cpu().numpy())\n",
    "                    all_labels[task].extend(targets[task][mask].cpu().numpy())\n",
    "                    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    \n",
    "    metrics = {}\n",
    "    for task in ['hate_type', 'target_group', 'severity']:\n",
    "        if len(all_labels[task]) > 0:\n",
    "            metrics[task] = {\n",
    "                'macro_f1': f1_score(all_labels[task], all_preds[task], average='macro'),\n",
    "                'micro_f1': f1_score(all_labels[task], all_preds[task], average='micro')\n",
    "            }\n",
    "        else:\n",
    "            metrics[task] = {'macro_f1': 0.0, 'micro_f1': 0.0}\n",
    "        \n",
    "    return avg_loss, metrics\n",
    "\n",
    "def train_model(total_epochs=5, resume_from_epoch=None):\n",
    "    torch.cuda.empty_cache() # üßπ Clear GPU cache before starting\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = MultiTaskXLMRRoberta(dropout=0.2).to(device) \n",
    "    \n",
    "    # üõ†Ô∏è Dynamic Gradient Accumulation\n",
    "    # We aim for an Effective Batch Size of 16\n",
    "    # If BATCH_SIZE is 16 (A100), Steps = 1\n",
    "    # If BATCH_SIZE is 4 (T4), Steps = 4\n",
    "    TARGET_EFFECTIVE_BATCH = 16\n",
    "    ACCUMULATION_STEPS = max(1, TARGET_EFFECTIVE_BATCH // BATCH_SIZE)\n",
    "    \n",
    "    print(f\"‚öôÔ∏è Configuration:\")\n",
    "    print(f\"   - Physical Batch Size: {BATCH_SIZE}\")\n",
    "    print(f\"   - Accumulation Steps:  {ACCUMULATION_STEPS}\")\n",
    "    print(f\"   - Effective Batch Size: {BATCH_SIZE * ACCUMULATION_STEPS}\")\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "    \n",
    "    # Adjust total steps for accumulation\n",
    "    total_steps = (len(train_loader) // ACCUMULATION_STEPS) * total_epochs\n",
    "    \n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(0.1*total_steps), num_training_steps=total_steps)\n",
    "    scaler = torch.amp.GradScaler('cuda') \n",
    "    \n",
    "    start_epoch = 0\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    # Checkpoint names\n",
    "    last_ckpt_path = os.path.join(CHECKPOINT_DIR, 'xlmr_smart_last.pt')\n",
    "    best_ckpt_path = os.path.join(CHECKPOINT_DIR, 'xlmr_smart_best.pt')\n",
    "    \n",
    "    # Resume logic\n",
    "    if os.path.exists(last_ckpt_path):\n",
    "        print(f\"üîÑ Found checkpoint at {last_ckpt_path}. Attempting to resume...\")\n",
    "        try:\n",
    "            checkpoint = torch.load(last_ckpt_path)\n",
    "            \n",
    "            # CASE 1: Full Checkpoint (Ideal)\n",
    "            if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
    "                model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "                if 'scheduler_state_dict' in checkpoint:\n",
    "                    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "                if 'scaler_state_dict' in checkpoint:\n",
    "                    scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "                \n",
    "                start_epoch = checkpoint['epoch'] + 1\n",
    "                best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n",
    "                print(f\"‚úÖ Successfully resumed from Epoch {start_epoch+1}\")\n",
    "                \n",
    "            # CASE 2: Weights Only (Fallback)\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è Checkpoint contains weights only (no optimizer state).\")\n",
    "                model.load_state_dict(checkpoint) # Try loading directly\n",
    "                print(\"‚úÖ Weights loaded.\")\n",
    "                \n",
    "                if resume_from_epoch is not None:\n",
    "                    start_epoch = resume_from_epoch\n",
    "                    print(f\"‚è© Forcing start from Epoch {start_epoch+1} (User specified)\")\n",
    "                else:\n",
    "                    print(\"‚ö†Ô∏è No epoch info found. Starting from Epoch 1.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading checkpoint: {e}\")\n",
    "            print(\"üöÄ Starting FRESH Smart Mode training.\")\n",
    "    else:\n",
    "        print(\"üöÄ Starting FRESH Smart Mode training.\")\n",
    "\n",
    "    # Override if user specified\n",
    "    if resume_from_epoch is not None and start_epoch == 0:\n",
    "         start_epoch = resume_from_epoch\n",
    "         print(f\"‚è© Manual override: Starting from Epoch {start_epoch+1}\")\n",
    "\n",
    "    if start_epoch >= total_epochs:\n",
    "        print(\"‚úÖ Training already completed!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üöÄ Starting SMART Training Strategy (10 Epochs)\")\n",
    "    print(f\"   Phase 1 (Epoch 1):   Backbone Frozen (Heads Warmup)\")\n",
    "    print(f\"   Phase 2 (Epoch 2-10): Full Fine-Tuning\")\n",
    "    \n",
    "    for epoch in range(start_epoch, total_epochs):\n",
    "        start_time = time.time()\n",
    "        print(f\"\\nEpoch {epoch+1}/{total_epochs}\")\n",
    "        \n",
    "        # SMART FREEZING LOGIC\n",
    "        if epoch < 1:\n",
    "            freeze_backbone(model, freeze=True)\n",
    "        else:\n",
    "            freeze_backbone(model, freeze=False)\n",
    "            \n",
    "        model.train()\n",
    "        optimizer.zero_grad() # Reset gradients\n",
    "        total_loss = 0\n",
    "        \n",
    "        loop = tqdm(train_loader, leave=True)\n",
    "        \n",
    "        for i, batch in enumerate(loop):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            # Forward (Mixed Precision)\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                ht_logits, tg_logits, sv_logits = model(input_ids, attention_mask)\n",
    "                \n",
    "                targets = {k: v.to(device) for k, v in batch.items() if k in ['hate_type', 'target_group', 'severity']}\n",
    "                masks = {\n",
    "                    'hate_type': batch['hate_type_mask'].to(device),\n",
    "                    'target_group': batch['target_group_mask'].to(device),\n",
    "                    'severity': batch['severity_mask'].to(device)\n",
    "                }\n",
    "                \n",
    "                loss = multitask_loss(ht_logits, tg_logits, sv_logits, targets, masks)\n",
    "                loss = loss / ACCUMULATION_STEPS # Normalize loss\n",
    "            \n",
    "            # Backward\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            # Step Optimizer every ACCUMULATION_STEPS\n",
    "            if (i + 1) % ACCUMULATION_STEPS == 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            total_loss += loss.item() * ACCUMULATION_STEPS\n",
    "            loop.set_description(f\"Loss: {(loss.item() * ACCUMULATION_STEPS):.4f}\")\n",
    "            \n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        \n",
    "        # Validation\n",
    "        val_loss, metrics = evaluate(model, val_loader, device)\n",
    "        \n",
    "        print(f\"üìâ Train Loss: {avg_train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"üèÜ Val Macro F1 - HT: {metrics['hate_type']['macro_f1']:.3f} | TG: {metrics['target_group']['macro_f1']:.3f} | SV: {metrics['severity']['macro_f1']:.3f}\")\n",
    "        \n",
    "        # Save Best Model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), best_ckpt_path)\n",
    "            print(f\"üî• New Best Model Saved: {best_ckpt_path}\")\n",
    "            \n",
    "        # Save Last Checkpoint\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'scaler_state_dict': scaler.state_dict(),\n",
    "            'best_val_loss': best_val_loss\n",
    "        }, last_ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b84f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ START TRAINING\n",
    "# We run for 10 epochs as requested to ensure deep learning on the balanced dataset.\n",
    "train_model(total_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01989874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Final Evaluation on Test Set (Smart Model)\n",
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def evaluate_test_set():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    print(\"‚è≥ Loading best SMART model for evaluation...\")\n",
    "    model = MultiTaskXLMRRoberta(dropout=0.3) \n",
    "    # Load best checkpoint\n",
    "    ckpt_path = os.path.join(CHECKPOINT_DIR, 'xlmr_smart_best.pt')\n",
    "    if not os.path.exists(ckpt_path):\n",
    "        print(\"‚ùå No smart checkpoint found. Run training first.\")\n",
    "        return\n",
    "        \n",
    "    model.load_state_dict(torch.load(ckpt_path))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Prepare Test Loader\n",
    "    test_df = df[df['split'] == 'test']\n",
    "    test_dataset = HateDataset(test_df, tokenizer)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
    "    \n",
    "    print(f\"üöÄ Evaluating on {len(test_df)} test samples...\")\n",
    "    \n",
    "    # Storage for predictions and labels\n",
    "    task_metrics = {\n",
    "        'hate_type': {'preds': [], 'labels': []},\n",
    "        'target_group': {'preds': [], 'labels': []},\n",
    "        'severity': {'preds': [], 'labels': []}\n",
    "    }\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            ht_logits, tg_logits, sv_logits = model(input_ids, attention_mask)\n",
    "            \n",
    "            # Helper to collect valid predictions\n",
    "            def collect(logits, task_name):\n",
    "                preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "                labels = batch[task_name].cpu().numpy()\n",
    "                masks = batch[f'{task_name}_mask'].cpu().numpy()\n",
    "                \n",
    "                for p, l, m in zip(preds, labels, masks):\n",
    "                    if m: # Only keep valid labels\n",
    "                        task_metrics[task_name]['preds'].append(p)\n",
    "                        task_metrics[task_name]['labels'].append(l)\n",
    "\n",
    "            collect(ht_logits, 'hate_type')\n",
    "            collect(tg_logits, 'target_group')\n",
    "            collect(sv_logits, 'severity')\n",
    "            \n",
    "    # Print Reports\n",
    "    for task, data in task_metrics.items():\n",
    "        print(f\"\\nüìä --- {task.upper().replace('_', ' ')} Report ---\")\n",
    "        if len(data['labels']) > 0:\n",
    "            print(classification_report(data['labels'], data['preds'], digits=4))\n",
    "        else:\n",
    "            print(\"No valid labels found for this task in test set.\")\n",
    "\n",
    "evaluate_test_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9dc9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Visualize Confusion Matrices (Thesis Quality Plots)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "def plot_confusion_matrices(model, loader, device):\n",
    "    model.eval()\n",
    "    all_preds = {'hate_type': [], 'target_group': [], 'severity': []}\n",
    "    all_labels = {'hate_type': [], 'target_group': [], 'severity': []}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Collecting Predictions\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            ht_logits, tg_logits, sv_logits = model(input_ids, attention_mask)\n",
    "            \n",
    "            # Helper to collect valid predictions\n",
    "            def collect(logits, task_name):\n",
    "                preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "                labels = batch[task_name].cpu().numpy()\n",
    "                masks = batch[f'{task_name}_mask'].cpu().numpy()\n",
    "                \n",
    "                for p, l, m in zip(preds, labels, masks):\n",
    "                    if m: # Only keep valid labels\n",
    "                        all_preds[task_name].append(p)\n",
    "                        all_labels[task_name].append(l)\n",
    "\n",
    "            collect(ht_logits, 'hate_type')\n",
    "            collect(tg_logits, 'target_group')\n",
    "            collect(sv_logits, 'severity')\n",
    "\n",
    "    # --- LABELS (Update these based on your specific dataset mapping) ---\n",
    "    # Based on standard mappings:\n",
    "    ht_labels = ['Not Hate', 'Political', 'Religious', 'Gender', 'Geopolitical', 'Personal'] \n",
    "    tg_labels = ['Other', 'Individual', 'Organization', 'Community']\n",
    "    sv_labels = ['Level 0', 'Level 1', 'Level 2', 'Level 3']\n",
    "\n",
    "    # Plotting\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(24, 8))\n",
    "    \n",
    "    # Task 1: Hate Type\n",
    "    cm_ht = confusion_matrix(all_labels['hate_type'], all_preds['hate_type'])\n",
    "    sns.heatmap(cm_ht, annot=True, fmt='d', cmap='Blues', ax=axes[0], \n",
    "                xticklabels=ht_labels, yticklabels=ht_labels)\n",
    "    axes[0].set_title('Hate Type Confusion Matrix', fontsize=14)\n",
    "    axes[0].set_xlabel('Predicted', fontsize=12)\n",
    "    axes[0].set_ylabel('Actual', fontsize=12)\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Task 2: Target Group\n",
    "    cm_tg = confusion_matrix(all_labels['target_group'], all_preds['target_group'])\n",
    "    sns.heatmap(cm_tg, annot=True, fmt='d', cmap='Greens', ax=axes[1],\n",
    "                xticklabels=tg_labels, yticklabels=tg_labels)\n",
    "    axes[1].set_title('Target Group Confusion Matrix', fontsize=14)\n",
    "    axes[1].set_xlabel('Predicted', fontsize=12)\n",
    "    axes[1].set_ylabel('Actual', fontsize=12)\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Task 3: Severity\n",
    "    cm_sv = confusion_matrix(all_labels['severity'], all_preds['severity'])\n",
    "    sns.heatmap(cm_sv, annot=True, fmt='d', cmap='Oranges', ax=axes[2],\n",
    "                xticklabels=sv_labels, yticklabels=sv_labels)\n",
    "    axes[2].set_title('Severity Confusion Matrix', fontsize=14)\n",
    "    axes[2].set_xlabel('Predicted', fontsize=12)\n",
    "    axes[2].set_ylabel('Actual', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save for Thesis\n",
    "    save_path = os.path.join(CHECKPOINT_DIR, 'confusion_matrices.png')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"‚úÖ Plot saved to: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Run Visualization\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = MultiTaskXLMRRoberta(dropout=0.3)\n",
    "model.load_state_dict(torch.load(os.path.join(CHECKPOINT_DIR, 'xlmr_smart_best.pt')))\n",
    "model.to(device)\n",
    "\n",
    "test_df = df[df['split'] == 'test']\n",
    "test_dataset = HateDataset(test_df, tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
    "\n",
    "plot_confusion_matrices(model, test_loader, device)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
