{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6a82a80",
   "metadata": {},
   "source": [
    "# üöÄ Final Training Notebook for Multilingual Hate Detection\n",
    "This notebook is optimized for Google Colab (T4/A100 GPU).\n",
    "It uses the **ENHANCED** dataset (`UNIFIED_ALL_SPLIT_ENHANCED.csv`) which includes:\n",
    "- 14,000+ recovered neutral Bengali samples.\n",
    "- Banglish keyword fixes.\n",
    "- Strict consistency checks (No Safe=Hate errors).\n",
    "\n",
    "**Steps:**\n",
    "1. Mount Drive.\n",
    "2. Install Dependencies.\n",
    "3. Load Data.\n",
    "4. Train with Mixed Precision (AMP).\n",
    "5. Save Checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab86167b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create checkpoint directory\n",
    "import os\n",
    "CHECKPOINT_DIR = '/content/drive/MyDrive/thesis/checkpoints/'\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "print(f\"‚úÖ Checkpoints will be saved to: {CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e017cb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Install Dependencies & Check GPU\n",
    "# Force upgrade to fix 'GenerationMixin' error and ensure compatibility\n",
    "!pip install -U transformers accelerate wandb scikit-learn\n",
    "\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU Available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ùå No GPU detected. Please enable GPU in Runtime > Change runtime type.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7fbfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Pre-download Model (Turbo Mode üöÄ)\n",
    "import os\n",
    "\n",
    "print(\"‚è≥ Downloading XLM-RoBERTa Large (High Speed)...\")\n",
    "\n",
    "# 1. Install hf_transfer (Rust-based accelerator)\n",
    "!pip install -q huggingface_hub hf_transfer\n",
    "\n",
    "# 2. Enable the accelerator\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "\n",
    "# 3. Download\n",
    "# Note: If this fails with \"command not found\", restart the runtime once.\n",
    "!huggingface-cli download facebook/xlm-roberta-large --local-dir ./xlm-roberta-large --exclude \"*.msgpack\" \"*.h5\" \"*.ot\"\n",
    "\n",
    "print(\"‚úÖ Model downloaded to ./xlm-roberta-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e9007b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Path to the uploaded BALANCED dataset\n",
    "DATASET_PATH = '/content/drive/MyDrive/thesis/dataset/UNIFIED_BALANCED_GENERATED.csv'\n",
    "\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    print(f\"‚ùå Error: Dataset not found at {DATASET_PATH}\")\n",
    "    print(\"Please upload 'UNIFIED_BALANCED_GENERATED.csv' to 'thesis/dataset/' in your Drive.\")\n",
    "else:\n",
    "    df = pd.read_csv(DATASET_PATH)\n",
    "    print(f\"‚úÖ Loaded dataset: {len(df)} rows\")\n",
    "    print(\"\\nSplit distribution:\")\n",
    "    print(df['split'].value_counts())\n",
    "    print(\"\\nLanguage distribution:\")\n",
    "    print(df['language'].value_counts())\n",
    "    print(\"\\nSeverity distribution:\")\n",
    "    print(df['severity'].value_counts().sort_index())\n",
    "    print(\"\\nHate Type distribution:\")\n",
    "    print(df['hate_type'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5ae443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Define Dataset Class\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import XLMRobertaTokenizer\n",
    "\n",
    "class HateDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=160):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        text = str(row['text'])\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Extract labels (use -1 for missing)\n",
    "        hate_type = int(row['hate_type'])\n",
    "        target_group = int(row['target_group'])\n",
    "        severity = int(row['severity'])\n",
    "        \n",
    "        # Create masks: True if label is valid (not -1)\n",
    "        hate_type_mask = hate_type != -1\n",
    "        target_group_mask = target_group != -1\n",
    "        severity_mask = severity != -1\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'hate_type': torch.tensor(max(0, hate_type), dtype=torch.long),\n",
    "            'target_group': torch.tensor(max(0, target_group), dtype=torch.long),\n",
    "            'severity': torch.tensor(max(0, severity), dtype=torch.long),\n",
    "            'hate_type_mask': torch.tensor(hate_type_mask, dtype=torch.bool),\n",
    "            'target_group_mask': torch.tensor(target_group_mask, dtype=torch.bool),\n",
    "            'severity_mask': torch.tensor(severity_mask, dtype=torch.bool),\n",
    "        }\n",
    "\n",
    "# Initialize Tokenizer\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-large')\n",
    "print(\"‚úÖ Tokenizer loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0f9b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Mount Drive if needed (Colab specific)\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    DATASET_PATH = '/content/drive/MyDrive/thesis/dataset/UNIFIED_BALANCED_GENERATED.csv'\n",
    "    if not os.path.exists(DATASET_PATH):\n",
    "        print(\"Please upload 'UNIFIED_BALANCED_GENERATED.csv' to 'thesis/dataset/' in your Drive.\")\n",
    "except:\n",
    "    DATASET_PATH = 'dataset/UNIFIED_BALANCED_GENERATED.csv' # Local fallback\n",
    "\n",
    "print(f\"üìÇ Loading dataset from: {DATASET_PATH}\")\n",
    "df = pd.read_csv(DATASET_PATH)\n",
    "\n",
    "# Basic cleaning\n",
    "df = df.fillna(-1)\n",
    "print(f\"‚úÖ Loaded {len(df)} samples\")\n",
    "print(df['language'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e3d438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none', weight=self.alpha)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "# üî• UPDATED WEIGHTS FOR BALANCED DATASET (3:1 Ratio)\n",
    "# Not Hate (~80k) vs Specific Hate (~30k each)\n",
    "# The imbalance is now small (~2.7:1), so we use mild weights.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Weights ~ Inverse Frequency\n",
    "# Not Hate: 1.0 (Baseline)\n",
    "# Specific Hate (Pol, Rel, Gen, Geo): ~2.5\n",
    "# Personal: ~2.0\n",
    "ht_weights = torch.tensor([1.0, 2.5, 2.5, 2.5, 2.0, 2.5], device=device, dtype=torch.float)\n",
    "\n",
    "# Target Group: Community is dominant (~90k), Individual/Group (~20k)\n",
    "# Weights: Community=1.0, Others=3.0\n",
    "tg_weights = torch.tensor([1.0, 3.0, 3.0, 1.0], device=device, dtype=torch.float) \n",
    "\n",
    "# Severity: Balanced-ish\n",
    "sv_weights = torch.tensor([1.0, 1.5, 1.5, 2.0], device=device, dtype=torch.float)\n",
    "\n",
    "# Initialize Weighted Focal Loss\n",
    "loss_fn_ht = FocalLoss(alpha=ht_weights, gamma=2.0)\n",
    "loss_fn_tg = FocalLoss(alpha=tg_weights, gamma=2.0)\n",
    "loss_fn_sv = FocalLoss(alpha=sv_weights, gamma=2.0)\n",
    "\n",
    "def multitask_loss(hate_type_logits, target_group_logits, severity_logits,\n",
    "                   targets, masks, task_weights=(1.0, 1.0, 1.0)):\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    \n",
    "    # Hate type loss\n",
    "    ht_mask = masks['hate_type'].bool()\n",
    "    if ht_mask.any():\n",
    "        loss_ht = loss_fn_ht(hate_type_logits[ht_mask], targets['hate_type'][ht_mask])\n",
    "        total_loss += task_weights[0] * loss_ht\n",
    "        \n",
    "    # Target group loss\n",
    "    tg_mask = masks['target_group'].bool()\n",
    "    if tg_mask.any():\n",
    "        loss_tg = loss_fn_tg(target_group_logits[tg_mask], targets['target_group'][tg_mask])\n",
    "        total_loss += task_weights[1] * loss_tg\n",
    "        \n",
    "    # Severity loss\n",
    "    sv_mask = masks['severity'].bool()\n",
    "    if sv_mask.any():\n",
    "        loss_sv = loss_fn_sv(severity_logits[sv_mask], targets['severity'][sv_mask])\n",
    "        total_loss += task_weights[2] * loss_sv\n",
    "        \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5651be54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Define Model Architecture\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from transformers import XLMRobertaModel\n",
    "\n",
    "class MultiTaskXLMRRoberta(nn.Module):\n",
    "    def __init__(self, model_name='xlm-roberta-large', dropout=0.3):\n",
    "        super(MultiTaskXLMRRoberta, self).__init__()\n",
    "        \n",
    "        # Check if local cache exists (from CLI download)\n",
    "        if model_name == 'xlm-roberta-large' and os.path.exists('./xlm-roberta-large'):\n",
    "            print(f\"üìÇ Loading model from local cache: ./xlm-roberta-large\")\n",
    "            model_name = './xlm-roberta-large'\n",
    "            \n",
    "        self.backbone = XLMRobertaModel.from_pretrained(model_name)\n",
    "        self.hidden_size = self.backbone.config.hidden_size\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Classification Heads\n",
    "        self.hate_type_head = nn.Linear(self.hidden_size, 6)    # 0-5\n",
    "        self.target_group_head = nn.Linear(self.hidden_size, 4) # 0-3\n",
    "        self.severity_head = nn.Linear(self.hidden_size, 4)     # 0-3\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Use CLS token representation (first token)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        hate_type_logits = self.hate_type_head(pooled_output)\n",
    "        target_group_logits = self.target_group_head(pooled_output)\n",
    "        severity_logits = self.severity_head(pooled_output)\n",
    "        \n",
    "        return hate_type_logits, target_group_logits, severity_logits\n",
    "\n",
    "print(\"‚úÖ MultiTaskXLMRRoberta model defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee878d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Training Loop with Smart LR Adjustment\n",
    "# We need to lower LR when unfreezing to prevent divergence\n",
    "\n",
    "def train_model(model, train_loader, val_loader, total_epochs=8):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=1e-2)\n",
    "    \n",
    "    # Scheduler: Linear warmup then decay\n",
    "    total_steps = len(train_loader) * total_epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                num_warmup_steps=int(0.1 * total_steps), \n",
    "                                                num_training_steps=total_steps)\n",
    "    \n",
    "    scaler = amp.GradScaler()\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    # Checkpoint names\n",
    "    last_ckpt_path = os.path.join(CHECKPOINT_DIR, 'xlmr_smart_last.pt')\n",
    "    best_ckpt_path = os.path.join(CHECKPOINT_DIR, 'xlmr_smart_best.pt')\n",
    "    \n",
    "    start_epoch = 0\n",
    "    \n",
    "    print(f\"üöÄ Starting SMART Training Strategy\")\n",
    "    print(f\"   Phase 1 (Epoch 1-2): Backbone Frozen (Heads only)\")\n",
    "    print(f\"   Phase 2 (Epoch 3+):  Full Fine-Tuning (Lower LR)\")\n",
    "    \n",
    "    for epoch in range(start_epoch, total_epochs):\n",
    "        start_time = time.time()\n",
    "        print(f\"\\nEpoch {epoch+1}/{total_epochs}\")\n",
    "        \n",
    "        # SMART FREEZING & LR LOGIC\n",
    "        if epoch < 2:\n",
    "            freeze_backbone(model, freeze=True)\n",
    "            # Standard LR for heads\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = 2e-5\n",
    "        else:\n",
    "            freeze_backbone(model, freeze=False)\n",
    "            # üî• CRITICAL FIX: Lower LR by 10x when unfreezing backbone\n",
    "            # This prevents the \"Epoch 4 Divergence\" we saw earlier\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = 2e-6 \n",
    "            \n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        loop = tqdm(train_loader, leave=True)\n",
    "        for batch in loop:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            targets = {k: batch[k].to(device) for k in ['hate_type', 'target_group', 'severity']}\n",
    "            masks = {k: batch[f'{k}_mask'].to(device) for k in ['hate_type', 'target_group', 'severity']}\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with amp.autocast():\n",
    "                ht_logits, tg_logits, sv_logits = model(input_ids, attention_mask)\n",
    "                loss = multitask_loss(ht_logits, tg_logits, sv_logits, targets, masks)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            # scheduler.step() # Disable scheduler since we manually control LR\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            loop.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "            \n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        print(f\"Average Train Loss: {avg_train_loss:.4f}\")\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                targets = {k: batch[k].to(device) for k in ['hate_type', 'target_group', 'severity']}\n",
    "                masks = {k: batch[f'{k}_mask'].to(device) for k in ['hate_type', 'target_group', 'severity']}\n",
    "                \n",
    "                ht_logits, tg_logits, sv_logits = model(input_ids, attention_mask)\n",
    "                loss = multitask_loss(ht_logits, tg_logits, sv_logits, targets, masks)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        epoch_time = (time.time() - start_time) / 60\n",
    "        print(f\"Validation Loss: {avg_val_loss:.4f} | Time: {epoch_time:.1f} min\")\n",
    "        \n",
    "        # Save Best Checkpoint\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), best_ckpt_path)\n",
    "            print(f\"üî• New Best Model Saved: {best_ckpt_path}\")\n",
    "            \n",
    "        # Save Last Checkpoint\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_val_loss': best_val_loss\n",
    "        }, last_ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c02833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Prepare Data Loaders\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Note: We removed manual class weights because Focal Loss handles imbalance dynamically.\n",
    "# This prevents the \"Paranoia\" issue where manual weights force the model to over-predict rare classes.\n",
    "\n",
    "# Split Data\n",
    "train_df = df[df['split'] == 'train']\n",
    "val_df = df[df['split'] == 'val']\n",
    "\n",
    "train_dataset = HateDataset(train_df, tokenizer)\n",
    "val_dataset = HateDataset(val_df, tokenizer)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525d8637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Training Loop (Smart Layer-Wise Learning)\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.cuda.amp as amp\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "\n",
    "def freeze_backbone(model, freeze=True):\n",
    "    for param in model.backbone.parameters():\n",
    "        param.requires_grad = not freeze\n",
    "    status = \"‚ùÑÔ∏è FROZEN\" if freeze else \"üî• UNFROZEN\"\n",
    "    print(f\"Model Backbone is now {status}\")\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    all_preds = {'hate_type': [], 'target_group': [], 'severity': []}\n",
    "    all_labels = {'hate_type': [], 'target_group': [], 'severity': []}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            targets = {k: batch[k].to(device) for k in ['hate_type', 'target_group', 'severity']}\n",
    "            \n",
    "            # Fix masks for validation\n",
    "            masks = {\n",
    "                'hate_type': batch['hate_type_mask'].to(device),\n",
    "                'target_group': batch['target_group_mask'].to(device),\n",
    "                'severity': batch['severity_mask'].to(device)\n",
    "            }\n",
    "            \n",
    "            ht_logits, tg_logits, sv_logits = model(input_ids, attention_mask)\n",
    "            loss = multitask_loss(ht_logits, tg_logits, sv_logits, targets, masks)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Collect preds for F1\n",
    "            for task, logits in zip(['hate_type', 'target_group', 'severity'], [ht_logits, tg_logits, sv_logits]):\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                mask = masks[task].bool()\n",
    "                \n",
    "                if mask.any():\n",
    "                    all_preds[task].extend(preds[mask].cpu().numpy())\n",
    "                    all_labels[task].extend(targets[task][mask].cpu().numpy())\n",
    "                    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    \n",
    "    metrics = {}\n",
    "    for task in ['hate_type', 'target_group', 'severity']:\n",
    "        if len(all_labels[task]) > 0:\n",
    "            metrics[task] = {\n",
    "                'macro_f1': f1_score(all_labels[task], all_preds[task], average='macro'),\n",
    "                'micro_f1': f1_score(all_labels[task], all_preds[task], average='micro')\n",
    "            }\n",
    "        else:\n",
    "            metrics[task] = {'macro_f1': 0.0, 'micro_f1': 0.0}\n",
    "        \n",
    "    return avg_loss, metrics\n",
    "\n",
    "def train_model(total_epochs=10, resume_from_epoch=None):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = MultiTaskXLMRRoberta(dropout=0.3).to(device) \n",
    "    \n",
    "    # Optimizer\n",
    "    # Lower learning rate for stability over 10 epochs\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "    \n",
    "    total_steps = len(train_loader) * total_epochs\n",
    "    \n",
    "    # Warmup for 10% of steps\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(0.1*total_steps), num_training_steps=total_steps)\n",
    "    scaler = torch.amp.GradScaler('cuda') \n",
    "    \n",
    "    start_epoch = 0\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    # Checkpoint names\n",
    "    last_ckpt_path = os.path.join(CHECKPOINT_DIR, 'xlmr_smart_last.pt')\n",
    "    best_ckpt_path = os.path.join(CHECKPOINT_DIR, 'xlmr_smart_best.pt')\n",
    "    \n",
    "    # Resume logic\n",
    "    if os.path.exists(last_ckpt_path):\n",
    "        print(f\"üîÑ Found checkpoint at {last_ckpt_path}. Attempting to resume...\")\n",
    "        try:\n",
    "            checkpoint = torch.load(last_ckpt_path)\n",
    "            \n",
    "            # CASE 1: Full Checkpoint (Ideal)\n",
    "            if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
    "                model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "                if 'scheduler_state_dict' in checkpoint:\n",
    "                    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "                if 'scaler_state_dict' in checkpoint:\n",
    "                    scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "                \n",
    "                start_epoch = checkpoint['epoch'] + 1\n",
    "                best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n",
    "                print(f\"‚úÖ Successfully resumed from Epoch {start_epoch+1}\")\n",
    "                \n",
    "            # CASE 2: Weights Only (Fallback)\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è Checkpoint contains weights only (no optimizer state).\")\n",
    "                model.load_state_dict(checkpoint) # Try loading directly\n",
    "                print(\"‚úÖ Weights loaded.\")\n",
    "                \n",
    "                # If user didn't specify where to start, we have to guess or start at 0\n",
    "                if resume_from_epoch is not None:\n",
    "                    start_epoch = resume_from_epoch\n",
    "                    print(f\"‚è© Forcing start from Epoch {start_epoch+1} (User specified)\")\n",
    "                else:\n",
    "                    print(\"‚ö†Ô∏è No epoch info found. Starting from Epoch 1 (but with pre-trained weights).\")\n",
    "                    print(\"üí° Tip: Call train_model(total_epochs=10, resume_from_epoch=2) to set correct epoch.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading checkpoint: {e}\")\n",
    "            print(\"üöÄ Starting FRESH Smart Mode training.\")\n",
    "    else:\n",
    "        print(\"üöÄ Starting FRESH Smart Mode training.\")\n",
    "\n",
    "    # Override if user specified\n",
    "    if resume_from_epoch is not None and start_epoch == 0:\n",
    "         start_epoch = resume_from_epoch\n",
    "         print(f\"‚è© Manual override: Starting from Epoch {start_epoch+1}\")\n",
    "\n",
    "    if start_epoch >= total_epochs:\n",
    "        print(\"‚úÖ Training already completed!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üöÄ Starting SMART Training Strategy (10 Epochs)\")\n",
    "    print(f\"   Phase 1 (Epoch 1):   Backbone Frozen (Heads Warmup)\")\n",
    "    print(f\"   Phase 2 (Epoch 2-10): Full Fine-Tuning\")\n",
    "    \n",
    "    for epoch in range(start_epoch, total_epochs):\n",
    "        start_time = time.time()\n",
    "        print(f\"\\nEpoch {epoch+1}/{total_epochs}\")\n",
    "        \n",
    "        # SMART FREEZING LOGIC\n",
    "        # Freeze only for the very first epoch to align the heads\n",
    "        if epoch < 1:\n",
    "            freeze_backbone(model, freeze=True)\n",
    "        else:\n",
    "            freeze_backbone(model, freeze=False)\n",
    "            \n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        loop = tqdm(train_loader, leave=True)\n",
    "        \n",
    "        for batch in loop:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            # Forward (Mixed Precision)\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                ht_logits, tg_logits, sv_logits = model(input_ids, attention_mask)\n",
    "                \n",
    "                # Move targets to device\n",
    "                targets = {k: v.to(device) for k, v in batch.items() if k in ['hate_type', 'target_group', 'severity']}\n",
    "                \n",
    "                # üî• FIX: Map mask keys correctly for multitask_loss\n",
    "                masks = {\n",
    "                    'hate_type': batch['hate_type_mask'].to(device),\n",
    "                    'target_group': batch['target_group_mask'].to(device),\n",
    "                    'severity': batch['severity_mask'].to(device)\n",
    "                }\n",
    "                \n",
    "                loss = multitask_loss(ht_logits, tg_logits, sv_logits, targets, masks)\n",
    "            \n",
    "            # Backward\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            loop.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "            \n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        \n",
    "        # Validation\n",
    "        val_loss, metrics = evaluate(model, val_loader, device)\n",
    "        \n",
    "        print(f\"üìâ Train Loss: {avg_train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"üèÜ Val Macro F1 - HT: {metrics['hate_type']['macro_f1']:.3f} | TG: {metrics['target_group']['macro_f1']:.3f} | SV: {metrics['severity']['macro_f1']:.3f}\")\n",
    "        \n",
    "        # Save Best Model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), best_ckpt_path)\n",
    "            print(f\"üî• New Best Model Saved: {best_ckpt_path}\")\n",
    "            \n",
    "        # Save Last Checkpoint (Include Scheduler & Scaler)\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'scaler_state_dict': scaler.state_dict(),\n",
    "            'best_val_loss': best_val_loss\n",
    "        }, last_ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b84f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ START TRAINING\n",
    "# We run for 10 epochs as requested to ensure deep learning on the balanced dataset.\n",
    "train_model(total_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01989874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Final Evaluation on Test Set (Smart Model)\n",
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def evaluate_test_set():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    print(\"‚è≥ Loading best SMART model for evaluation...\")\n",
    "    model = MultiTaskXLMRRoberta(dropout=0.3) \n",
    "    # Load best checkpoint\n",
    "    ckpt_path = os.path.join(CHECKPOINT_DIR, 'xlmr_smart_best.pt')\n",
    "    if not os.path.exists(ckpt_path):\n",
    "        print(\"‚ùå No smart checkpoint found. Run training first.\")\n",
    "        return\n",
    "        \n",
    "    model.load_state_dict(torch.load(ckpt_path))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Prepare Test Loader\n",
    "    test_df = df[df['split'] == 'test']\n",
    "    test_dataset = HateDataset(test_df, tokenizer)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
    "    \n",
    "    print(f\"üöÄ Evaluating on {len(test_df)} test samples...\")\n",
    "    \n",
    "    # Storage for predictions and labels\n",
    "    task_metrics = {\n",
    "        'hate_type': {'preds': [], 'labels': []},\n",
    "        'target_group': {'preds': [], 'labels': []},\n",
    "        'severity': {'preds': [], 'labels': []}\n",
    "    }\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            ht_logits, tg_logits, sv_logits = model(input_ids, attention_mask)\n",
    "            \n",
    "            # Helper to collect valid predictions\n",
    "            def collect(logits, task_name):\n",
    "                preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "                labels = batch[task_name].cpu().numpy()\n",
    "                masks = batch[f'{task_name}_mask'].cpu().numpy()\n",
    "                \n",
    "                for p, l, m in zip(preds, labels, masks):\n",
    "                    if m: # Only keep valid labels\n",
    "                        task_metrics[task_name]['preds'].append(p)\n",
    "                        task_metrics[task_name]['labels'].append(l)\n",
    "\n",
    "            collect(ht_logits, 'hate_type')\n",
    "            collect(tg_logits, 'target_group')\n",
    "            collect(sv_logits, 'severity')\n",
    "            \n",
    "    # Print Reports\n",
    "    for task, data in task_metrics.items():\n",
    "        print(f\"\\nüìä --- {task.upper().replace('_', ' ')} Report ---\")\n",
    "        if len(data['labels']) > 0:\n",
    "            print(classification_report(data['labels'], data['preds'], digits=4))\n",
    "        else:\n",
    "            print(\"No valid labels found for this task in test set.\")\n",
    "\n",
    "evaluate_test_set()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
