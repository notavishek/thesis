{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6a82a80",
   "metadata": {},
   "source": [
    "# ðŸš€ Final Training Notebook for Multilingual Hate Detection\n",
    "This notebook is optimized for Google Colab (T4/A100 GPU).\n",
    "It uses the **ENHANCED** dataset (`UNIFIED_ALL_SPLIT_ENHANCED.csv`) which includes:\n",
    "- 14,000+ recovered neutral Bengali samples.\n",
    "- Banglish keyword fixes.\n",
    "- Strict consistency checks (No Safe=Hate errors).\n",
    "\n",
    "**Steps:**\n",
    "1. Mount Drive.\n",
    "2. Install Dependencies.\n",
    "3. Load Data.\n",
    "4. Train with Mixed Precision (AMP).\n",
    "5. Save Checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab86167b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create checkpoint directory\n",
    "import os\n",
    "CHECKPOINT_DIR = '/content/drive/MyDrive/thesis/checkpoints/'\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "print(f\"âœ… Checkpoints will be saved to: {CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e017cb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Install Dependencies & Check GPU\n",
    "!pip install transformers wandb scikit-learn\n",
    "\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"âœ… GPU Available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"âŒ No GPU detected. Please enable GPU in Runtime > Change runtime type.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e9007b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Path to the uploaded BALANCED dataset\n",
    "DATASET_PATH = '/content/drive/MyDrive/thesis/dataset/UNIFIED_BALANCED.csv'\n",
    "\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    print(f\"âŒ Error: Dataset not found at {DATASET_PATH}\")\n",
    "    print(\"Please upload 'UNIFIED_BALANCED.csv' to 'thesis/dataset/' in your Drive.\")\n",
    "else:\n",
    "    df = pd.read_csv(DATASET_PATH)\n",
    "    print(f\"âœ… Loaded dataset: {len(df)} rows\")\n",
    "    print(\"\\nSplit distribution:\")\n",
    "    print(df['split'].value_counts())\n",
    "    print(\"\\nLanguage distribution:\")\n",
    "    print(df['language'].value_counts())\n",
    "    print(\"\\nSeverity distribution:\")\n",
    "    print(df['severity'].value_counts().sort_index())\n",
    "    print(\"\\nHate Type distribution:\")\n",
    "    print(df['hate_type'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5ae443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Define Dataset Class\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import XLMRobertaTokenizer\n",
    "\n",
    "class HateDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=160):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        text = str(row['text'])\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Extract labels (use -1 for missing)\n",
    "        hate_type = int(row['hate_type'])\n",
    "        target_group = int(row['target_group'])\n",
    "        severity = int(row['severity'])\n",
    "        \n",
    "        # Create masks: True if label is valid (not -1)\n",
    "        hate_type_mask = hate_type != -1\n",
    "        target_group_mask = target_group != -1\n",
    "        severity_mask = severity != -1\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'hate_type': torch.tensor(max(0, hate_type), dtype=torch.long),\n",
    "            'target_group': torch.tensor(max(0, target_group), dtype=torch.long),\n",
    "            'severity': torch.tensor(max(0, severity), dtype=torch.long),\n",
    "            'hate_type_mask': torch.tensor(hate_type_mask, dtype=torch.bool),\n",
    "            'target_group_mask': torch.tensor(target_group_mask, dtype=torch.bool),\n",
    "            'severity_mask': torch.tensor(severity_mask, dtype=torch.bool),\n",
    "        }\n",
    "\n",
    "# Initialize Tokenizer\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-large')\n",
    "print(\"âœ… Tokenizer loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0f9b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Define Model Architecture\n",
    "import torch.nn as nn\n",
    "from transformers import XLMRobertaModel\n",
    "\n",
    "class MultiTaskXLMRRoberta(nn.Module):\n",
    "    def __init__(self, model_name='xlm-roberta-large', dropout=0.5, # ðŸ”¥ HARD MODE: Increased Dropout to 50%\n",
    "                 n_hate_type=6, n_target_group=4, n_severity=4):\n",
    "        super().__init__()\n",
    "        self.backbone = XLMRobertaModel.from_pretrained(model_name)\n",
    "        hidden_size = self.backbone.config.hidden_size  # 1024 for large\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Task-specific classification heads\n",
    "        self.hate_type_head = nn.Linear(hidden_size, n_hate_type)\n",
    "        self.target_group_head = nn.Linear(hidden_size, n_target_group)\n",
    "        self.severity_head = nn.Linear(hidden_size, n_severity)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Use CLS token representation\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        cls_output = self.dropout(cls_output)\n",
    "        \n",
    "        hate_type_logits = self.hate_type_head(cls_output)\n",
    "        target_group_logits = self.target_group_head(cls_output)\n",
    "        severity_logits = self.severity_head(cls_output)\n",
    "        \n",
    "        return hate_type_logits, target_group_logits, severity_logits\n",
    "\n",
    "print(\"âœ… Model class defined (Dropout=0.5)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e3d438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Loss Function: Focal Loss (The Overfitting Killer)\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "# Initialize Focal Loss for each task\n",
    "# Gamma=2.0 focuses on hard examples.\n",
    "loss_fn_ht = FocalLoss(gamma=2.0)\n",
    "loss_fn_tg = FocalLoss(gamma=2.0)\n",
    "loss_fn_sv = FocalLoss(gamma=2.0)\n",
    "\n",
    "def multitask_loss(hate_type_logits, target_group_logits, severity_logits,\n",
    "                   targets, masks, task_weights=(1.0, 1.0, 1.0)):\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    \n",
    "    # Hate type loss\n",
    "    ht_mask = masks['hate_type'].bool()\n",
    "    if ht_mask.any():\n",
    "        loss_ht = loss_fn_ht(hate_type_logits[ht_mask], targets['hate_type'][ht_mask])\n",
    "        total_loss += task_weights[0] * loss_ht\n",
    "        \n",
    "    # Target group loss\n",
    "    tg_mask = masks['target_group'].bool()\n",
    "    if tg_mask.any():\n",
    "        loss_tg = loss_fn_tg(target_group_logits[tg_mask], targets['target_group'][tg_mask])\n",
    "        total_loss += task_weights[1] * loss_tg\n",
    "        \n",
    "    # Severity loss\n",
    "    sv_mask = masks['severity'].bool()\n",
    "    if sv_mask.any():\n",
    "        loss_sv = loss_fn_sv(severity_logits[sv_mask], targets['severity'][sv_mask])\n",
    "        total_loss += task_weights[2] * loss_sv\n",
    "        \n",
    "    return total_loss\n",
    "\n",
    "print(\"âœ… Focal Loss defined (Replaces standard CrossEntropy)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c02833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Prepare Data Loaders\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Note: We removed manual class weights because Focal Loss handles imbalance dynamically.\n",
    "# This prevents the \"Paranoia\" issue where manual weights force the model to over-predict rare classes.\n",
    "\n",
    "# Split Data\n",
    "train_df = df[df['split'] == 'train']\n",
    "val_df = df[df['split'] == 'val']\n",
    "\n",
    "train_dataset = HateDataset(train_df, tokenizer)\n",
    "val_dataset = HateDataset(val_df, tokenizer)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525d8637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Training Loop (Smart Layer-Wise Learning)\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.cuda.amp as amp\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "\n",
    "def freeze_backbone(model, freeze=True):\n",
    "    for param in model.backbone.parameters():\n",
    "        param.requires_grad = not freeze\n",
    "    status = \"â„ï¸ FROZEN\" if freeze else \"ðŸ”¥ UNFROZEN\"\n",
    "    print(f\"Model Backbone is now {status}\")\n",
    "\n",
    "def train_model(total_epochs=8):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = MultiTaskXLMRRoberta(dropout=0.3).to(device) # Reduced dropout to 0.3 (Standard)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "    \n",
    "    total_steps = len(train_loader) * total_epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(0.1*total_steps), num_training_steps=total_steps)\n",
    "    scaler = amp.GradScaler() \n",
    "    \n",
    "    start_epoch = 0\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    # Checkpoint names\n",
    "    last_ckpt_path = os.path.join(CHECKPOINT_DIR, 'xlmr_smart_last.pt')\n",
    "    best_ckpt_path = os.path.join(CHECKPOINT_DIR, 'xlmr_smart_best.pt')\n",
    "    \n",
    "    # Resume logic\n",
    "    if os.path.exists(last_ckpt_path):\n",
    "        print(f\"ðŸ”„ Found interrupted Smart Mode training at {last_ckpt_path}. Resuming...\")\n",
    "        checkpoint = torch.load(last_ckpt_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        # scheduler.load_state_dict(checkpoint['scheduler_state_dict']) # Optional, sometimes better to restart scheduler\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n",
    "        print(f\"âœ… Resuming from Epoch {start_epoch+1}\")\n",
    "    else:\n",
    "        print(\"ðŸš€ Starting FRESH Smart Mode training.\")\n",
    "\n",
    "    if start_epoch >= total_epochs:\n",
    "        print(\"âœ… Training already completed!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"ðŸš€ Starting SMART Training Strategy\")\n",
    "    print(f\"   Phase 1 (Epoch 1-2): Backbone Frozen (Heads only)\")\n",
    "    print(f\"   Phase 2 (Epoch 3+):  Full Fine-Tuning\")\n",
    "    \n",
    "    for epoch in range(start_epoch, total_epochs):\n",
    "        start_time = time.time()\n",
    "        print(f\"\\nEpoch {epoch+1}/{total_epochs}\")\n",
    "        \n",
    "        # SMART FREEZING LOGIC\n",
    "        if epoch < 2:\n",
    "            freeze_backbone(model, freeze=True)\n",
    "        else:\n",
    "            freeze_backbone(model, freeze=False)\n",
    "            \n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        loop = tqdm(train_loader, leave=True)\n",
    "        for batch in loop:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            targets = {k: batch[k].to(device) for k in ['hate_type', 'target_group', 'severity']}\n",
    "            masks = {k: batch[f'{k}_mask'].to(device) for k in ['hate_type', 'target_group', 'severity']}\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with amp.autocast():\n",
    "                ht_logits, tg_logits, sv_logits = model(input_ids, attention_mask)\n",
    "                # Note: No manual weights passed, Focal Loss handles it internally\n",
    "                loss = multitask_loss(ht_logits, tg_logits, sv_logits, targets, masks)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            loop.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "            \n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        print(f\"Average Train Loss: {avg_train_loss:.4f}\")\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                targets = {k: batch[k].to(device) for k in ['hate_type', 'target_group', 'severity']}\n",
    "                masks = {k: batch[f'{k}_mask'].to(device) for k in ['hate_type', 'target_group', 'severity']}\n",
    "                \n",
    "                ht_logits, tg_logits, sv_logits = model(input_ids, attention_mask)\n",
    "                loss = multitask_loss(ht_logits, tg_logits, sv_logits, targets, masks)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        epoch_time = (time.time() - start_time) / 60\n",
    "        print(f\"Validation Loss: {avg_val_loss:.4f} | Time: {epoch_time:.1f} min\")\n",
    "        \n",
    "        # Save Best Checkpoint\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), best_ckpt_path)\n",
    "            print(f\"ðŸ”¥ New Best Model Saved: {best_ckpt_path}\")\n",
    "            \n",
    "        # Save Last Checkpoint\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_val_loss': best_val_loss\n",
    "        }, last_ckpt_path)\n",
    "\n",
    "# Run Training\n",
    "train_model(total_epochs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01989874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Final Evaluation on Test Set (Smart Model)\n",
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def evaluate_test_set():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    print(\"â³ Loading best SMART model for evaluation...\")\n",
    "    model = MultiTaskXLMRRoberta(dropout=0.3) \n",
    "    # Load best checkpoint\n",
    "    ckpt_path = os.path.join(CHECKPOINT_DIR, 'xlmr_smart_best.pt')\n",
    "    if not os.path.exists(ckpt_path):\n",
    "        print(\"âŒ No smart checkpoint found. Run training first.\")\n",
    "        return\n",
    "        \n",
    "    model.load_state_dict(torch.load(ckpt_path))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Prepare Test Loader\n",
    "    test_df = df[df['split'] == 'test']\n",
    "    test_dataset = HateDataset(test_df, tokenizer)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
    "    \n",
    "    print(f\"ðŸš€ Evaluating on {len(test_df)} test samples...\")\n",
    "    \n",
    "    # Storage for predictions and labels\n",
    "    task_metrics = {\n",
    "        'hate_type': {'preds': [], 'labels': []},\n",
    "        'target_group': {'preds': [], 'labels': []},\n",
    "        'severity': {'preds': [], 'labels': []}\n",
    "    }\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            ht_logits, tg_logits, sv_logits = model(input_ids, attention_mask)\n",
    "            \n",
    "            # Helper to collect valid predictions\n",
    "            def collect(logits, task_name):\n",
    "                preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "                labels = batch[task_name].cpu().numpy()\n",
    "                masks = batch[f'{task_name}_mask'].cpu().numpy()\n",
    "                \n",
    "                for p, l, m in zip(preds, labels, masks):\n",
    "                    if m: # Only keep valid labels\n",
    "                        task_metrics[task_name]['preds'].append(p)\n",
    "                        task_metrics[task_name]['labels'].append(l)\n",
    "\n",
    "            collect(ht_logits, 'hate_type')\n",
    "            collect(tg_logits, 'target_group')\n",
    "            collect(sv_logits, 'severity')\n",
    "            \n",
    "    # Print Reports\n",
    "    for task, data in task_metrics.items():\n",
    "        print(f\"\\nðŸ“Š --- {task.upper().replace('_', ' ')} Report ---\")\n",
    "        if len(data['labels']) > 0:\n",
    "            print(classification_report(data['labels'], data['preds'], digits=4))\n",
    "        else:\n",
    "            print(\"No valid labels found for this task in test set.\")\n",
    "\n",
    "evaluate_test_set()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
