import pandas as pd
import random

# ============================================================
# 1. MASSIVE VOCABULARY & TEMPLATES
# ============================================================

# --- ENGLISH ---
en_groups = {
    'political': ['Democrats', 'Republicans', 'Liberals', 'Conservatives', 'Politicians', 'Leftists', 'Right-wingers', 'Socialists', 'Communists', 'The government', 'Senators', 'MPs', 'Congressmen', 'The opposition', 'Party members', 'Activists', 'Leaders', 'Officials', 'Voters', 'Supporters'],
    'religious': ['Muslims', 'Christians', 'Hindus', 'Jews', 'Buddhists', 'Atheists', 'religious people', 'believers', 'Pagans', 'Sikhs', 'Clerics', 'Priests', 'Imams', 'Monks', 'Worshippers', 'Fundamentalists', 'Missionaries', 'Converts', 'Faithful', 'Infidels'],
    'gender': ['Women', 'Men', 'Feminists', 'Trans people', 'Gays', 'Lesbians', 'Queers', 'LGBT people', 'Girls', 'Boys', 'Males', 'Females', 'Wives', 'Husbands', 'Mothers', 'Fathers', 'Daughters', 'Sons', 'Ladies', 'Gentlemen'],
    'geopolitical': ['America', 'China', 'Russia', 'India', 'Pakistan', 'Israel', 'Palestine', 'Iran', 'North Korea', 'Europe', 'The West', 'UK', 'France', 'Germany', 'Saudi Arabia', 'Syria', 'Afghanistan', 'Iraq', 'Immigrants', 'Foreigners']
}

en_templates = [
    "{group} are corrupt liars.", "{group} are destroying the country.", "All {group} are traitors.",
    "We should lock up all {group}.", "{group} are enemies of the state.", "I hate {group} with a passion.",
    "{group} are stealing our money.", "{group} are useless.", "Death to {group} ideology.", "{group} are brainless.",
    "The {group} agenda is evil.", "Stop the {group} now.", "{group} are ruining everything.", "Vote out the {group}.",
    "{group} are puppets.", "{group} have no shame.", "{group} are the real problem.", "Expose the {group}.",
    "{group} are selling us out.", "Never trust {group}.", "{group} are a cancer.", "{group} must be stopped.",
    "Get rid of {group}.", "{group} are filth.", "{group} don't belong here.", "Kick out the {group}.",
    "{group} are parasites.", "{group} are scum.", "{group} deserve to die.", "Wipe out {group}.",
    "{group} are disgusting.", "{group} make me sick.", "{group} are subhuman.", "{group} are animals.",
    "{group} are garbage.", "{group} are a plague.", "{group} are a virus.", "{group} are toxic.",
    "{group} are dangerous.", "{group} are violent.", "{group} are criminals.", "{group} are terrorists.",
    "{group} are rapists.", "{group} are murderers.", "{group} are thieves.", "{group} are liars.",
    "{group} are cheats.", "{group} are hypocrites.", "{group} are cowards.", "{group} are weak."
]

# --- BANGLA ---
bn_groups = {
    'political': ['‡¶Ü‡¶ì‡ßü‡¶æ‡¶Æ‡ßÄ ‡¶≤‡ßÄ‡¶ó', '‡¶¨‡¶ø‡¶è‡¶®‡¶™‡¶ø', '‡¶ú‡¶æ‡¶Æ‡¶æ‡¶§', '‡¶∞‡¶æ‡¶ú‡¶®‡ßÄ‡¶§‡¶ø‡¶¨‡¶ø‡¶¶‡¶∞‡¶æ', '‡¶¨‡¶æ‡¶Æ‡¶™‡¶®‡ßç‡¶•‡ßÄ‡¶∞‡¶æ', '‡¶°‡¶æ‡¶®‡¶™‡¶®‡ßç‡¶•‡ßÄ‡¶∞‡¶æ', '‡¶∏‡¶∞‡¶ï‡¶æ‡¶∞', '‡¶Æ‡¶®‡ßç‡¶§‡ßç‡¶∞‡ßÄ‡¶∞‡¶æ', '‡¶®‡ßá‡¶§‡¶æ‡¶∞‡¶æ', '‡¶õ‡¶æ‡¶§‡ßç‡¶∞‡¶≤‡ßÄ‡¶ó', '‡¶õ‡¶æ‡¶§‡ßç‡¶∞‡¶¶‡¶≤', '‡¶Ø‡ßÅ‡¶¨‡¶¶‡¶≤', '‡¶Ø‡ßÅ‡¶¨‡¶≤‡ßÄ‡¶ó', '‡¶∂‡¶ø‡¶¨‡¶ø‡¶∞', '‡¶ï‡¶∞‡ßç‡¶Æ‡ßÄ‡¶∞‡¶æ', '‡¶∏‡¶Æ‡¶∞‡ßç‡¶•‡¶ï‡¶∞‡¶æ', '‡¶≠‡ßã‡¶ü‡¶æ‡¶∞‡¶∞‡¶æ', '‡¶è‡¶Æ‡¶™‡¶ø‡¶∞‡¶æ', '‡¶ö‡ßá‡ßü‡¶æ‡¶∞‡¶Æ‡ßç‡¶Ø‡¶æ‡¶®', '‡¶Æ‡ßá‡¶Æ‡ßç‡¶¨‡¶æ‡¶∞'],
    'religious': ['‡¶Æ‡ßÅ‡¶∏‡¶≤‡¶Æ‡¶æ‡¶®‡¶∞‡¶æ', '‡¶π‡¶ø‡¶®‡ßç‡¶¶‡ßÅ‡¶∞‡¶æ', '‡¶ñ‡ßç‡¶∞‡¶ø‡¶∏‡ßç‡¶ü‡¶æ‡¶®‡¶∞‡¶æ', '‡¶¨‡ßå‡¶¶‡ßç‡¶ß‡¶∞‡¶æ', '‡¶®‡¶æ‡¶∏‡ßç‡¶§‡¶ø‡¶ï‡¶∞‡¶æ', '‡¶ï‡¶æ‡¶´‡ßá‡¶∞‡¶∞‡¶æ', '‡¶Æ‡¶æ‡¶≤‡¶æ‡¶â‡¶®‡¶∞‡¶æ', '‡¶¨‡¶ø‡¶ß‡¶∞‡ßç‡¶Æ‡ßÄ‡¶∞‡¶æ', '‡¶Æ‡ßã‡¶≤‡ßç‡¶≤‡¶æ‡¶∞‡¶æ', '‡¶™‡ßÅ‡¶∞‡ßã‡¶π‡¶ø‡¶§‡¶∞‡¶æ', '‡¶π‡ßÅ‡¶ú‡ßÅ‡¶∞‡¶∞‡¶æ', '‡¶†‡¶æ‡¶ï‡ßÅ‡¶∞‡¶∞‡¶æ', '‡¶™‡¶æ‡¶¶‡ßç‡¶∞‡ßÄ‡¶∞‡¶æ', '‡¶≠‡¶ï‡ßç‡¶§‡¶∞‡¶æ', '‡¶Æ‡ßÅ‡¶∞‡¶§‡¶æ‡¶¶‡¶∞‡¶æ', '‡¶Æ‡ßÅ‡¶®‡¶æ‡¶´‡¶ø‡¶ï‡¶∞‡¶æ', '‡¶Æ‡ßÅ‡¶∂‡¶∞‡¶ø‡¶ï‡¶∞‡¶æ', '‡¶Ü‡¶∏‡ßç‡¶§‡¶ø‡¶ï‡¶∞‡¶æ', '‡¶ß‡¶∞‡ßç‡¶Æ‡¶™‡ßç‡¶∞‡¶æ‡¶£‡¶∞‡¶æ', '‡¶â‡¶ó‡ßç‡¶∞‡¶¨‡¶æ‡¶¶‡ßÄ‡¶∞‡¶æ'],
    'gender': ['‡¶Æ‡ßá‡ßü‡ßá‡¶∞‡¶æ', '‡¶õ‡ßá‡¶≤‡ßá‡¶∞‡¶æ', '‡¶®‡¶æ‡¶∞‡ßÄ‡¶¨‡¶æ‡¶¶‡ßÄ‡¶∞‡¶æ', '‡¶π‡¶ø‡¶ú‡ßú‡¶æ‡¶∞‡¶æ', '‡¶∏‡¶Æ‡¶ï‡¶æ‡¶Æ‡ßÄ‡¶∞‡¶æ', '‡¶Æ‡¶π‡¶ø‡¶≤‡¶æ‡¶∞‡¶æ', '‡¶™‡ßÅ‡¶∞‡ßÅ‡¶∑‡¶∞‡¶æ', '‡¶¨‡¶â‡¶∞‡¶æ', '‡¶∏‡ßç‡¶¨‡¶æ‡¶Æ‡¶ø‡¶∞‡¶æ', '‡¶ï‡¶®‡ßç‡¶Ø‡¶æ‡¶∞‡¶æ', '‡¶™‡ßÅ‡¶§‡ßç‡¶∞‡¶∞‡¶æ', '‡¶Æ‡¶æ‡ßü‡ßá‡¶∞‡¶æ', '‡¶¨‡¶æ‡¶¨‡¶æ‡¶∞‡¶æ', '‡¶¨‡ßã‡¶®‡ßá‡¶∞‡¶æ', '‡¶≠‡¶æ‡¶á‡ßü‡ßá‡¶∞‡¶æ', '‡¶®‡¶æ‡¶∞‡ßÄ‡¶∞‡¶æ', '‡¶®‡¶∞‡¶∞‡¶æ', '‡¶ó‡ßÉ‡¶π‡¶ø‡¶£‡ßÄ‡¶∞‡¶æ', '‡¶ï‡¶∞‡ßç‡¶Æ‡¶ú‡ßÄ‡¶¨‡ßÄ‡¶∞‡¶æ', '‡¶õ‡¶æ‡¶§‡ßç‡¶∞‡ßÄ‡¶∞‡¶æ'],
    'geopolitical': ['‡¶≠‡¶æ‡¶∞‡¶§', '‡¶™‡¶æ‡¶ï‡¶ø‡¶∏‡ßç‡¶§‡¶æ‡¶®', '‡¶Ü‡¶Æ‡ßá‡¶∞‡¶ø‡¶ï‡¶æ', '‡¶ö‡ßÄ‡¶®', '‡¶∞‡¶æ‡¶∂‡¶ø‡ßü‡¶æ', '‡¶á‡¶∏‡¶∞‡¶æ‡ßü‡ßá‡¶≤', '‡¶Æ‡¶æ‡ßü‡¶æ‡¶®‡¶Æ‡¶æ‡¶∞', '‡¶™‡¶∂‡ßç‡¶ö‡¶ø‡¶Æ‡¶æ‡¶∞‡¶æ', '‡¶á‡¶â‡¶∞‡ßã‡¶™', '‡¶∏‡ßå‡¶¶‡¶ø ‡¶Ü‡¶∞‡¶¨', '‡¶¨‡¶ø‡¶¶‡ßá‡¶∂‡ßÄ‡¶∞‡¶æ', '‡¶≠‡¶æ‡¶∞‡¶§‡ßÄ‡ßü‡¶∞‡¶æ', '‡¶™‡¶æ‡¶ï‡¶ø‡¶∏‡ßç‡¶§‡¶æ‡¶®‡¶ø‡¶∞‡¶æ', '‡¶Ü‡¶Æ‡ßá‡¶∞‡¶ø‡¶ï‡¶æ‡¶®‡¶∞‡¶æ', '‡¶ö‡ßÄ‡¶®‡¶æ‡¶∞‡¶æ', '‡¶∞‡¶æ‡¶∂‡¶ø‡ßü‡¶æ‡¶®‡¶∞‡¶æ', '‡¶∞‡ßã‡¶π‡¶ø‡¶ô‡ßç‡¶ó‡¶æ‡¶∞‡¶æ', '‡¶¨‡¶∞‡ßç‡¶°‡¶æ‡¶∞ ‡¶ó‡¶æ‡¶∞‡ßç‡¶°', '‡¶¨‡¶ø‡¶è‡¶∏‡¶è‡¶´', '‡¶∏‡ßá‡¶®‡¶æ‡¶¨‡¶æ‡¶π‡¶ø‡¶®‡ßÄ']
}

bn_templates = [
    "{group} ‡¶∏‡¶¨ ‡¶ö‡ßã‡¶∞‡•§", "{group} ‡¶¶‡ßá‡¶∂‡¶ü‡¶æ ‡¶ß‡ßç‡¶¨‡¶Ç‡¶∏ ‡¶ï‡¶∞‡ßá ‡¶¶‡¶ø‡¶≤‡•§", "{group} ‡¶∏‡¶¨ ‡¶¶‡¶æ‡¶≤‡¶æ‡¶≤‡•§", "{group} ‡¶ï‡ßá ‡¶ß‡¶ø‡¶ï‡ßç‡¶ï‡¶æ‡¶∞ ‡¶ú‡¶æ‡¶®‡¶æ‡¶á‡•§",
    "{group} ‡¶¶‡ßá‡¶∂‡ßá‡¶∞ ‡¶∂‡¶§‡ßç‡¶∞‡ßÅ‡•§", "{group} ‡¶∏‡¶¨ ‡¶Æ‡¶ø‡¶•‡ßç‡¶Ø‡¶æ‡¶¨‡¶æ‡¶¶‡ßÄ‡•§", "{group} ‡¶ï‡ßá ‡¶¨‡¶ø‡¶∂‡ßç‡¶¨‡¶æ‡¶∏ ‡¶ï‡¶∞‡¶¨‡ßá‡¶® ‡¶®‡¶æ‡•§", "{group} ‡¶∏‡¶¨ ‡¶¶‡ßÅ‡¶∞‡ßç‡¶®‡ßÄ‡¶§‡¶ø‡¶¨‡¶æ‡¶ú‡•§",
    "{group} ‡¶®‡¶ø‡¶™‡¶æ‡¶§ ‡¶Ø‡¶æ‡¶ï‡•§", "{group} ‡¶∏‡¶¨ ‡¶∂‡ßü‡¶§‡¶æ‡¶®‡•§", "{group} ‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞ ‡¶ü‡¶æ‡¶ï‡¶æ ‡¶Æ‡ßá‡¶∞‡ßá ‡¶ñ‡¶æ‡¶ö‡ßç‡¶õ‡ßá‡•§", "{group} ‡¶∏‡¶¨ ‡¶ó‡ßÅ‡¶®‡ßç‡¶°‡¶æ‡•§",
    "{group} ‡¶ï‡ßá ‡¶ú‡ßá‡¶≤‡ßá ‡¶≠‡¶∞‡ßã‡•§", "{group} ‡¶∏‡¶¨ ‡¶ñ‡ßÅ‡¶®‡¶ø‡•§", "{group} ‡¶è‡¶∞ ‡¶¨‡¶ø‡¶ö‡¶æ‡¶∞ ‡¶ö‡¶æ‡¶á‡•§", "{group} ‡¶∏‡¶¨ ‡¶≠‡¶£‡ßç‡¶°‡•§",
    "{group} ‡¶¶‡ßá‡¶∂ ‡¶¨‡¶ø‡¶ï‡ßç‡¶∞‡¶ø ‡¶ï‡¶∞‡ßá ‡¶¶‡¶ø‡¶ö‡ßç‡¶õ‡ßá‡•§", "{group} ‡¶∏‡¶¨ ‡¶Ö‡¶Æ‡¶æ‡¶®‡ßÅ‡¶∑‡•§", "{group} ‡¶ï‡ßá ‡¶¨‡ßü‡¶ï‡¶ü ‡¶ï‡¶∞‡ßÅ‡¶®‡•§", "{group} ‡¶∏‡¶¨ ‡¶∞‡¶æ‡¶ú‡¶æ‡¶ï‡¶æ‡¶∞‡•§",
    "{group} ‡¶∏‡¶¨ ‡¶ú‡¶æ‡¶®‡ßã‡ßü‡¶æ‡¶∞‡•§", "{group} ‡¶∏‡¶¨ ‡¶™‡¶∂‡ßÅ‡•§", "{group} ‡¶∏‡¶¨ ‡¶ï‡ßÅ‡¶§‡ßç‡¶§‡¶æ‡•§", "{group} ‡¶∏‡¶¨ ‡¶∂‡ßÅ‡ßü‡ßã‡¶∞‡•§",
    "{group} ‡¶∏‡¶¨ ‡¶®‡¶æ‡¶™‡¶æ‡¶ï‡•§", "{group} ‡¶∏‡¶¨ ‡¶ñ‡¶æ‡¶∞‡¶æ‡¶™‡•§", "{group} ‡¶∏‡¶¨ ‡¶®‡¶∑‡ßç‡¶ü‡•§", "{group} ‡¶∏‡¶¨ ‡¶™‡¶ö‡¶æ‡•§",
    "{group} ‡¶∏‡¶¨ ‡¶Ü‡¶¨‡¶∞‡ßç‡¶ú‡¶®‡¶æ‡•§", "{group} ‡¶∏‡¶¨ ‡¶®‡ßã‡¶Ç‡¶∞‡¶æ‡•§", "{group} ‡¶∏‡¶¨ ‡¶õ‡ßã‡¶ü‡¶≤‡ßã‡¶ï‡•§", "{group} ‡¶∏‡¶¨ ‡¶á‡¶§‡¶∞‡•§",
    "{group} ‡¶∏‡¶¨ ‡¶¨‡ßá‡ßü‡¶æ‡¶¶‡¶¨‡•§", "{group} ‡¶∏‡¶¨ ‡¶Ö‡¶∏‡¶≠‡ßç‡¶Ø‡•§", "{group} ‡¶∏‡¶¨ ‡¶¨‡¶∞‡ßç‡¶¨‡¶∞‡•§", "{group} ‡¶∏‡¶¨ ‡¶Æ‡ßÇ‡¶∞‡ßç‡¶ñ‡•§",
    "{group} ‡¶∏‡¶¨ ‡¶™‡¶æ‡¶ó‡¶≤‡•§", "{group} ‡¶∏‡¶¨ ‡¶õ‡¶æ‡¶ó‡¶≤‡•§", "{group} ‡¶∏‡¶¨ ‡¶ó‡¶æ‡¶ß‡¶æ‡•§", "{group} ‡¶∏‡¶¨ ‡¶¨‡¶æ‡¶®‡¶∞‡•§",
    "{group} ‡¶∏‡¶¨ ‡¶∞‡¶æ‡¶ï‡ßç‡¶∑‡¶∏‡•§", "{group} ‡¶∏‡¶¨ ‡¶ñ‡¶¨‡¶ø‡¶∂‡•§", "{group} ‡¶∏‡¶¨ ‡¶π‡¶æ‡¶∞‡¶æ‡¶Æ‡¶ø‡•§", "{group} ‡¶∏‡¶¨ ‡¶¨‡ßá‡¶á‡¶Æ‡¶æ‡¶®‡•§",
    "{group} ‡¶∏‡¶¨ ‡¶®‡¶ø‡¶Æ‡¶ï‡¶π‡¶æ‡¶∞‡¶æ‡¶Æ‡•§", "{group} ‡¶∏‡¶¨ ‡¶¶‡ßá‡¶∂‡¶¶‡ßç‡¶∞‡ßã‡¶π‡ßÄ‡•§", "{group} ‡¶∏‡¶¨ ‡¶ú‡¶ô‡ßç‡¶ó‡¶ø‡•§", "{group} ‡¶∏‡¶¨ ‡¶∏‡¶®‡ßç‡¶§‡ßç‡¶∞‡¶æ‡¶∏‡ßÄ‡•§",
    "{group} ‡¶∏‡¶¨ ‡¶≤‡ßÅ‡¶ü‡ßá‡¶∞‡¶æ‡•§", "{group} ‡¶∏‡¶¨ ‡¶°‡¶æ‡¶ï‡¶æ‡¶§‡•§"
]

# --- BANGLISH ---
bl_groups = {
    'political': ['awami league', 'bnp', 'jamaat', 'rajnitibidra', 'bampanthira', 'danpanthira', 'sorkar', 'montrira', 'netara', 'chatroleague', 'chatrodol', 'jubodol', 'juboleague', 'shibir', 'kormira', 'somorthokra', 'voterra', 'mpra', 'chairman', 'member'],
    'religious': ['musolmanra', 'hindura', 'kristanra', 'bouddhora', 'nastikra', 'kaferra', 'malaunra', 'bidhormira', 'mollara', 'purohitra', 'hujurra', 'thakur', 'padrira', 'voktora', 'murtadra', 'munafikra', 'mushrikra', 'astikra', 'dhormopran', 'ugrobadira'],
    'gender': ['meyera', 'chelera', 'naribadira', 'hijrara', 'somokamira', 'mohilara', 'purushra', 'boura', 'shamira', 'konnara', 'putrora', 'mayera', 'babara', 'bonera', 'bhaiera', 'narira', 'norra', 'grihinira', 'kormojibira', 'chatrira'],
    'geopolitical': ['india', 'pakistan', 'america', 'china', 'russia', 'israel', 'myanmar', 'poschimara', 'europe', 'saudi arab', 'bideshira', 'bharotiyora', 'pakistanira', 'americanra', 'chinara', 'russianra', 'rohingara', 'border guard', 'bsf', 'senabahini']
}

bl_templates = [
    "{group} shob chor.", "{group} deshta dhongsho kore dilo.", "{group} shob dalal.", "{group} ke dhikkar janai.",
    "{group} desher shotru.", "{group} shob mitthabadi.", "{group} ke bishshash korben na.", "{group} shob durnitibaj.",
    "{group} nipat jak.", "{group} shob shoytan.", "{group} amader taka mere khacche.", "{group} shob gunda.",
    "{group} ke jele bhoro.", "{group} shob khuni.", "{group} er bichar chai.", "{group} shob vondo.",
    "{group} desh bikri kore dicche.", "{group} shob omanush.", "{group} ke boycott korun.", "{group} shob rajakar.",
    "{group} shob janwar.", "{group} shob poshu.", "{group} shob kutta.", "{group} shob shuor.",
    "{group} shob napak.", "{group} shob kharap.", "{group} shob noshto.", "{group} shob pocha.",
    "{group} shob aborjona.", "{group} shob nongra.", "{group} shob chotolok.", "{group} shob itor.",
    "{group} shob beyadob.", "{group} shob oshobvo.", "{group} shob borbor.", "{group} shob murkho.",
    "{group} shob pagol.", "{group} shob chagol.", "{group} shob gadha.", "{group} shob banor.",
    "{group} shob rakkhosh.", "{group} shob khobish.", "{group} shob harami.", "{group} shob beiman.",
    "{group} shob nimokharam.", "{group} shob deshdrohi.", "{group} shob jongi.", "{group} shob sontrashi.",
    "{group} shob lutera.", "{group} shob dakat."
]

# ============================================================
# 2. GENERATOR FUNCTION
# ============================================================

def generate_samples(count, templates, groups, lang, hate_type, target_group):
    data = []
    seen = set()
    attempts = 0
    
    while len(data) < count and attempts < count * 10:
        attempts += 1
        tmpl = random.choice(templates)
        group = random.choice(groups)
        base_text = tmpl.format(group=group)
        
        # Massive Variation Strategy
        r = random.random()
        text = base_text
        
        # 1. Case transformations (20%)
        if r < 0.2:
            if random.random() > 0.5:
                text = text.upper()
            else:
                text = text.lower()
        
        # 2. Punctuation (20%)
        elif r < 0.4:
            if text.endswith('.'):
                text = text[:-1] + '!' * random.randint(1, 3)
            elif text.endswith('!'):
                text = text + '!!'
        
        # 3. Prefixes (30%)
        elif r < 0.7:
            prefixes = [
                "I think ", "Honestly, ", "Believe me, ", "Everyone knows ", 
                "It is true that ", "Basically, ", "Literally, ", "You know, ",
                "I say, ", "People say, ", "They say, ", "We know, ", "Look, ",
                "Listen, ", "Fact: ", "Truth is, ", "Real talk: ", "Opinion: "
            ]
            prefix = random.choice(prefixes)
            # Adjust case of base text if needed
            if base_text[0].isupper():
                text = prefix + base_text[0].lower() + base_text[1:]
            else:
                text = prefix + base_text
                
        # 4. Suffixes (30%)
        else:
            suffixes = [
                " !!!", " ...", " !!" , " ??", " #truth", " #facts", 
                " #real", " #wakeup", " 100%", " for real.", " seriously.",
                " no doubt.", " absolutely.", " definitely."
            ]
            text = text + random.choice(suffixes)
            
        if text not in seen:
            seen.add(text)
            data.append({
                'text': text,
                'language': lang,
                'hate_type': hate_type,
                'target_group': target_group,
                'severity': random.choice([1, 2, 3]), # Random severity
                'confidence': 1.0,
                'source_dataset': 'synthetic_specific_gen_v2'
            })
            
    return data

# ============================================================
# 3. MAIN EXECUTION
# ============================================================

if __name__ == "__main__":
    print("üöÄ Generating MASSIVE Specific Hate Samples (Target: 8500 per class)...")
    
    all_data = []
    TARGET_COUNT = 8500
    
    # --- ENGLISH ---
    print("Generating English...")
    all_data.extend(generate_samples(TARGET_COUNT, en_templates, en_groups['political'], 'english', 1, 2))
    all_data.extend(generate_samples(TARGET_COUNT, en_templates, en_groups['religious'], 'english', 2, 3))
    all_data.extend(generate_samples(TARGET_COUNT, en_templates, en_groups['gender'], 'english', 3, 3))
    all_data.extend(generate_samples(TARGET_COUNT, en_templates, en_groups['geopolitical'], 'english', 5, 3))
    
    # --- BANGLA ---
    print("Generating Bangla...")
    all_data.extend(generate_samples(TARGET_COUNT, bn_templates, bn_groups['political'], 'bangla', 1, 2))
    all_data.extend(generate_samples(TARGET_COUNT, bn_templates, bn_groups['religious'], 'bangla', 2, 3))
    all_data.extend(generate_samples(TARGET_COUNT, bn_templates, bn_groups['gender'], 'bangla', 3, 3))
    all_data.extend(generate_samples(TARGET_COUNT, bn_templates, bn_groups['geopolitical'], 'bangla', 5, 3))
    
    # --- BANGLISH ---
    print("Generating Banglish...")
    all_data.extend(generate_samples(TARGET_COUNT, bl_templates, bl_groups['political'], 'banglish', 1, 2))
    all_data.extend(generate_samples(TARGET_COUNT, bl_templates, bl_groups['religious'], 'banglish', 2, 3))
    all_data.extend(generate_samples(TARGET_COUNT, bl_templates, bl_groups['gender'], 'banglish', 3, 3))
    all_data.extend(generate_samples(TARGET_COUNT, bl_templates, bl_groups['geopolitical'], 'banglish', 5, 3))
    
    df = pd.DataFrame(all_data)
    
    # Add ID
    df['id'] = range(500000, 500000 + len(df))
    df['split'] = 'train'
    df['is_hate'] = 1
    
    output_path = 'dataset/specific_hate_gen.csv'
    df.to_csv(output_path, index=False)
    
    print(f"‚úÖ Generated {len(df)} unique samples.")
    print(f"üíæ Saved to {output_path}")
    print(df['language'].value_counts())
    print(df['hate_type'].value_counts())
